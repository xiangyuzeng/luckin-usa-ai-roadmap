# Luckin Coffee NA â€” Alert Runbook Part 7: INFRA-VM (VM/Host)
# ç‘å¹¸å’–å•¡åŒ—ç¾ â€” æŠ¥è­¦è¿ç»´æ‰‹å†Œ ç¬¬7éƒ¨åˆ†ï¼šåŸºç¡€è®¾æ–½-è™šæ‹Ÿæœº/ä¸»æœº

> **Version / ç‰ˆæœ¬:** 1.0 | **Category / åˆ†ç±»:** INFRA-VM | **Alerts / æŠ¥è­¦æ•°:** 8 (VM-01 to VM-08)
> **Cluster / é›†ç¾¤:** luckyus-prod | **Region / åŒºåŸŸ:** us-east-1 | **Account / è´¦æˆ·:** 257394478466
> **Consolidation / æ•´åˆ:** 17 legacy alerts â†’ 8 new alerts (53% reduction / ç¼©å‡53%)
> **Skill Reference / æŠ€èƒ½å‚è€ƒ:** `/app/skills/ec2-alert-investigation.md` â†’ `/investigate-ec2`
> **Pattern / æ¨¡å¼:** 5A Response (Assess â†’ Acknowledge â†’ Analyze â†’ Act â†’ Aftermath)
> **Last Updated / æœ€åæ›´æ–°:** 2026-02-16

---

## Table of Contents / ç›®å½•

| # | Alert ID | Alert Name | Severity | Section |
|---|----------|------------|----------|---------|
| 1 | LCK-VM-001 | VmCpuUsageWarning | warning | [VM-01](#lck-vm-001) |
| 2 | LCK-VM-002 | VmCpuUsageCritical | critical | [VM-02](#lck-vm-002) |
| 3 | LCK-VM-003 | VmMemoryUsageWarning | warning | [VM-03](#lck-vm-003) |
| 4 | LCK-VM-004 | VmMemoryUsageCritical | critical | [VM-04](#lck-vm-004) |
| 5 | LCK-VM-005 | VmDiskUsageWarning | warning | [VM-05](#lck-vm-005) |
| 6 | LCK-VM-006 | VmDiskUsageCritical | critical | [VM-06](#lck-vm-006) |
| 7 | LCK-VM-007 | VmNetworkErrorsWarning | warning | [VM-07](#lck-vm-007) |
| 8 | LCK-VM-008 | VmInstanceDownCritical | critical | [VM-08](#lck-vm-008) |

---

## Category Overview / åˆ†ç±»æ¦‚è¿°

**English:** INFRA-VM alerts monitor the health and performance of all EC2/VM instances in the Luckin Coffee NA production environment. These alerts cover CPU utilization, memory usage, disk capacity, network errors, and instance availability. All alerts use `node_exporter` metrics collected via Prometheus and evaluated by the **Basic VMAlert endpoint** (10.238.3.153:8880). The 8 alerts are organized into warning/critical pairs for CPU, memory, and disk, plus a network errors warning and an instance-down critical alert.

**ä¸­æ–‡:** INFRA-VM æŠ¥è­¦ç›‘æ§ç‘å¹¸å’–å•¡åŒ—ç¾ç”Ÿäº§ç¯å¢ƒä¸­æ‰€æœ‰ EC2/VM å®ä¾‹çš„å¥åº·çŠ¶å†µå’Œæ€§èƒ½è¡¨ç°ã€‚è¿™äº›æŠ¥è­¦æ¶µç›– CPU åˆ©ç”¨ç‡ã€å†…å­˜ä½¿ç”¨ã€ç£ç›˜å®¹é‡ã€ç½‘ç»œé”™è¯¯å’Œå®ä¾‹å¯ç”¨æ€§ã€‚æ‰€æœ‰æŠ¥è­¦ä½¿ç”¨é€šè¿‡ Prometheus é‡‡é›†çš„ `node_exporter` æŒ‡æ ‡ï¼Œç”± **Basic VMAlert èŠ‚ç‚¹** (10.238.3.153:8880) è¿›è¡Œè¯„ä¼°ã€‚8 ä¸ªæŠ¥è­¦æŒ‰ CPUã€å†…å­˜å’Œç£ç›˜çš„è­¦å‘Š/ä¸¥é‡é…å¯¹ç»„ç»‡ï¼ŒåŠ ä¸Šç½‘ç»œé”™è¯¯è­¦å‘Šå’Œå®ä¾‹å®•æœºä¸¥é‡æŠ¥è­¦ã€‚

---

## Recording Rules / é¢„è®¡ç®—è§„åˆ™

All recording rules are in group `lck-na.recording.vm` with evaluation interval `30s`.

| Record Name | Expression | Purpose / ç”¨é€” |
|-------------|-----------|----------------|
| `lckna:vm:cpu_avg5m` | `100 - avg by (instance)(rate(node_cpu_seconds_total{mode="idle", env="production"}[5m])) * 100` | 5-minute average CPU usage per instance / æ¯å®ä¾‹5åˆ†é’Ÿå¹³å‡CPUä½¿ç”¨ç‡ |
| `lckna:vm:memory_avg10m` | `avg_over_time((1 - node_memory_MemAvailable_bytes{env="production"} / node_memory_MemTotal_bytes{env="production"})[10m:]) * 100` | 10-minute average memory usage / 10åˆ†é’Ÿå¹³å‡å†…å­˜ä½¿ç”¨ç‡ |
| `lckna:vm:disk_util` | `100 - (node_filesystem_avail_bytes{env="production", mountpoint="/", fstype!="tmpfs"} / node_filesystem_size_bytes{env="production", mountpoint="/", fstype!="tmpfs"}) * 100` | Root partition disk utilization / æ ¹åˆ†åŒºç£ç›˜åˆ©ç”¨ç‡ |
| `lckna:vm:net_errors_rate5m` | `rate(node_network_receive_errs_total{env="production"}[5m]) + rate(node_network_transmit_errs_total{env="production"}[5m])` | 5-minute network error rate (rx+tx) / 5åˆ†é’Ÿç½‘ç»œé”™è¯¯ç‡(æ”¶+å‘) |

---

## Alert Summary / æŠ¥è­¦æ€»è§ˆ

| ID | Alert Name | Severity | Tier | Threshold | For | Old IDs | Consolidation |
|----|-----------|----------|------|-----------|-----|---------|---------------|
| VM-01 | VmCpuUsageWarning | warning | 2 | CPU > 80% â‰¤ 95% | 5m | ALR-100, ALR-101 | MERGE |
| VM-02 | VmCpuUsageCritical | critical | 1 | CPU > 95% | 3m | ALR-102, ALR-103 | MERGE |
| VM-03 | VmMemoryUsageWarning | warning | 2 | Memory > 85% â‰¤ 95% | 10m | ALR-109 | KEEP |
| VM-04 | VmMemoryUsageCritical | critical | 1 | Memory > 95% | 5m | ALR-109 | SPLIT |
| VM-05 | VmDiskUsageWarning | warning | 2 | Disk > 85% â‰¤ 95% | 10m | ALR-111 | KEEP |
| VM-06 | VmDiskUsageCritical | critical | 1 | Disk > 95% | 5m | ALR-104, ALR-105, ALR-111 | MERGE |
| VM-07 | VmNetworkErrorsWarning | warning | 2 | Errors > 200/s or Drops > 20/s | 5m | ALR-108, ALR-112â€“115 | MERGE |
| VM-08 | VmInstanceDownCritical | critical | 1 | up == 0 | 10m | ALR-110, ALR-116 | MERGE |

---

## Alert Chains / æŠ¥è­¦é“¾

```
CPU Chain / CPUé“¾:
  VM-01 (warning >80%) â”€â”€escalateâ”€â”€â–¶ VM-02 (critical >95%)

Memory Chain / å†…å­˜é“¾:
  VM-03 (warning >85%) â”€â”€escalateâ”€â”€â–¶ VM-04 (critical >95%)

Disk Chain / ç£ç›˜é“¾:
  VM-05 (warning >85%) â”€â”€escalateâ”€â”€â–¶ VM-06 (critical >95%)

Independent / ç‹¬ç«‹:
  VM-07 (network errors/drops)
  VM-08 (instance down) â† highest priority, may cause all other alerts
```

---

<a id="lck-vm-001"></a>
## VM-01: VmCpuUsageWarning / VM CPU ä½¿ç”¨ç‡åé«˜ï¼ˆè­¦å‘Šçº§ï¼‰

```yaml
alert_id: LCK-VM-001
alert_name: VmCpuUsageWarning
old_ids: [ALR-100, ALR-101]
consolidation: MERGE
severity: warning
tier: "2"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call
sla_response: 15 min acknowledge / 1 hour first update / 4 hours resolution
notification_channel: wecom+twilio-lead
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-overview
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmCpuUsageWarning
expr: lckna:vm:cpu_avg5m > 80 and lckna:vm:cpu_avg5m <= 95
for: 5m
labels:
  severity: warning
  tier: "2"
  category: infra-vm
  team: sys-ops
  dashboard: vm-overview
annotations:
  summary: "VM CPU usage warning on {{ $labels.instance }}"
  description: "CPU usage is {{ $value | printf \"%.1f\" }}% (threshold: 80%) for 5 minutes on {{ $labels.instance }}."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-cpu-warning"
  dashboard_url: "https://grafana.luckinus.com/d/vm-overview"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
# Recording rule used:
lckna:vm:cpu_avg5m > 80 and lckna:vm:cpu_avg5m <= 95

# Underlying raw expression:
(100 - avg by (instance)(rate(node_cpu_seconds_total{mode="idle", env="production"}[5m])) * 100) > 80
```

**Meaning / å«ä¹‰:** The 5-minute average CPU utilization on a production VM has exceeded 80% but remains below 95% for at least 5 minutes. This is an early warning that the instance may be approaching capacity. Check for runaway processes, increased traffic, or batch jobs consuming CPU.

**å«ä¹‰ï¼š** ç”Ÿäº§ç¯å¢ƒè™šæ‹Ÿæœºçš„5åˆ†é’Ÿå¹³å‡CPUåˆ©ç”¨ç‡å·²è¶…è¿‡80%ä½†ä½äº95%ï¼ŒæŒç»­è‡³å°‘5åˆ†é’Ÿã€‚è¿™æ˜¯å®ä¾‹å¯èƒ½æ¥è¿‘å®¹é‡ä¸Šé™çš„æ—©æœŸè­¦å‘Šã€‚æ£€æŸ¥å¤±æ§è¿›ç¨‹ã€æµé‡å¢é•¿æˆ–æ¶ˆè€—CPUçš„æ‰¹å¤„ç†ä½œä¸šã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
# Check if golden path (order flow) is impacted
# æ£€æŸ¥é»„é‡‘æµç¨‹ï¼ˆä¸‹å•æµç¨‹ï¼‰æ˜¯å¦å—å½±å“
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
# If == 0 for 10 min â†’ Golden path DOWN â†’ Escalate to Tier 3
# å¦‚æœ10åˆ†é’Ÿå†…ä¸º0 â†’ é»„é‡‘æµç¨‹ä¸­æ–­ â†’ å‡çº§è‡³Tier 3
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
# Identify the affected instance from alert labels
# ä»å‘Šè­¦æ ‡ç­¾è¯†åˆ«å—å½±å“å®ä¾‹
INSTANCE="{{ $labels.instance }}"  # e.g., 10.238.x.x:9100

# Check current CPU value
# æ£€æŸ¥å½“å‰CPUå€¼
curl -s "http://localhost:9090/api/v1/query?query=lckna:vm:cpu_avg5m{instance='${INSTANCE}'}" | jq '.data.result[0].value[1]'
```

#### 1.3 Severity Classification / ä¸¥é‡ç¨‹åº¦åˆ†ç±»

| Condition / æ¡ä»¶ | Classification / åˆ†ç±» | Action / æ“ä½œ |
|---|---|---|
| CPU 80-85%, no golden path impact | Low Warning / ä½çº§è­¦å‘Š | Monitor, investigate when convenient |
| CPU 85-90%, trending up | Medium Warning / ä¸­çº§è­¦å‘Š | Investigate within 30 min |
| CPU 90-95%, services slowing | High Warning / é«˜çº§è­¦å‘Š | Investigate immediately, prepare to escalate |
| Golden path impacted | â†’ Treat as Tier 3 Critical / æŒ‰Tier 3å¤„ç† | Immediate escalation |

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 15 min / 15 åˆ†é’Ÿå†…)

```bash
# Silence alert for investigation window (1 hour)
# é™é»˜å‘Šè­¦ä»¥ä¾¿è°ƒæŸ¥ï¼ˆ1å°æ—¶ï¼‰
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="Investigating VM CPU warning on ${INSTANCE}" \
  --duration=1h \
  alertname=VmCpuUsageWarning instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸŸ¡ [ACKNOWLEDGED] VM-01: VmCpuUsageWarning
Instance: {{ $labels.instance }}
CPU: {{ $value }}%
Threshold: 80%
Responder: [Your Name]
ETA: Investigating, update in 30 min
Status: Silenced for 1h
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **Runaway process / å¤±æ§è¿›ç¨‹:** Java/Python process in infinite loop or memory thrashing
- **Traffic spike / æµé‡çªå¢:** Seasonal promotion or marketing event driving increased requests
- **Batch job overlap / æ‰¹å¤„ç†ä½œä¸šé‡å :** Multiple cron jobs running simultaneously
- **Insufficient instance size / å®ä¾‹è§„æ ¼ä¸è¶³:** Instance type too small for workload
- **Container resource limits / å®¹å™¨èµ„æºé™åˆ¶:** If running containers, CPU limits too high relative to instance

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
# SSH to the affected instance
# SSHç™»å½•å—å½±å“å®ä¾‹
ssh ec2-user@${INSTANCE%%:*}

# Top CPU consumers (top 10)
# CPUæ¶ˆè€—æœ€é«˜çš„è¿›ç¨‹ï¼ˆå‰10ï¼‰
top -bn1 -o %CPU | head -17

# Detailed process CPU usage
# è¯¦ç»†è¿›ç¨‹CPUä½¿ç”¨æƒ…å†µ
ps aux --sort=-%cpu | head -15

# Check load average vs CPU cores
# æ£€æŸ¥è´Ÿè½½å‡è¡¡ä¸CPUæ ¸å¿ƒæ•°å¯¹æ¯”
echo "Load: $(cat /proc/loadavg) | Cores: $(nproc)"

# Check for IOWait (indicates disk bottleneck)
# æ£€æŸ¥IOWaitï¼ˆè¡¨ç¤ºç£ç›˜ç“¶é¢ˆï¼‰
iostat -x 1 3

# Check steal time (noisy neighbor on shared instance)
# æ£€æŸ¥stealæ—¶é—´ï¼ˆå…±äº«å®ä¾‹ä¸Šçš„å™ªå£°é‚»å±…ï¼‰
mpstat 1 5

# Check for OOM kills driving CPU spikes
# æ£€æŸ¥OOM killæ˜¯å¦å¯¼è‡´CPUé£™å‡
dmesg | grep -i "oom\|killed" | tail -10

# Check recent cron jobs
# æ£€æŸ¥æœ€è¿‘çš„å®šæ—¶ä»»åŠ¡
grep CRON /var/log/syslog | tail -20 2>/dev/null || journalctl -u cron --since "1 hour ago" | tail -20
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
# Verify alert is evaluating on the Basic VMAlert endpoint
# éªŒè¯å‘Šè­¦åœ¨Basic VMAlertèŠ‚ç‚¹ä¸Šè¯„ä¼°
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmCpuUsageWarning")'
```

#### 3.4 PromQL Deep Dive / PromQL æ·±å…¥æŸ¥è¯¢

```promql
# CPU breakdown by mode for the instance
# æŒ‰æ¨¡å¼åˆ†è§£å®ä¾‹CPU
rate(node_cpu_seconds_total{instance="${INSTANCE}", env="production"}[5m]) * 100

# CPU trend over last hour
# æœ€è¿‘1å°æ—¶CPUè¶‹åŠ¿
lckna:vm:cpu_avg5m{instance="${INSTANCE}"}[1h]

# Check if load average exceeds cores
# æ£€æŸ¥è´Ÿè½½å‡è¡¡æ˜¯å¦è¶…è¿‡æ ¸å¿ƒæ•°
node_load5{instance="${INSTANCE}"} / count without(cpu)(node_cpu_seconds_total{instance="${INSTANCE}", mode="idle"})
```

**Dashboard:** [VM Overview](https://grafana.luckinus.com/d/vm-overview)

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| Single runaway process | `kill -15 <PID>` or `kill -9 <PID>` | On-call DevOps |
| Batch job overlap | Reschedule cron jobs to stagger | On-call DevOps |
| Traffic spike (expected) | Scale instance type or add capacity | On-call DevOps + Team Lead |
| Traffic spike (unexpected) | Investigate source, consider rate limiting | On-call DevOps + Team Lead |
| Steal time > 10% | Migrate to dedicated instance | Team Lead approval |
| Consistently > 80% | Right-size instance (upgrade) | Change request |

```bash
# Kill a runaway process (use -15 first, then -9 if needed)
# ç»ˆæ­¢å¤±æ§è¿›ç¨‹ï¼ˆå…ˆç”¨-15ï¼Œå¿…è¦æ—¶ç”¨-9ï¼‰
kill -15 <PID>
sleep 10
# If still running / å¦‚æœä»åœ¨è¿è¡Œ:
kill -9 <PID>

# Check if CPU recovered
# æ£€æŸ¥CPUæ˜¯å¦æ¢å¤
watch -n5 "top -bn1 | head -5"
```

**Escalation / å‡çº§:**
```
If CPU remains > 80% after 30 min investigation â†’ Escalate to Tier 3
å¦‚æœè°ƒæŸ¥30åˆ†é’ŸåCPUä» > 80% â†’ å‡çº§è‡³Tier 3

Tier 2 (Warning) â†’ (30 min no resolution) â†’ Tier 3 (Critical)
                                                    â†“
                                         China HQ Engineering
                                         ä¸­å›½æ€»éƒ¨å·¥ç¨‹å›¢é˜Ÿ
```

### 5. AFTERMATH / å–„å

- [ ] Verify CPU < 80% for 15 minutes after remediation / éªŒè¯ä¿®å¤åCPUä½äº80%æŒç»­15åˆ†é’Ÿ
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary / åœ¨ä¼ä¸šå¾®ä¿¡å‘å¸ƒè§£å†³æ‘˜è¦
- [ ] If process killed: verify service health / å¦‚æœç»ˆæ­¢äº†è¿›ç¨‹ï¼šéªŒè¯æœåŠ¡å¥åº·
- [ ] Update capacity planning if instance is consistently hot / å¦‚å®ä¾‹æŒç»­é«˜è´Ÿè½½åˆ™æ›´æ–°å®¹é‡è§„åˆ’
- [ ] File change request for instance upgrade if needed / å¦‚éœ€è¦åˆ™æäº¤å®ä¾‹å‡çº§å˜æ›´è¯·æ±‚
- [ ] Related alerts: VM-02 (CPU Critical) â€” check if escalation is imminent / ç›¸å…³å‘Šè­¦ï¼šVM-02 (CPUä¸¥é‡) â€” æ£€æŸ¥æ˜¯å¦å³å°†å‡çº§

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-100 (EC2 CPU > 80%), ALR-101 (EC2 CPU > 85%) â†’ Merged into LCK-VM-001

---

<a id="lck-vm-002"></a>
## VM-02: VmCpuUsageCritical / VM CPU ä½¿ç”¨ç‡è¿‡é«˜ï¼ˆä¸¥é‡çº§ï¼‰

```yaml
alert_id: LCK-VM-002
alert_name: VmCpuUsageCritical
old_ids: [ALR-102, ALR-103]
consolidation: MERGE
severity: critical
tier: "1"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call + Team Lead
sla_response: 5 min acknowledge / 15 min first update / 1 hour resolution
notification_channel: wecom+twilio-all
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-overview
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmCpuUsageCritical
expr: lckna:vm:cpu_avg5m > 95
for: 3m
labels:
  severity: critical
  tier: "1"
  category: infra-vm
  team: sys-ops
  dashboard: vm-overview
annotations:
  summary: "VM CPU CRITICAL on {{ $labels.instance }}"
  description: "CPU usage is {{ $value | printf \"%.1f\" }}% (threshold: 95%) for 3 minutes on {{ $labels.instance }}. Immediate action required."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-cpu-critical"
  dashboard_url: "https://grafana.luckinus.com/d/vm-overview"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
lckna:vm:cpu_avg5m > 95
```

**Meaning / å«ä¹‰:** CPU utilization on a production VM has exceeded 95% for 3 minutes. The instance is effectively saturated. Services may be unresponsive or severely degraded. This is an emergency requiring immediate triage â€” identify the cause and either kill processes or scale the instance.

**å«ä¹‰ï¼š** ç”Ÿäº§ç¯å¢ƒè™šæ‹Ÿæœºçš„CPUåˆ©ç”¨ç‡å·²è¶…è¿‡95%æŒç»­3åˆ†é’Ÿã€‚å®ä¾‹å·²æœ‰æ•ˆé¥±å’Œã€‚æœåŠ¡å¯èƒ½æ— å“åº”æˆ–ä¸¥é‡é™çº§ã€‚è¿™æ˜¯éœ€è¦ç«‹å³åˆ†è¯Šçš„ç´§æ€¥æƒ…å†µâ€”â€”æ‰¾å‡ºåŸå› å¹¶ç»ˆæ­¢è¿›ç¨‹æˆ–æ‰©å±•å®ä¾‹ã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
# CRITICAL: Check golden path IMMEDIATELY
# ä¸¥é‡ï¼šç«‹å³æ£€æŸ¥é»„é‡‘æµç¨‹
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
# If == 0 â†’ Golden path DOWN â†’ All hands on deck
# å¦‚æœä¸º0 â†’ é»„é‡‘æµç¨‹ä¸­æ–­ â†’ å…¨å‘˜å“åº”
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
# What services run on this instance?
# è¯¥å®ä¾‹ä¸Šè¿è¡Œå“ªäº›æœåŠ¡ï¼Ÿ
INSTANCE="{{ $labels.instance }}"
ssh ec2-user@${INSTANCE%%:*} "systemctl list-units --type=service --state=running | grep -v snapd"

# Is this a critical path server?
# è¿™æ˜¯å¦æ˜¯å…³é”®è·¯å¾„æœåŠ¡å™¨ï¼Ÿ
# Check if instance hosts: order-service, payment-service, user-service
```

#### 1.3 Severity Classification / ä¸¥é‡ç¨‹åº¦åˆ†ç±»

| Condition / æ¡ä»¶ | Classification / åˆ†ç±» | Action / æ“ä½œ |
|---|---|---|
| CPU > 95%, no golden path impact | Critical / ä¸¥é‡ | Immediate investigation, 5 min acknowledge |
| CPU > 95%, golden path degraded | Emergency / ç´§æ€¥ | All DevOps respond, China HQ notified |
| CPU > 95% on multiple instances | Incident / äº‹ä»¶ | Declare incident, assemble response team |

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 5 min / 5 åˆ†é’Ÿå†…)

```bash
# Silence alert for emergency window (30 min)
# é™é»˜å‘Šè­¦ä»¥ä¾¿ç´§æ€¥å¤„ç†ï¼ˆ30åˆ†é’Ÿï¼‰
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="CRITICAL: Investigating VM CPU > 95% on ${INSTANCE}" \
  --duration=30m \
  alertname=VmCpuUsageCritical instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸ”´ [CRITICAL] VM-02: VmCpuUsageCritical
Instance: {{ $labels.instance }}
CPU: {{ $value }}%
Threshold: 95%
Responder: [Your Name]
Action: Immediate investigation
ETA: First update in 15 min
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **Fork bomb / forkç‚¸å¼¹:** Malicious or accidental process spawning
- **Memory thrashing / å†…å­˜æŠ–åŠ¨:** Swap usage causing CPU spin on page faults
- **Application deadlock / åº”ç”¨æ­»é”:** Java thread deadlock causing CPU spin-wait
- **Crypto mining / æŒ–çŸ¿:** Compromised instance running unauthorized workloads
- **Cascading failure / çº§è”æ•…éšœ:** Upstream service down, retries consuming CPU

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
ssh ec2-user@${INSTANCE%%:*}

# Immediate top-level view
# ç«‹å³æŸ¥çœ‹é¡¶çº§è§†å›¾
top -bn1 -o %CPU | head -20

# Check for IOWait and steal
# æ£€æŸ¥IOWaitå’Œstealæ—¶é—´
mpstat -P ALL 1 3

# Process tree to find parent of runaway processes
# è¿›ç¨‹æ ‘æŸ¥æ‰¾å¤±æ§è¿›ç¨‹çš„çˆ¶è¿›ç¨‹
ps auxf --sort=-%cpu | head -30

# Check for swap usage (indicates memory pressure driving CPU)
# æ£€æŸ¥swapä½¿ç”¨ï¼ˆè¡¨ç¤ºå†…å­˜å‹åŠ›å¯¼è‡´CPUå‡é«˜ï¼‰
free -h
vmstat 1 5

# Check open file descriptors (fd leak)
# æ£€æŸ¥æ‰“å¼€æ–‡ä»¶æè¿°ç¬¦ï¼ˆfdæ³„æ¼ï¼‰
lsof | wc -l

# Check for unusual network connections
# æ£€æŸ¥å¼‚å¸¸ç½‘ç»œè¿æ¥
ss -tunap | grep ESTABLISHED | wc -l
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmCpuUsageCritical")'
```

#### 3.4 AWS Console Check / AWS æ§åˆ¶å°æ£€æŸ¥

```bash
# Get instance ID from IP
# ä»IPè·å–å®ä¾‹ID
INSTANCE_IP="${INSTANCE%%:*}"
aws ec2 describe-instances --filters "Name=private-ip-address,Values=${INSTANCE_IP}" \
  --query "Reservations[0].Instances[0].{ID:InstanceId,Type:InstanceType,State:State.Name}" --output table

# Check CloudWatch CPU metrics
# æ£€æŸ¥CloudWatch CPUæŒ‡æ ‡
aws cloudwatch get-metric-statistics --namespace AWS/EC2 \
  --metric-name CPUUtilization --dimensions Name=InstanceId,Value=<INSTANCE_ID> \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 60 --statistics Maximum
```

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| Runaway process identified | `kill -9 <PID>` immediately | On-call DevOps |
| Swap thrashing | Identify memory hog, kill or restart service | On-call DevOps |
| Suspected compromise | Isolate instance (security group), escalate to security | Team Lead + Security |
| Cascading failure | Fix upstream service first | On-call DevOps + App team |
| Instance undersized | Stop & resize instance type | Team Lead approval |
| Multiple instances affected | Declare incident, scale ASG | Team Lead + China HQ |

```bash
# Emergency: kill top CPU process
# ç´§æ€¥ï¼šç»ˆæ­¢æœ€é«˜CPUè¿›ç¨‹
TOP_PID=$(ps aux --sort=-%cpu | awk 'NR==2{print $2}')
echo "Killing PID ${TOP_PID}: $(ps -p ${TOP_PID} -o comm=)"
kill -9 ${TOP_PID}

# If instance needs to be resized (requires downtime)
# å¦‚æœéœ€è¦è°ƒæ•´å®ä¾‹å¤§å°ï¼ˆéœ€è¦åœæœºï¼‰
# aws ec2 stop-instances --instance-ids <ID>
# aws ec2 modify-instance-attribute --instance-id <ID> --instance-type '{"Value":"m5.xlarge"}'
# aws ec2 start-instances --instance-ids <ID>
```

**Escalation / å‡çº§:**
```
Tier 3 (Critical) â€” immediate notify: All US DevOps + China HQ
Tier 3ï¼ˆä¸¥é‡ï¼‰â€” ç«‹å³é€šçŸ¥ï¼šå…¨éƒ¨ç¾å›½DevOps + ä¸­å›½æ€»éƒ¨

If no resolution in 30 min â†’ Executive escalation
å¦‚æœ30åˆ†é’Ÿå†…æœªè§£å†³ â†’ ç®¡ç†å±‚å‡çº§
```

### 5. AFTERMATH / å–„å

- [ ] Verify CPU < 80% for 15 minutes / éªŒè¯CPUä½äº80%æŒç»­15åˆ†é’Ÿ
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary with root cause / åœ¨ä¼ä¸šå¾®ä¿¡å‘å¸ƒå«æ ¹å› çš„è§£å†³æ‘˜è¦
- [ ] If process killed: verify affected service restored / å¦‚ç»ˆæ­¢äº†è¿›ç¨‹ï¼šéªŒè¯å—å½±å“æœåŠ¡å·²æ¢å¤
- [ ] Conduct brief post-incident review / è¿›è¡Œç®€è¦äº‹åå›é¡¾
- [ ] File capacity planning ticket if instance is undersized / å¦‚å®ä¾‹è§„æ ¼ä¸è¶³åˆ™æäº¤å®¹é‡è§„åˆ’å·¥å•
- [ ] Update monitoring if new failure mode discovered / å¦‚å‘ç°æ–°æ•…éšœæ¨¡å¼åˆ™æ›´æ–°ç›‘æ§
- [ ] Related alerts: VM-01 (CPU Warning) â€” should resolve after VM-02 resolved / ç›¸å…³å‘Šè­¦ï¼šVM-01 (CPUè­¦å‘Š) â€” åœ¨VM-02è§£å†³ååº”è‡ªåŠ¨æ¢å¤

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-102 (EC2 CPU > 90%), ALR-103 (EC2 CPU > 95%) â†’ Merged into LCK-VM-002

---

<a id="lck-vm-003"></a>
## VM-03: VmMemoryUsageWarning / VM å†…å­˜ä½¿ç”¨ç‡åé«˜ï¼ˆè­¦å‘Šçº§ï¼‰

```yaml
alert_id: LCK-VM-003
alert_name: VmMemoryUsageWarning
old_ids: [ALR-109]
consolidation: KEEP
severity: warning
tier: "2"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call
sla_response: 15 min acknowledge / 1 hour first update / 4 hours resolution
notification_channel: wecom+twilio-lead
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-memory
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmMemoryUsageWarning
expr: lckna:vm:memory_avg10m > 85 and lckna:vm:memory_avg10m <= 95
for: 10m
labels:
  severity: warning
  tier: "2"
  category: infra-vm
  team: sys-ops
  dashboard: vm-memory
annotations:
  summary: "VM memory usage warning on {{ $labels.instance }}"
  description: "Memory usage is {{ $value | printf \"%.1f\" }}% (threshold: 85%) for 10 minutes on {{ $labels.instance }}."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-memory-warning"
  dashboard_url: "https://grafana.luckinus.com/d/vm-memory"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
lckna:vm:memory_avg10m > 85 and lckna:vm:memory_avg10m <= 95

# Underlying:
avg_over_time(
  (1 - node_memory_MemAvailable_bytes{env="production"} / node_memory_MemTotal_bytes{env="production"})[10m:]
) * 100
```

**Meaning / å«ä¹‰:** The 10-minute average memory utilization on a production VM has exceeded 85% but remains below 95% for at least 10 minutes. This indicates the instance is running low on available memory. Investigate for memory leaks, oversized JVM heaps, or unexpected memory consumers before an OOM event occurs.

**å«ä¹‰ï¼š** ç”Ÿäº§ç¯å¢ƒè™šæ‹Ÿæœºçš„10åˆ†é’Ÿå¹³å‡å†…å­˜åˆ©ç”¨ç‡å·²è¶…è¿‡85%ä½†ä½äº95%ï¼ŒæŒç»­è‡³å°‘10åˆ†é’Ÿã€‚è¿™è¡¨ç¤ºå®ä¾‹å¯ç”¨å†…å­˜ä¸è¶³ã€‚åœ¨å‘ç”ŸOOMäº‹ä»¶ä¹‹å‰ï¼Œè°ƒæŸ¥å†…å­˜æ³„æ¼ã€è¿‡å¤§çš„JVMå †æˆ–æ„å¤–çš„å†…å­˜æ¶ˆè€—è€…ã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
INSTANCE="{{ $labels.instance }}"
# Check current memory value
curl -s "http://localhost:9090/api/v1/query?query=lckna:vm:memory_avg10m{instance='${INSTANCE}'}" | jq '.data.result[0].value[1]'
```

#### 1.3 Severity Classification / ä¸¥é‡ç¨‹åº¦åˆ†ç±»

| Condition / æ¡ä»¶ | Classification / åˆ†ç±» | Action / æ“ä½œ |
|---|---|---|
| Memory 85-90%, stable | Low Warning / ä½çº§è­¦å‘Š | Monitor, investigate when convenient |
| Memory 90-95%, trending up | High Warning / é«˜çº§è­¦å‘Š | Investigate immediately |
| Swap usage increasing | High Warning / é«˜çº§è­¦å‘Š | Potential OOM, prepare for escalation |
| Golden path impacted | â†’ Treat as Tier 3 / æŒ‰Tier 3å¤„ç† | Immediate escalation |

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 15 min / 15 åˆ†é’Ÿå†…)

```bash
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="Investigating VM memory warning on ${INSTANCE}" \
  --duration=1h \
  alertname=VmMemoryUsageWarning instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸŸ¡ [ACKNOWLEDGED] VM-03: VmMemoryUsageWarning
Instance: {{ $labels.instance }}
Memory: {{ $value }}%
Threshold: 85%
Responder: [Your Name]
ETA: Investigating, update in 30 min
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **Memory leak / å†…å­˜æ³„æ¼:** Application gradually consuming memory without releasing
- **JVM heap oversized / JVMå †è¿‡å¤§:** Java process with -Xmx set too high for instance
- **Cache growth / ç¼“å­˜å¢é•¿:** In-memory cache (ehcache, Guava) growing unbounded
- **Log buffer buildup / æ—¥å¿—ç¼“å†²ç§¯å‹:** Filebeat or Fluentd buffering logs in memory
- **Container memory limits / å®¹å™¨å†…å­˜é™åˆ¶:** Containers consuming more than expected

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
ssh ec2-user@${INSTANCE%%:*}

# Memory overview
# å†…å­˜æ¦‚è§ˆ
free -h

# Top memory consumers
# å†…å­˜æ¶ˆè€—æœ€é«˜çš„è¿›ç¨‹
ps aux --sort=-%mem | head -15

# Detailed memory breakdown
# è¯¦ç»†å†…å­˜åˆ†è§£
cat /proc/meminfo | grep -E "MemTotal|MemFree|MemAvailable|Buffers|Cached|SwapTotal|SwapFree|Slab"

# Check for swap usage (early OOM indicator)
# æ£€æŸ¥swapä½¿ç”¨ï¼ˆOOMæ—©æœŸæŒ‡æ ‡ï¼‰
swapon --show
vmstat 1 5

# Check for memory leaks (RSS growth over time)
# æ£€æŸ¥å†…å­˜æ³„æ¼ï¼ˆRSSéšæ—¶é—´å¢é•¿ï¼‰
# Compare with earlier values if available
ps -eo pid,ppid,rss,vsz,comm --sort=-rss | head -15

# Check OOM killer history
# æ£€æŸ¥OOM killerå†å²
dmesg | grep -i "oom\|killed" | tail -20

# Java heap usage (if Java process is top consumer)
# Javaå †ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœJavaè¿›ç¨‹æ˜¯æœ€å¤§æ¶ˆè€—è€…ï¼‰
jcmd $(pgrep java | head -1) GC.heap_info 2>/dev/null || echo "No Java process or jcmd unavailable"
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmMemoryUsageWarning")'
```

#### 3.4 PromQL Deep Dive / PromQL æ·±å…¥æŸ¥è¯¢

```promql
# Memory trend over 6 hours
lckna:vm:memory_avg10m{instance="${INSTANCE}"}[6h]

# Breakdown: used vs cached vs buffers
node_memory_MemTotal_bytes{instance="${INSTANCE}"} - node_memory_MemAvailable_bytes{instance="${INSTANCE}"}

# Swap usage
node_memory_SwapTotal_bytes{instance="${INSTANCE}"} - node_memory_SwapFree_bytes{instance="${INSTANCE}"}
```

**Dashboard:** [VM Memory](https://grafana.luckinus.com/d/vm-memory)

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| Identified memory leak | Restart affected service gracefully | On-call DevOps |
| JVM heap too large | Adjust -Xmx, restart service | On-call DevOps + App team |
| Cache growth | Clear cache or set eviction policy | On-call DevOps + App team |
| Log buffer buildup | Restart log shipper, check downstream | On-call DevOps |
| Consistently > 85% | File capacity planning ticket | Change request |

```bash
# Restart a service gracefully (example: Java application)
# ä¼˜é›…é‡å¯æœåŠ¡ï¼ˆç¤ºä¾‹ï¼šJavaåº”ç”¨ï¼‰
systemctl restart <service-name>

# Clear filesystem caches (temporary relief, not a fix)
# æ¸…é™¤æ–‡ä»¶ç³»ç»Ÿç¼“å­˜ï¼ˆä¸´æ—¶ç¼“è§£ï¼Œéä¿®å¤ï¼‰
sync && echo 3 > /proc/sys/vm/drop_caches
```

**Escalation / å‡çº§:**
```
If memory continues rising toward 95% â†’ Prepare for VM-04 Critical
å¦‚æœå†…å­˜æŒç»­ä¸Šå‡è‡³95% â†’ å‡†å¤‡åº”å¯¹VM-04ä¸¥é‡å‘Šè­¦

Tier 2 â†’ (30 min no resolution) â†’ Tier 3
```

### 5. AFTERMATH / å–„å

- [ ] Verify memory < 85% for 15 minutes / éªŒè¯å†…å­˜ä½äº85%æŒç»­15åˆ†é’Ÿ
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary / åœ¨ä¼ä¸šå¾®ä¿¡å‘å¸ƒè§£å†³æ‘˜è¦
- [ ] If service restarted: verify all health checks pass / å¦‚é‡å¯äº†æœåŠ¡ï¼šéªŒè¯æ‰€æœ‰å¥åº·æ£€æŸ¥é€šè¿‡
- [ ] For memory leaks: file bug ticket with application team / å¯¹äºå†…å­˜æ³„æ¼ï¼šå‘åº”ç”¨å›¢é˜Ÿæäº¤bugå·¥å•
- [ ] Review JVM/application memory settings / æ£€æŸ¥JVM/åº”ç”¨å†…å­˜è®¾ç½®
- [ ] Related alerts: VM-04 (Memory Critical) â€” monitor for escalation / ç›¸å…³å‘Šè­¦ï¼šVM-04 (å†…å­˜ä¸¥é‡) â€” ç›‘æ§æ˜¯å¦å‡çº§

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-109 (Host Memory > 85%) â†’ Kept as LCK-VM-003

---

<a id="lck-vm-004"></a>
## VM-04: VmMemoryUsageCritical / VM å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼ˆä¸¥é‡çº§ï¼‰

```yaml
alert_id: LCK-VM-004
alert_name: VmMemoryUsageCritical
old_ids: [ALR-109]
consolidation: SPLIT
severity: critical
tier: "1"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call + Team Lead
sla_response: 5 min acknowledge / 15 min first update / 1 hour resolution
notification_channel: wecom+twilio-all
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-memory
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmMemoryUsageCritical
expr: lckna:vm:memory_avg10m > 95
for: 5m
labels:
  severity: critical
  tier: "1"
  category: infra-vm
  team: sys-ops
  dashboard: vm-memory
annotations:
  summary: "VM memory CRITICAL on {{ $labels.instance }}"
  description: "Memory usage is {{ $value | printf \"%.1f\" }}% (threshold: 95%) for 5 minutes on {{ $labels.instance }}. OOM imminent."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-memory-critical"
  dashboard_url: "https://grafana.luckinus.com/d/vm-memory"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
lckna:vm:memory_avg10m > 95
```

**Meaning / å«ä¹‰:** Memory utilization has exceeded 95% for 5 minutes. OOM (Out of Memory) kill is imminent. The Linux kernel will start killing processes to reclaim memory, which may take down critical services. Act immediately to identify the largest memory consumer and either restart it or add memory.

**å«ä¹‰ï¼š** å†…å­˜åˆ©ç”¨ç‡å·²è¶…è¿‡95%æŒç»­5åˆ†é’Ÿã€‚OOM (å†…å­˜ä¸è¶³) killå³å°†å‘ç”Ÿã€‚Linuxå†…æ ¸å°†å¼€å§‹ç»ˆæ­¢è¿›ç¨‹ä»¥å›æ”¶å†…å­˜ï¼Œè¿™å¯èƒ½å¯¼è‡´å…³é”®æœåŠ¡å®•æœºã€‚ç«‹å³è¡ŒåŠ¨ï¼Œæ‰¾å‡ºæœ€å¤§å†…å­˜æ¶ˆè€—è€…å¹¶é‡å¯æˆ–å¢åŠ å†…å­˜ã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
# CRITICAL: Check golden path IMMEDIATELY
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
INSTANCE="{{ $labels.instance }}"
# Check if OOM kills have already occurred
ssh ec2-user@${INSTANCE%%:*} "dmesg | grep -c 'Out of memory'" 2>/dev/null
```

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 5 min / 5 åˆ†é’Ÿå†…)

```bash
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="CRITICAL: Investigating VM memory > 95% on ${INSTANCE}" \
  --duration=30m \
  alertname=VmMemoryUsageCritical instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸ”´ [CRITICAL] VM-04: VmMemoryUsageCritical
Instance: {{ $labels.instance }}
Memory: {{ $value }}%
Threshold: 95% â€” OOM imminent
Responder: [Your Name]
Action: Immediate investigation
ETA: First update in 15 min
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **Memory leak reaching critical / å†…å­˜æ³„æ¼åˆ°è¾¾ä¸´ç•Œ:** Application exhausting available memory
- **OOM cascade / OOMçº§è”:** OOM killer killing processes, other processes consuming freed memory
- **Fork bomb / forkç‚¸å¼¹:** Process spawning unbounded children
- **Shared memory segment leak / å…±äº«å†…å­˜æ®µæ³„æ¼:** IPC shm not released

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
ssh ec2-user@${INSTANCE%%:*}

# Immediate assessment
free -h
ps aux --sort=-%mem | head -10

# Check OOM killer log
dmesg -T | grep -i "oom\|killed" | tail -20

# Check swap exhaustion
swapon --show
cat /proc/meminfo | grep -E "Swap|Committed"

# Check shared memory segments
ipcs -m

# Check /tmp and tmpfs usage (counts against memory)
df -h /tmp /dev/shm
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmMemoryUsageCritical")'
```

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| Single process memory hog | `kill -9 <PID>`, restart service | On-call DevOps |
| OOM kills already happening | Identify and kill top RSS process | On-call DevOps |
| All processes are legitimate | Scale instance (add memory) | Team Lead approval |
| Tmpfs full | Clear /tmp, /dev/shm | On-call DevOps |
| Multiple instances affected | Declare incident | Team Lead + China HQ |

```bash
# Emergency: kill highest memory process
# ç´§æ€¥ï¼šç»ˆæ­¢æœ€é«˜å†…å­˜æ¶ˆè€—è¿›ç¨‹
TOP_MEM_PID=$(ps aux --sort=-%mem | awk 'NR==2{print $2}')
echo "Killing PID ${TOP_MEM_PID}: $(ps -p ${TOP_MEM_PID} -o comm=) ($(ps -p ${TOP_MEM_PID} -o rss= | awk '{printf "%.0f MB", $1/1024}'))"
kill -9 ${TOP_MEM_PID}

# Verify memory freed
sleep 5 && free -h
```

**Escalation / å‡çº§:**
```
Tier 3 (Critical) â€” All US DevOps + China HQ notified immediately
å¦‚æœ15åˆ†é’Ÿå†…æœªè§£å†³ â†’ ç®¡ç†å±‚å‡çº§
```

### 5. AFTERMATH / å–„å

- [ ] Verify memory < 85% for 15 minutes / éªŒè¯å†…å­˜ä½äº85%æŒç»­15åˆ†é’Ÿ
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary with root cause / å‘å¸ƒå«æ ¹å› çš„è§£å†³æ‘˜è¦
- [ ] Verify no OOM kills occurred during incident / éªŒè¯äº‹ä»¶æœŸé—´æ— OOM killå‘ç”Ÿ
- [ ] Conduct post-incident review / è¿›è¡Œäº‹åå›é¡¾
- [ ] File capacity planning ticket / æäº¤å®¹é‡è§„åˆ’å·¥å•
- [ ] Related alerts: VM-03 (Memory Warning) should also resolve / ç›¸å…³å‘Šè­¦ï¼šVM-03 (å†…å­˜è­¦å‘Š) ä¹Ÿåº”æ¢å¤

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-109 (Host Memory > 85%) â†’ Split into LCK-VM-003 (warning) + LCK-VM-004 (critical)

---

<a id="lck-vm-005"></a>
## VM-05: VmDiskUsageWarning / VM ç£ç›˜ä½¿ç”¨ç‡åé«˜ï¼ˆè­¦å‘Šçº§ï¼‰

```yaml
alert_id: LCK-VM-005
alert_name: VmDiskUsageWarning
old_ids: [ALR-111]
consolidation: KEEP
severity: warning
tier: "2"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call
sla_response: 15 min acknowledge / 1 hour first update / 4 hours resolution
notification_channel: wecom+twilio-lead
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-disk
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmDiskUsageWarning
expr: lckna:vm:disk_util > 85 and lckna:vm:disk_util <= 95
for: 10m
labels:
  severity: warning
  tier: "2"
  category: infra-vm
  team: sys-ops
  dashboard: vm-disk
annotations:
  summary: "VM disk usage warning on {{ $labels.instance }}"
  description: "Root partition is {{ $value | printf \"%.1f\" }}% full (threshold: 85%) for 10 minutes on {{ $labels.instance }}."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-disk-warning"
  dashboard_url: "https://grafana.luckinus.com/d/vm-disk"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
lckna:vm:disk_util > 85 and lckna:vm:disk_util <= 95

# Underlying:
100 - (node_filesystem_avail_bytes{env="production", mountpoint="/", fstype!="tmpfs"}
/ node_filesystem_size_bytes{env="production", mountpoint="/", fstype!="tmpfs"}) * 100
```

**Meaning / å«ä¹‰:** The root partition on a production VM has exceeded 85% utilization for 10 minutes. Disk space is running low. Identify large files, old logs, or unused packages consuming space. If unchecked, the disk will fill completely, causing service failures and potential data corruption.

**å«ä¹‰ï¼š** ç”Ÿäº§ç¯å¢ƒè™šæ‹Ÿæœºçš„æ ¹åˆ†åŒºåˆ©ç”¨ç‡å·²è¶…è¿‡85%æŒç»­10åˆ†é’Ÿã€‚ç£ç›˜ç©ºé—´ä¸è¶³ã€‚æ‰¾å‡ºå ç”¨ç©ºé—´çš„å¤§æ–‡ä»¶ã€æ—§æ—¥å¿—æˆ–æœªä½¿ç”¨çš„è½¯ä»¶åŒ…ã€‚å¦‚ä¸å¤„ç†ï¼Œç£ç›˜å°†å®Œå…¨å¡«æ»¡ï¼Œå¯¼è‡´æœåŠ¡æ•…éšœå’Œæ½œåœ¨æ•°æ®æŸåã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
INSTANCE="{{ $labels.instance }}"
ssh ec2-user@${INSTANCE%%:*} "df -h / && echo '---' && df -i /"
```

#### 1.3 Severity Classification / ä¸¥é‡ç¨‹åº¦åˆ†ç±»

| Condition / æ¡ä»¶ | Classification / åˆ†ç±» | Action / æ“ä½œ |
|---|---|---|
| Disk 85-90%, stable | Low Warning / ä½çº§è­¦å‘Š | Clean logs within 4 hours |
| Disk 90-95%, growing | High Warning / é«˜çº§è­¦å‘Š | Clean immediately, prepare volume extension |
| Inodes > 80% | High Warning / é«˜çº§è­¦å‘Š | Find inode consumers (many small files) |
| Database partition filling | High Warning / é«˜çº§è­¦å‘Š | Coordinate with DBA team |

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 15 min / 15 åˆ†é’Ÿå†…)

```bash
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="Investigating VM disk warning on ${INSTANCE}" \
  --duration=2h \
  alertname=VmDiskUsageWarning instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸŸ¡ [ACKNOWLEDGED] VM-05: VmDiskUsageWarning
Instance: {{ $labels.instance }}
Disk: {{ $value }}%
Threshold: 85%
Responder: [Your Name]
ETA: Cleaning up, update in 1 hour
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **Log file growth / æ—¥å¿—æ–‡ä»¶å¢é•¿:** Application or system logs not rotated
- **Docker/container images / Docker/å®¹å™¨é•œåƒ:** Old images and layers accumulating
- **Core dumps / æ ¸å¿ƒè½¬å‚¨:** Application crashes generating large core files
- **Temp files / ä¸´æ—¶æ–‡ä»¶:** Build artifacts, temp downloads not cleaned
- **Database WAL/binlog / æ•°æ®åº“WAL/binlog:** If local DB, transaction logs growing

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
ssh ec2-user@${INSTANCE%%:*}

# Disk usage overview
# ç£ç›˜ä½¿ç”¨æ¦‚è§ˆ
df -h

# Find largest directories
# æŸ¥æ‰¾æœ€å¤§ç›®å½•
du -sh /* 2>/dev/null | sort -rh | head -10

# Find largest files (> 100MB)
# æŸ¥æ‰¾æœ€å¤§æ–‡ä»¶ï¼ˆ> 100MBï¼‰
find / -type f -size +100M -exec ls -lh {} \; 2>/dev/null | sort -k5 -rh | head -20

# Check log directory sizes
# æ£€æŸ¥æ—¥å¿—ç›®å½•å¤§å°
du -sh /var/log/* 2>/dev/null | sort -rh | head -10

# Check Docker disk usage (if applicable)
# æ£€æŸ¥Dockerç£ç›˜ä½¿ç”¨ï¼ˆå¦‚é€‚ç”¨ï¼‰
docker system df 2>/dev/null

# Check inode usage
# æ£€æŸ¥inodeä½¿ç”¨
df -i

# Find directories with most files (inode consumers)
# æŸ¥æ‰¾æ–‡ä»¶æœ€å¤šçš„ç›®å½•ï¼ˆinodeæ¶ˆè€—è€…ï¼‰
find / -xdev -type d -exec sh -c 'echo "$(find "$1" -maxdepth 1 | wc -l) $1"' _ {} \; 2>/dev/null | sort -rn | head -10

# Check deleted but open files (space not reclaimed)
# æ£€æŸ¥å·²åˆ é™¤ä½†ä»æ‰“å¼€çš„æ–‡ä»¶ï¼ˆç©ºé—´æœªå›æ”¶ï¼‰
lsof +L1 2>/dev/null | head -20
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmDiskUsageWarning")'
```

#### 3.4 PromQL Deep Dive / PromQL æ·±å…¥æŸ¥è¯¢

```promql
# Disk usage trend over 24 hours
lckna:vm:disk_util{instance="${INSTANCE}"}[24h]

# Predict when disk will be full (linear prediction 24h)
predict_linear(node_filesystem_avail_bytes{instance="${INSTANCE}", mountpoint="/", fstype!="tmpfs"}[6h], 24*3600) < 0
```

**Dashboard:** [VM Disk](https://grafana.luckinus.com/d/vm-disk)

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| Old logs | `find /var/log -name "*.gz" -mtime +7 -delete` | On-call DevOps |
| Docker images | `docker system prune -af --volumes` | On-call DevOps |
| Core dumps | `find / -name "core.*" -delete` | On-call DevOps |
| Deleted open files | Restart service holding the file | On-call DevOps |
| Legitimate growth | Extend EBS volume | Team Lead approval |

```bash
# Clean old compressed logs (> 7 days)
# æ¸…ç†æ—§çš„å‹ç¼©æ—¥å¿—ï¼ˆ> 7å¤©ï¼‰
find /var/log -name "*.gz" -mtime +7 -delete
find /var/log -name "*.log.*" -mtime +7 -delete

# Clean journal logs (keep last 2 days)
# æ¸…ç†journalæ—¥å¿—ï¼ˆä¿ç•™æœ€è¿‘2å¤©ï¼‰
journalctl --vacuum-time=2d

# Clean Docker (if applicable)
# æ¸…ç†Dockerï¼ˆå¦‚é€‚ç”¨ï¼‰
docker system prune -af --volumes 2>/dev/null

# Extend EBS volume (non-disruptive for ext4/xfs)
# æ‰©å±•EBSå·ï¼ˆå¯¹ext4/xfsæ— ä¸­æ–­ï¼‰
# Step 1: Modify volume in AWS Console or CLI
# aws ec2 modify-volume --volume-id <vol-id> --size <new-size-gb>
# Step 2: Grow filesystem
# sudo growpart /dev/xvda 1
# sudo resize2fs /dev/xvda1   # ext4
# sudo xfs_growfs /            # xfs

# Verify disk usage after cleanup
# æ¸…ç†åéªŒè¯ç£ç›˜ä½¿ç”¨
df -h /
```

### 5. AFTERMATH / å–„å

- [ ] Verify disk < 85% after cleanup / éªŒè¯æ¸…ç†åç£ç›˜ä½äº85%
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary / å‘å¸ƒè§£å†³æ‘˜è¦
- [ ] Set up log rotation if missing / å¦‚ç¼ºå¤±åˆ™è®¾ç½®æ—¥å¿—è½®è½¬
- [ ] Configure logrotate for application logs / ä¸ºåº”ç”¨æ—¥å¿—é…ç½®logrotate
- [ ] If volume extended: update capacity tracking / å¦‚æ‰©å±•äº†å·ï¼šæ›´æ–°å®¹é‡è·Ÿè¸ª
- [ ] Related alerts: VM-06 (Disk Critical) â€” ensure not approaching / ç›¸å…³å‘Šè­¦ï¼šVM-06 (ç£ç›˜ä¸¥é‡) â€” ç¡®ä¿æœªæ¥è¿‘

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-111 (Disk Usage > 85%) â†’ Kept as LCK-VM-005

---

<a id="lck-vm-006"></a>
## VM-06: VmDiskUsageCritical / VM ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜ï¼ˆä¸¥é‡çº§ï¼‰

```yaml
alert_id: LCK-VM-006
alert_name: VmDiskUsageCritical
old_ids: [ALR-104, ALR-105, ALR-111]
consolidation: MERGE
severity: critical
tier: "1"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call + Team Lead
sla_response: 5 min acknowledge / 15 min first update / 1 hour resolution
notification_channel: wecom+twilio-all
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-disk
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmDiskUsageCritical
expr: lckna:vm:disk_util > 95
for: 5m
labels:
  severity: critical
  tier: "1"
  category: infra-vm
  team: sys-ops
  dashboard: vm-disk
annotations:
  summary: "VM disk CRITICAL on {{ $labels.instance }}"
  description: "Root partition is {{ $value | printf \"%.1f\" }}% full (threshold: 95%) for 5 minutes on {{ $labels.instance }}. Emergency cleanup required."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-disk-critical"
  dashboard_url: "https://grafana.luckinus.com/d/vm-disk"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
lckna:vm:disk_util > 95
```

**Meaning / å«ä¹‰:** The root partition has exceeded 95% capacity. The filesystem is nearly full and services will begin failing (unable to write logs, create temp files, or write data). Emergency cleanup is required immediately. If the disk reaches 100%, services will crash and data may be corrupted.

**å«ä¹‰ï¼š** æ ¹åˆ†åŒºå·²è¶…è¿‡95%å®¹é‡ã€‚æ–‡ä»¶ç³»ç»Ÿå‡ ä¹å·²æ»¡ï¼ŒæœåŠ¡å°†å¼€å§‹å¤±è´¥ï¼ˆæ— æ³•å†™å…¥æ—¥å¿—ã€åˆ›å»ºä¸´æ—¶æ–‡ä»¶æˆ–å†™å…¥æ•°æ®ï¼‰ã€‚éœ€è¦ç«‹å³è¿›è¡Œç´§æ€¥æ¸…ç†ã€‚å¦‚æœç£ç›˜è¾¾åˆ°100%ï¼ŒæœåŠ¡å°†å´©æºƒä¸”æ•°æ®å¯èƒ½æŸåã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
# CRITICAL: Check immediately
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
INSTANCE="{{ $labels.instance }}"
# Check exact usage and available space
ssh ec2-user@${INSTANCE%%:*} "df -h / && echo '---Inodes---' && df -i / && echo '---Largest files---' && find / -type f -size +100M -exec ls -lh {} \; 2>/dev/null | sort -k5 -rh | head -5"
```

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 5 min / 5 åˆ†é’Ÿå†…)

```bash
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="CRITICAL: VM disk > 95% on ${INSTANCE}, emergency cleanup" \
  --duration=30m \
  alertname=VmDiskUsageCritical instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸ”´ [CRITICAL] VM-06: VmDiskUsageCritical
Instance: {{ $labels.instance }}
Disk: {{ $value }}%
Threshold: 95% â€” services may fail
Responder: [Your Name]
Action: Emergency cleanup in progress
ETA: First update in 15 min
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **Unrotated logs / æœªè½®è½¬çš„æ—¥å¿—:** Application writing unbounded log files
- **Database binlog / æ•°æ®åº“binlog:** MySQL binary logs consuming all space
- **Core dump storm / æ ¸å¿ƒè½¬å‚¨é£æš´:** Application crashing repeatedly, generating core files
- **Inode exhaustion / Inodeè€—å°½:** Millions of small files (session files, mail queue)
- **Read-only filesystem / åªè¯»æ–‡ä»¶ç³»ç»Ÿ:** Disk errors causing remount as read-only

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
ssh ec2-user@${INSTANCE%%:*}

# Quick space assessment
df -h / && df -i /

# Check if filesystem is read-only
mount | grep "on / " | grep -o "r[ow]"

# Find space hogs fast
du -sh /var/log /tmp /var/lib/docker /var/lib/mysql 2>/dev/null

# Find largest files created in last 24 hours
find / -type f -mtime -1 -size +50M -exec ls -lh {} \; 2>/dev/null | sort -k5 -rh | head -10

# Check deleted but open files
lsof +L1 2>/dev/null | awk '{sum+=$7} END {printf "Deleted but open: %.0f MB\n", sum/1024/1024}'
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmDiskUsageCritical")'
```

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| Large log files | Truncate: `> /var/log/large.log` | On-call DevOps |
| Deleted open files | Restart service to release space | On-call DevOps |
| Docker storage | `docker system prune -af --volumes` | On-call DevOps |
| Inode exhaustion | Find and remove small file directories | On-call DevOps |
| Read-only filesystem | `fsck` (requires downtime) or replace | Team Lead + China HQ |
| Need more space NOW | Extend EBS volume online | On-call DevOps (emergency) |

```bash
# EMERGENCY CLEANUP â€” execute in order of impact
# ç´§æ€¥æ¸…ç† â€” æŒ‰å½±å“é¡ºåºæ‰§è¡Œ

# 1. Truncate large log files (don't delete â€” keeps fd open)
# 1. æˆªæ–­å¤§æ—¥å¿—æ–‡ä»¶ï¼ˆä¸è¦åˆ é™¤â€”â€”ä¿æŒfdæ‰“å¼€ï¼‰
find /var/log -name "*.log" -size +100M -exec sh -c '> "$1"' _ {} \;

# 2. Clean old logs
# 2. æ¸…ç†æ—§æ—¥å¿—
find /var/log -name "*.gz" -delete
journalctl --vacuum-size=100M

# 3. Clean temp files
# 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find /tmp -type f -mtime +1 -delete

# 4. Clean Docker (if applicable)
# 4. æ¸…ç†Dockerï¼ˆå¦‚é€‚ç”¨ï¼‰
docker system prune -af --volumes 2>/dev/null

# 5. If still critical, extend EBS volume
# 5. å¦‚ä»ç„¶ä¸¥é‡ï¼Œæ‰©å±•EBSå·
# aws ec2 modify-volume --volume-id <vol-id> --size <new-size>
# sudo growpart /dev/xvda 1 && sudo resize2fs /dev/xvda1

# Verify
df -h /
```

**Escalation / å‡çº§:**
```
Tier 3 (Critical) â€” All US DevOps + China HQ notified immediately
If read-only filesystem â†’ May require instance replacement
å¦‚æœåªè¯»æ–‡ä»¶ç³»ç»Ÿ â†’ å¯èƒ½éœ€è¦æ›¿æ¢å®ä¾‹
```

### 5. AFTERMATH / å–„å

- [ ] Verify disk < 85% / éªŒè¯ç£ç›˜ä½äº85%
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary / å‘å¸ƒè§£å†³æ‘˜è¦
- [ ] Set up logrotate if missing / å¦‚ç¼ºå¤±åˆ™è®¾ç½®logrotate
- [ ] Set up disk usage monitoring cron job / è®¾ç½®ç£ç›˜ä½¿ç”¨ç›‘æ§å®šæ—¶ä»»åŠ¡
- [ ] Conduct post-incident review / è¿›è¡Œäº‹åå›é¡¾
- [ ] File capacity planning ticket for volume extension / æäº¤å·æ‰©å±•å®¹é‡è§„åˆ’å·¥å•
- [ ] Related alerts: VM-05 (Disk Warning) should resolve / ç›¸å…³å‘Šè­¦ï¼šVM-05 (ç£ç›˜è­¦å‘Š) åº”æ¢å¤

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-104 (Disk Full), ALR-105 (Inode Full), ALR-111 (Disk > 85%) â†’ Merged into LCK-VM-006

---

<a id="lck-vm-007"></a>
## VM-07: VmNetworkErrorsWarning / VM ç½‘ç»œé”™è¯¯ï¼ˆè­¦å‘Šçº§ï¼‰

```yaml
alert_id: LCK-VM-007
alert_name: VmNetworkErrorsWarning
old_ids: [ALR-108, ALR-112, ALR-113, ALR-114, ALR-115]
consolidation: MERGE
severity: warning
tier: "2"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call
sla_response: 15 min acknowledge / 1 hour first update / 4 hours resolution
notification_channel: wecom+twilio-lead
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-network
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmNetworkErrorsWarning
expr: lckna:vm:net_errors_rate5m > 200 or rate(node_network_receive_drop_total{env="production"}[5m]) > 20
for: 5m
labels:
  severity: warning
  tier: "2"
  category: infra-vm
  team: sys-ops
  dashboard: vm-network
annotations:
  summary: "VM network errors on {{ $labels.instance }}"
  description: "Network errors ({{ $value }}/s) or packet drops exceed threshold on {{ $labels.instance }}."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-network-errors"
  dashboard_url: "https://grafana.luckinus.com/d/vm-network"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
# Two conditions (OR):
# 1. Network errors (rx+tx) > 200/s
lckna:vm:net_errors_rate5m > 200

# 2. Packet drops > 20/s
rate(node_network_receive_drop_total{env="production"}[5m]) > 20
```

**Meaning / å«ä¹‰:** The production VM is experiencing either high network error rates (> 200 errors/second for TX+RX combined) or significant packet drops (> 20 drops/second) for at least 5 minutes. This may indicate NIC issues, MTU mismatches, network congestion, or underlying infrastructure problems. Can cause application timeouts and failed requests.

**å«ä¹‰ï¼š** ç”Ÿäº§ç¯å¢ƒè™šæ‹Ÿæœºæ­£ç»å†é«˜ç½‘ç»œé”™è¯¯ç‡ï¼ˆTX+RXåˆè®¡ > 200é”™è¯¯/ç§’ï¼‰æˆ–æ˜¾è‘—ä¸¢åŒ…ï¼ˆ> 20ä¸¢åŒ…/ç§’ï¼‰æŒç»­è‡³å°‘5åˆ†é’Ÿã€‚è¿™å¯èƒ½è¡¨ç¤ºç½‘å¡é—®é¢˜ã€MTUä¸åŒ¹é…ã€ç½‘ç»œæ‹¥å¡æˆ–åº•å±‚åŸºç¡€è®¾æ–½é—®é¢˜ã€‚å¯èƒ½å¯¼è‡´åº”ç”¨è¶…æ—¶å’Œè¯·æ±‚å¤±è´¥ã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
INSTANCE="{{ $labels.instance }}"
# Check which condition triggered
curl -s "http://localhost:9090/api/v1/query?query=lckna:vm:net_errors_rate5m{instance='${INSTANCE}'}" | jq '.data.result[0].value[1]'
curl -s "http://localhost:9090/api/v1/query?query=rate(node_network_receive_drop_total{instance='${INSTANCE}',env='production'}[5m])" | jq '.data.result[0].value[1]'
```

#### 1.3 Severity Classification / ä¸¥é‡ç¨‹åº¦åˆ†ç±»

| Condition / æ¡ä»¶ | Classification / åˆ†ç±» | Action / æ“ä½œ |
|---|---|---|
| Errors 200-500/s, no app impact | Low Warning / ä½çº§è­¦å‘Š | Monitor, investigate when convenient |
| Errors > 500/s or drops > 50/s | High Warning / é«˜çº§è­¦å‘Š | Investigate immediately |
| Multiple instances affected | Potential infra issue / æ½œåœ¨åŸºç¡€è®¾æ–½é—®é¢˜ | Escalate to AWS support |
| Application timeouts observed | â†’ Treat as Tier 3 / æŒ‰Tier 3å¤„ç† | Immediate escalation |

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 15 min / 15 åˆ†é’Ÿå†…)

```bash
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="Investigating VM network errors on ${INSTANCE}" \
  --duration=1h \
  alertname=VmNetworkErrorsWarning instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸŸ¡ [ACKNOWLEDGED] VM-07: VmNetworkErrorsWarning
Instance: {{ $labels.instance }}
Error Rate: {{ $value }}/s
Responder: [Your Name]
ETA: Investigating, update in 30 min
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **MTU mismatch / MTUä¸åŒ¹é…:** Jumbo frames between VPC and external networks
- **NIC driver issue / ç½‘å¡é©±åŠ¨é—®é¢˜:** ENA driver bug or misconfiguration
- **Network congestion / ç½‘ç»œæ‹¥å¡:** Bandwidth saturation on the instance
- **Security group rate limit / å®‰å…¨ç»„é€Ÿç‡é™åˆ¶:** Exceeding connection tracking limits
- **AWS infrastructure issue / AWSåŸºç¡€è®¾æ–½é—®é¢˜:** Underlying host or network issue
- **TCP retransmissions / TCPé‡ä¼ :** Application-level timeout/retry storms

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
ssh ec2-user@${INSTANCE%%:*}

# Network interface statistics
# ç½‘ç»œæ¥å£ç»Ÿè®¡
ip -s link show

# Detailed error breakdown
# è¯¦ç»†é”™è¯¯åˆ†è§£
ethtool -S eth0 2>/dev/null | grep -i "err\|drop\|timeout\|reset"

# Check MTU
# æ£€æŸ¥MTU
ip link show | grep mtu

# TCP connection stats
# TCPè¿æ¥ç»Ÿè®¡
ss -s

# Check for TCP retransmissions
# æ£€æŸ¥TCPé‡ä¼ 
netstat -s | grep -i "retransmit\|timeout\|reset\|overflow"

# Check conntrack table (if applicable)
# æ£€æŸ¥è¿æ¥è·Ÿè¸ªè¡¨ï¼ˆå¦‚é€‚ç”¨ï¼‰
cat /proc/sys/net/netfilter/nf_conntrack_count 2>/dev/null
cat /proc/sys/net/netfilter/nf_conntrack_max 2>/dev/null

# Bandwidth check
# å¸¦å®½æ£€æŸ¥
sar -n DEV 1 5 2>/dev/null || (apt-get install -y sysstat && sar -n DEV 1 5)

# Check for DNS resolution failures
# æ£€æŸ¥DNSè§£æå¤±è´¥
dig +short google.com @169.254.169.253
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmNetworkErrorsWarning")'
```

#### 3.4 PromQL Deep Dive / PromQL æ·±å…¥æŸ¥è¯¢

```promql
# Error rate by interface
rate(node_network_receive_errs_total{instance="${INSTANCE}", env="production"}[5m])
rate(node_network_transmit_errs_total{instance="${INSTANCE}", env="production"}[5m])

# Drop rate
rate(node_network_receive_drop_total{instance="${INSTANCE}", env="production"}[5m])
rate(node_network_transmit_drop_total{instance="${INSTANCE}", env="production"}[5m])

# Bandwidth utilization
rate(node_network_receive_bytes_total{instance="${INSTANCE}", env="production"}[5m]) * 8
rate(node_network_transmit_bytes_total{instance="${INSTANCE}", env="production"}[5m]) * 8
```

**Dashboard:** [VM Network](https://grafana.luckinus.com/d/vm-network)

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| MTU mismatch | Adjust MTU: `ip link set eth0 mtu 1500` | On-call DevOps |
| NIC driver issue | Update ENA driver or reboot | Team Lead approval |
| Bandwidth saturation | Rate limit applications or scale | On-call DevOps + App team |
| Conntrack table full | Increase `nf_conntrack_max` | On-call DevOps |
| AWS infrastructure issue | Open AWS support case (Severity 1) | Team Lead |
| Multiple instances affected | Declare incident, AWS support | Team Lead + China HQ |

```bash
# Increase conntrack limit (temporary fix)
# å¢åŠ è¿æ¥è·Ÿè¸ªé™åˆ¶ï¼ˆä¸´æ—¶ä¿®å¤ï¼‰
echo 262144 > /proc/sys/net/netfilter/nf_conntrack_max

# Fix MTU if needed
# å¦‚éœ€è¦ä¿®å¤MTU
ip link set eth0 mtu 1500

# Check if errors cleared
# æ£€æŸ¥é”™è¯¯æ˜¯å¦æ¸…é™¤
sleep 60 && ethtool -S eth0 2>/dev/null | grep -i "err\|drop"
```

### 5. AFTERMATH / å–„å

- [ ] Verify error rate < 200/s and drops < 20/s / éªŒè¯é”™è¯¯ç‡ < 200/s ä¸”ä¸¢åŒ… < 20/s
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary / å‘å¸ƒè§£å†³æ‘˜è¦
- [ ] If MTU changed: make persistent in network config / å¦‚ä¿®æ”¹äº†MTUï¼šåœ¨ç½‘ç»œé…ç½®ä¸­æŒä¹…åŒ–
- [ ] If conntrack changed: add to sysctl.conf / å¦‚ä¿®æ”¹äº†conntrackï¼šæ·»åŠ åˆ°sysctl.conf
- [ ] If AWS issue: track support case / å¦‚AWSé—®é¢˜ï¼šè·Ÿè¸ªæ”¯æŒæ¡ˆä¾‹
- [ ] Related alerts: VM-08 (Instance Down) â€” may indicate progression / ç›¸å…³å‘Šè­¦ï¼šVM-08 (å®ä¾‹å®•æœº) â€” å¯èƒ½è¡¨ç¤ºæ¶åŒ–

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-108 (TCP Retransmits), ALR-112 (NIC RX Errors), ALR-113 (NIC TX Errors), ALR-114 (NIC RX Drops), ALR-115 (NIC TX Drops) â†’ Merged into LCK-VM-007

---

<a id="lck-vm-008"></a>
## VM-08: VmInstanceDownCritical / VM å®ä¾‹å®•æœºï¼ˆä¸¥é‡çº§ï¼‰

```yaml
alert_id: LCK-VM-008
alert_name: VmInstanceDownCritical
old_ids: [ALR-110, ALR-116]
consolidation: MERGE
severity: critical
tier: "1"
category: INFRA-VM
team: sys-ops
first_responder: US DevOps On-Call + Team Lead
sla_response: 5 min acknowledge / 15 min first update / 1 hour resolution
notification_channel: wecom+twilio-all
skill_reference: /app/skills/ec2-alert-investigation.md
dashboard: vm-overview
last_updated: 2026-02-16
```

### Alert Rule / æŠ¥è­¦è§„åˆ™

```yaml
alert: VmInstanceDownCritical
expr: up{job="node-exporter", env="production"} == 0
for: 10m
labels:
  severity: critical
  tier: "1"
  category: infra-vm
  team: sys-ops
  dashboard: vm-overview
annotations:
  summary: "VM instance DOWN: {{ $labels.instance }}"
  description: "Instance {{ $labels.instance }} has been unreachable for 10 minutes. Node exporter is not responding."
  runbook_url: "https://runbooks.luckinus.com/infra-vm/vm-instance-down"
  dashboard_url: "https://grafana.luckinus.com/d/vm-overview"
```

### PromQL Expression / PromQL è¡¨è¾¾å¼

```promql
up{job="node-exporter", env="production"} == 0
```

**Meaning / å«ä¹‰:** The `node_exporter` on a production VM has been unreachable for 10 minutes. The instance may be down, the network may be partitioned, or the exporter process may have crashed. This is the highest severity VM alert because a down instance means all services on that host are unavailable. Determine if the instance is truly down or if only monitoring is affected.

**å«ä¹‰ï¼š** ç”Ÿäº§ç¯å¢ƒè™šæ‹Ÿæœºä¸Šçš„ `node_exporter` å·²æ— æ³•è®¿é—®10åˆ†é’Ÿã€‚å®ä¾‹å¯èƒ½å·²å®•æœºã€ç½‘ç»œå¯èƒ½å·²éš”ç¦»ã€æˆ–å¯¼å‡ºå™¨è¿›ç¨‹å¯èƒ½å·²å´©æºƒã€‚è¿™æ˜¯æœ€é«˜ä¸¥é‡çº§åˆ«çš„VMå‘Šè­¦ï¼Œå› ä¸ºå®•æœºçš„å®ä¾‹æ„å‘³ç€è¯¥ä¸»æœºä¸Šçš„æ‰€æœ‰æœåŠ¡éƒ½ä¸å¯ç”¨ã€‚ç¡®å®šå®ä¾‹æ˜¯å¦çœŸæ­£å®•æœºæˆ–ä»…ç›‘æ§å—å½±å“ã€‚

### 1. ASSESS / è¯„ä¼°

#### 1.1 Golden Path Impact Check / é»„é‡‘æµç¨‹å½±å“æ£€æŸ¥

```bash
# CRITICAL: Check golden path FIRST
curl -s "http://localhost:9090/api/v1/query?query=sum_over_time(business_completed_orders_total[10m])" | jq '.data.result[0].value[1]'
```

#### 1.2 Quick Triage / å¿«é€Ÿåˆ†è¯Š

```bash
INSTANCE="{{ $labels.instance }}"
INSTANCE_IP="${INSTANCE%%:*}"

# Can we reach the instance at all?
# èƒ½å¦åˆ°è¾¾å®ä¾‹ï¼Ÿ
ping -c 3 -W 2 ${INSTANCE_IP}

# Can we SSH?
# èƒ½å¦SSHï¼Ÿ
ssh -o ConnectTimeout=5 ec2-user@${INSTANCE_IP} "hostname && uptime" 2>&1

# Check instance state in AWS
# åœ¨AWSä¸­æ£€æŸ¥å®ä¾‹çŠ¶æ€
aws ec2 describe-instances --filters "Name=private-ip-address,Values=${INSTANCE_IP}" \
  --query "Reservations[0].Instances[0].{ID:InstanceId,State:State.Name,StatusChecks:StatusChecks}" --output table
```

#### 1.3 Severity Classification / ä¸¥é‡ç¨‹åº¦åˆ†ç±»

| Condition / æ¡ä»¶ | Classification / åˆ†ç±» | Action / æ“ä½œ |
|---|---|---|
| Instance responding to SSH | node_exporter down only | Restart node_exporter |
| Instance not responding, AWS shows running | Possible kernel panic / å¯èƒ½å†…æ ¸ææ…Œ | Reboot via AWS |
| AWS shows stopped/terminated | Instance actually down | Investigate and restart |
| Multiple instances down | Major incident / é‡å¤§äº‹ä»¶ | Declare incident, all hands |
| Golden path impacted | Emergency / ç´§æ€¥ | All DevOps + China HQ |

### 2. ACKNOWLEDGE / ç¡®è®¤ (Within 5 min / 5 åˆ†é’Ÿå†…)

```bash
amtool silence add \
  --alertmanager.url=http://alertmanager:9093 \
  --author="$(whoami)" \
  --comment="CRITICAL: Instance ${INSTANCE} is DOWN, investigating" \
  --duration=30m \
  alertname=VmInstanceDownCritical instance="${INSTANCE}"
```

**WeCom Notification Template / ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ¨¡æ¿:**
```
ğŸ”´ [CRITICAL] VM-08: VmInstanceDownCritical
Instance: {{ $labels.instance }}
Status: Instance unreachable for 10+ minutes
Responder: [Your Name]
Action: Immediate investigation
ETA: First update in 15 min
```

### 3. ANALYZE / åˆ†æ

#### 3.1 Common Causes / å¸¸è§åŸå› 

- **Instance terminated/stopped / å®ä¾‹ç»ˆæ­¢/åœæ­¢:** Accidental or scheduled termination
- **Kernel panic / å†…æ ¸ææ…Œ:** OS crash requiring hard reboot
- **OOM kill cascade / OOM killçº§è”:** OOM killer killed critical processes including systemd
- **EBS volume issue / EBSå·é—®é¢˜:** Root volume detached or impaired
- **Network partition / ç½‘ç»œåˆ†åŒº:** Security group change or VPC routing issue
- **node_exporter crash / node_exporterå´©æºƒ:** Only monitoring affected, instance is fine
- **AWS maintenance event / AWSç»´æŠ¤äº‹ä»¶:** Scheduled or unscheduled host maintenance

#### 3.2 Diagnostic Commands / è¯Šæ–­å‘½ä»¤

```bash
INSTANCE_IP="${INSTANCE%%:*}"

# AWS Console checks (from bastion/local)
# AWSæ§åˆ¶å°æ£€æŸ¥ï¼ˆä»å ¡å’æœº/æœ¬åœ°ï¼‰

# Get instance details
INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=private-ip-address,Values=${INSTANCE_IP}" \
  --query "Reservations[0].Instances[0].InstanceId" --output text)

echo "Instance ID: ${INSTANCE_ID}"

# Check instance status
aws ec2 describe-instance-status --instance-ids ${INSTANCE_ID} --output table

# Check system and instance status checks
aws ec2 describe-instance-status --instance-ids ${INSTANCE_ID} \
  --query "InstanceStatuses[0].{System:SystemStatus.Status,Instance:InstanceStatus.Status}" --output table

# Check for scheduled events
aws ec2 describe-instance-status --instance-ids ${INSTANCE_ID} \
  --query "InstanceStatuses[0].Events" --output table

# Check CloudWatch system metrics
aws cloudwatch get-metric-statistics --namespace AWS/EC2 \
  --metric-name StatusCheckFailed --dimensions Name=InstanceId,Value=${INSTANCE_ID} \
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%S) --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 60 --statistics Maximum

# If SSH works â€” check node_exporter
# å¦‚æœSSHå¯ç”¨ â€” æ£€æŸ¥node_exporter
ssh ec2-user@${INSTANCE_IP} "
  systemctl status node_exporter
  journalctl -u node_exporter --since '30 minutes ago' --no-pager | tail -20
  curl -s http://localhost:9100/metrics | head -5
"
```

#### 3.3 VMAlert Endpoint Verification / VMAlert èŠ‚ç‚¹éªŒè¯

```bash
curl -s "http://10.238.3.153:8880/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname=="VmInstanceDownCritical")'
```

#### 3.4 Check Related Services / æ£€æŸ¥ç›¸å…³æœåŠ¡

```bash
# What services were running on this instance?
# è¯¥å®ä¾‹ä¸Šè¿è¡Œäº†å“ªäº›æœåŠ¡ï¼Ÿ
# Check Kubernetes pods if this was an EKS node
kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${INSTANCE_IP} 2>/dev/null

# Check if other instances in same AZ are affected
# æ£€æŸ¥åŒä¸€å¯ç”¨åŒºçš„å…¶ä»–å®ä¾‹æ˜¯å¦å—å½±å“
aws ec2 describe-instances --instance-ids ${INSTANCE_ID} \
  --query "Reservations[0].Instances[0].Placement.AvailabilityZone" --output text
```

### 4. ACT / å¤„ç½®

| Scenario / åœºæ™¯ | Action / æ“ä½œ | Authority / æƒé™ |
|---|---|---|
| node_exporter down only | `ssh & systemctl restart node_exporter` | On-call DevOps |
| Instance hung (SSH timeout) | Reboot: `aws ec2 reboot-instances` | On-call DevOps |
| Instance stopped | Start: `aws ec2 start-instances` | On-call DevOps |
| Instance terminated | Investigate, launch replacement | Team Lead |
| System status check failed | Stop & start (moves to new host) | On-call DevOps |
| Kernel panic (serial console) | Force reboot: `aws ec2 stop-instances --force` | Team Lead |
| Multiple instances / AZ issue | Declare incident, failover to other AZ | Team Lead + China HQ |

```bash
# Restart node_exporter (if SSH works)
# é‡å¯node_exporterï¼ˆå¦‚æœSSHå¯ç”¨ï¼‰
ssh ec2-user@${INSTANCE_IP} "sudo systemctl restart node_exporter && systemctl status node_exporter"

# Reboot instance (if instance is hung)
# é‡å¯å®ä¾‹ï¼ˆå¦‚æœå®ä¾‹æŒ‚èµ·ï¼‰
aws ec2 reboot-instances --instance-ids ${INSTANCE_ID}
echo "Waiting for instance to reboot..."
aws ec2 wait instance-status-ok --instance-ids ${INSTANCE_ID}

# Force stop and start (moves to new host hardware)
# å¼ºåˆ¶åœæ­¢å’Œå¯åŠ¨ï¼ˆè¿ç§»åˆ°æ–°ä¸»æœºç¡¬ä»¶ï¼‰
# WARNING: IP address may change if not using Elastic IP
# è­¦å‘Šï¼šå¦‚æœæœªä½¿ç”¨å¼¹æ€§IPï¼ŒIPåœ°å€å¯èƒ½æ”¹å˜
# aws ec2 stop-instances --instance-ids ${INSTANCE_ID} --force
# aws ec2 wait instance-stopped --instance-ids ${INSTANCE_ID}
# aws ec2 start-instances --instance-ids ${INSTANCE_ID}

# Start a stopped instance
# å¯åŠ¨å·²åœæ­¢çš„å®ä¾‹
# aws ec2 start-instances --instance-ids ${INSTANCE_ID}
# aws ec2 wait instance-running --instance-ids ${INSTANCE_ID}

# Verify instance is back and node_exporter responding
# éªŒè¯å®ä¾‹æ¢å¤ä¸”node_exporterå“åº”
sleep 60
curl -s "http://${INSTANCE_IP}:9100/metrics" | head -5 && echo "node_exporter is UP" || echo "node_exporter still DOWN"
```

**Escalation / å‡çº§:**
```
Tier 3 (Critical) â€” All US DevOps + China HQ notified immediately
Multiple instances down â†’ Major incident declaration
å¤šå®ä¾‹å®•æœº â†’ å®£å¸ƒé‡å¤§äº‹ä»¶

If AZ-level issue â†’ AWS support case (Severity 1 / Business Critical)
å¦‚æœæ˜¯å¯ç”¨åŒºçº§åˆ«é—®é¢˜ â†’ AWSæ”¯æŒæ¡ˆä¾‹ï¼ˆä¸¥é‡æ€§1/ä¸šåŠ¡å…³é”®ï¼‰
```

### 5. AFTERMATH / å–„å

- [ ] Verify `up{instance="${INSTANCE}"}` == 1 for 15 minutes / éªŒè¯å®ä¾‹æ¢å¤15åˆ†é’Ÿ
- [ ] Remove alert silence / ç§»é™¤å‘Šè­¦é™é»˜
- [ ] Post WeCom resolution summary with root cause / å‘å¸ƒå«æ ¹å› çš„è§£å†³æ‘˜è¦
- [ ] Verify all services on the instance are healthy / éªŒè¯å®ä¾‹ä¸Šæ‰€æœ‰æœåŠ¡å¥åº·
- [ ] Check for any data loss or inconsistency / æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ä¸¢å¤±æˆ–ä¸ä¸€è‡´
- [ ] If instance rebooted: check system logs for crash cause / å¦‚é‡å¯äº†å®ä¾‹ï¼šæ£€æŸ¥ç³»ç»Ÿæ—¥å¿—æŸ¥æ‰¾å´©æºƒåŸå› 
  ```bash
  ssh ec2-user@${INSTANCE_IP} "journalctl --since '1 hour ago' | grep -iE 'panic|error|crash|oom|killed' | head -20"
  ```
- [ ] If replaced/started: update monitoring config if IP changed / å¦‚æ›¿æ¢/å¯åŠ¨ï¼šå¦‚IPå˜åŒ–åˆ™æ›´æ–°ç›‘æ§é…ç½®
- [ ] Conduct post-incident review for prolonged outage / å¯¹é•¿æ—¶é—´ä¸­æ–­è¿›è¡Œäº‹åå›é¡¾
- [ ] Update runbook if new failure mode discovered / å¦‚å‘ç°æ–°æ•…éšœæ¨¡å¼åˆ™æ›´æ–°è¿ç»´æ‰‹å†Œ
- [ ] Related alerts: ALL VM alerts for this instance should resolve when instance recovers / ç›¸å…³å‘Šè­¦ï¼šè¯¥å®ä¾‹çš„æ‰€æœ‰VMå‘Šè­¦åœ¨å®ä¾‹æ¢å¤æ—¶åº”å…¨éƒ¨æ¢å¤

**Old Alert Reference / æ—§å‘Šè­¦å‚è€ƒ:** ALR-110 (Instance Heartbeat Lost), ALR-116 (NIC Down) â†’ Merged into LCK-VM-008

---

## Cross-Reference: Old Alert ID Mapping / æ—§å‘Šè­¦IDæ˜ å°„

| Old Alert ID | Old Alert Name | New Alert ID | New Alert Name |
|---|---|---|---|
| ALR-100 | EC2 CPU > 80% | LCK-VM-001 | VmCpuUsageWarning |
| ALR-101 | EC2 CPU > 85% | LCK-VM-001 | VmCpuUsageWarning |
| ALR-102 | EC2 CPU > 90% | LCK-VM-002 | VmCpuUsageCritical |
| ALR-103 | EC2 CPU > 95% | LCK-VM-002 | VmCpuUsageCritical |
| ALR-104 | Disk Full | LCK-VM-006 | VmDiskUsageCritical |
| ALR-105 | Inode Full | LCK-VM-006 | VmDiskUsageCritical |
| ALR-106 | CPU IOWait > 80% | LCK-VM-002 | VmCpuUsageCritical |
| ALR-107 | CPU Load > Cores | LCK-VM-001 | VmCpuUsageWarning |
| ALR-108 | TCP Retransmits | LCK-VM-007 | VmNetworkErrorsWarning |
| ALR-109 | Host Memory > 85% | LCK-VM-003 / LCK-VM-004 | VmMemoryUsageWarning / VmMemoryUsageCritical |
| ALR-110 | Instance Heartbeat Lost | LCK-VM-008 | VmInstanceDownCritical |
| ALR-111 | Disk Usage > 85% | LCK-VM-005 / LCK-VM-006 | VmDiskUsageWarning / VmDiskUsageCritical |
| ALR-112 | NIC RX Errors | LCK-VM-007 | VmNetworkErrorsWarning |
| ALR-113 | NIC TX Errors | LCK-VM-007 | VmNetworkErrorsWarning |
| ALR-114 | NIC RX Drops | LCK-VM-007 | VmNetworkErrorsWarning |
| ALR-115 | NIC TX Drops | LCK-VM-007 | VmNetworkErrorsWarning |
| ALR-116 | NIC Down | LCK-VM-008 | VmInstanceDownCritical |

---

*End of Part 7: INFRA-VM (VM/Host) â€” 8 alerts covering CPU, Memory, Disk, Network, and Instance availability.*
*ç¬¬7éƒ¨åˆ†ç»“æŸï¼šåŸºç¡€è®¾æ–½-è™šæ‹Ÿæœº/ä¸»æœº â€” 8ä¸ªå‘Šè­¦è¦†ç›–CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œå’Œå®ä¾‹å¯ç”¨æ€§ã€‚*
