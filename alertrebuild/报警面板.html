<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>瑞幸北美告警梳理面板</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Noto+Sans+SC:wght@400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/marked/9.1.6/marked.min.js"></script>
    <style>
        :root {
            --primary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            --secondary-gradient: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            --success-gradient: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            --warning-gradient: linear-gradient(135deg, #F2994A 0%, #F2C94C 100%);
            
            --bg-primary: #0f1419;
            --bg-secondary: #192734;
            --bg-card: #22303c;
            --bg-hover: #2d4050;
            
            --text-primary: #ffffff;
            --text-secondary: #8899a6;
            --text-muted: #657786;
            
            --border-color: #38444d;
            --border-light: rgba(255,255,255,0.1);
            
            --p0-color: #ef4444;
            --p0-bg: linear-gradient(135deg, #ff6b6b 0%, #ee5a5a 100%);
            --p0-light: rgba(239, 68, 68, 0.15);
            --p1-color: #f97316;
            --p1-bg: linear-gradient(135deg, #ffa726 0%, #fb8c00 100%);
            --p1-light: rgba(249, 115, 22, 0.15);
            --p2-color: #eab308;
            --p2-bg: linear-gradient(135deg, #fdd835 0%, #f9a825 100%);
            --p2-light: rgba(234, 179, 8, 0.15);
            --p3-color: #22c55e;
            --p3-bg: linear-gradient(135deg, #66bb6a 0%, #43a047 100%);
            --p3-light: rgba(34, 197, 94, 0.15);
            
            --luckin-blue: #00529B;
            --luckin-gradient: linear-gradient(135deg, #00529B 0%, #002F5D 100%);
            
            --shadow-sm: 0 2px 4px rgba(0,0,0,0.2);
            --shadow-md: 0 4px 12px rgba(0,0,0,0.3);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.4);
            
            --radius-sm: 8px;
            --radius-md: 12px;
            --radius-lg: 16px;
            --radius-xl: 20px;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: 'Noto Sans SC', 'Inter', -apple-system, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            min-height: 100vh;
            line-height: 1.6;
        }

        /* Header */
        .header {
            background: var(--bg-secondary);
            padding: 0.875rem 1.5rem;
            position: sticky;
            top: 0;
            z-index: 100;
            border-bottom: 1px solid var(--border-color);
        }

        .header-content {
            max-width: 1800px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 280px 1fr 280px;
            align-items: center;
            gap: 1.5rem;
        }

        .header-brand {
            display: flex;
            align-items: center;
            gap: 0.75rem;
        }

        .brand-title {
            font-size: 1.1rem;
            font-weight: 700;
            background: linear-gradient(135deg, #667eea 0%, #f093fb 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .search-container {
            max-width: 600px;
            justify-self: center;
            width: 100%;
        }

        .search-wrapper {
            position: relative;
        }

        .search-input {
            width: 100%;
            padding: 0.75rem 1rem 0.75rem 3rem;
            border: 2px solid var(--border-color);
            border-radius: var(--radius-xl);
            font-size: 0.95rem;
            font-family: inherit;
            background: var(--bg-card);
            color: var(--text-primary);
            transition: all 0.3s ease;
        }

        .search-input::placeholder { color: var(--text-muted); }
        .search-input:focus {
            outline: none;
            border-color: #667eea;
            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.2);
        }

        .search-icon {
            position: absolute;
            left: 1rem;
            top: 50%;
            transform: translateY(-50%);
            color: var(--text-muted);
            width: 20px;
            height: 20px;
        }

        .header-stats {
            display: flex;
            gap: 0.5rem;
            justify-content: flex-end;
        }

        .stat-badge {
            padding: 0.375rem 0.75rem;
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-md);
            font-size: 0.75rem;
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            gap: 0.375rem;
        }

        .stat-badge .count {
            font-family: 'Fira Code', monospace;
            font-weight: 600;
            color: var(--text-primary);
        }

        /* Main Layout */
        .main-container {
            display: flex;
            min-height: calc(100vh - 60px);
        }

        /* Sidebar */
        .sidebar {
            width: 260px;
            background: var(--bg-secondary);
            border-right: 1px solid var(--border-color);
            padding: 1rem 0;
            overflow-y: auto;
            height: calc(100vh - 60px);
            position: sticky;
            top: 60px;
        }

        .nav-section { margin-bottom: 0.25rem; }

        .nav-header {
            padding: 0.625rem 1rem;
            font-weight: 600;
            font-size: 0.7rem;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 1px;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            transition: all 0.2s;
        }

        .nav-header:hover { color: var(--text-secondary); background: var(--bg-card); }
        .nav-header .arrow { transition: transform 0.3s; font-size: 0.6rem; }
        .nav-header.expanded .arrow { transform: rotate(90deg); }

        .nav-body { display: none; padding: 0.25rem 0.5rem; }
        .nav-body.expanded { display: block; }

        .nav-item {
            padding: 0.5rem 0.75rem;
            margin: 0.125rem 0;
            cursor: pointer;
            font-size: 0.8rem;
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            gap: 0.625rem;
            transition: all 0.2s;
            border-radius: var(--radius-sm);
        }

        .nav-item:hover { background: var(--bg-card); color: var(--text-primary); }
        .nav-item.active {
            background: rgba(102, 126, 234, 0.2);
            color: #a5b4fc;
            border-left: 3px solid #667eea;
        }

        .nav-item .count {
            margin-left: auto;
            font-size: 0.65rem;
            padding: 0.125rem 0.375rem;
            background: var(--bg-card);
            border-radius: 10px;
            color: var(--text-muted);
            font-family: 'Fira Code', monospace;
        }

        .priority-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            flex-shrink: 0;
        }

        .priority-dot.p0 { background: var(--p0-color); box-shadow: 0 0 6px var(--p0-color); }
        .priority-dot.p1 { background: var(--p1-color); box-shadow: 0 0 6px var(--p1-color); }
        .priority-dot.p2 { background: var(--p2-color); box-shadow: 0 0 6px var(--p2-color); }
        .priority-dot.p3 { background: var(--p3-color); box-shadow: 0 0 6px var(--p3-color); }

        /* Content Area */
        .content-area {
            flex: 1;
            padding: 1.25rem;
            overflow-y: auto;
            min-height: calc(100vh - 60px);
        }

        /* View Navigation */
        .view-nav {
            display: flex;
            gap: 0.5rem;
            margin-bottom: 1.25rem;
            flex-wrap: wrap;
        }

        .view-btn {
            padding: 0.5rem 1rem;
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-md);
            font-size: 0.8rem;
            font-family: inherit;
            font-weight: 500;
            color: var(--text-secondary);
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 0.375rem;
            transition: all 0.2s ease;
        }

        .view-btn:hover { background: var(--bg-hover); color: var(--text-primary); }
        .view-btn.active {
            background: var(--primary-gradient);
            border-color: transparent;
            color: white;
        }

        /* Card Grid */
        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(360px, 1fr));
            gap: 1rem;
        }

        .alert-card {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-lg);
            padding: 1rem;
            cursor: pointer;
            transition: all 0.25s ease;
            position: relative;
            overflow: hidden;
        }

        .alert-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 3px;
        }

        .alert-card.p0::before { background: var(--p0-bg); }
        .alert-card.p1::before { background: var(--p1-bg); }
        .alert-card.p2::before { background: var(--p2-bg); }
        .alert-card.p3::before { background: var(--p3-bg); }

        .alert-card:hover {
            border-color: #667eea;
            transform: translateY(-2px);
            box-shadow: var(--shadow-lg);
        }

        .alert-card.selected {
            border-color: #667eea;
            box-shadow: 0 0 0 2px rgba(102, 126, 234, 0.3);
        }

        .card-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 0.625rem;
            gap: 0.5rem;
        }

        .card-id {
            font-size: 0.65rem;
            font-family: 'Fira Code', monospace;
            color: var(--text-muted);
            margin-bottom: 0.25rem;
        }

        .card-title {
            font-size: 0.875rem;
            font-weight: 600;
            color: var(--text-primary);
            line-height: 1.4;
        }

        .priority-badge {
            padding: 0.2rem 0.5rem;
            border-radius: 12px;
            font-size: 0.65rem;
            font-weight: 700;
            font-family: 'Fira Code', monospace;
            flex-shrink: 0;
        }

        .priority-badge.p0 { background: var(--p0-light); color: var(--p0-color); }
        .priority-badge.p1 { background: var(--p1-light); color: var(--p1-color); }
        .priority-badge.p2 { background: var(--p2-light); color: var(--p2-color); }
        .priority-badge.p3 { background: var(--p3-light); color: var(--p3-color); }

        .card-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 0.375rem;
            margin-top: 0.75rem;
        }

        .meta-tag {
            padding: 0.2rem 0.5rem;
            background: var(--bg-hover);
            border-radius: var(--radius-sm);
            font-size: 0.7rem;
            color: var(--text-secondary);
        }

        .meta-tag.category { background: rgba(102, 126, 234, 0.15); color: #a5b4fc; }
        .meta-tag.team { background: rgba(240, 147, 251, 0.15); color: #f9a8d4; }

        /* Hierarchy View */
        .hierarchy-view { display: none; }
        .hierarchy-view.active { display: block; }

        .hierarchy-container {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
            gap: 1rem;
        }

        .hierarchy-category {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-lg);
            overflow: hidden;
        }

        .hierarchy-category-header {
            padding: 0.875rem 1rem;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .hierarchy-category:nth-child(8n+1) .hierarchy-category-header { background: linear-gradient(135deg, rgba(102, 126, 234, 0.25) 0%, rgba(102, 126, 234, 0.05) 100%); }
        .hierarchy-category:nth-child(8n+2) .hierarchy-category-header { background: linear-gradient(135deg, rgba(240, 147, 251, 0.25) 0%, rgba(240, 147, 251, 0.05) 100%); }
        .hierarchy-category:nth-child(8n+3) .hierarchy-category-header { background: linear-gradient(135deg, rgba(79, 172, 254, 0.25) 0%, rgba(79, 172, 254, 0.05) 100%); }
        .hierarchy-category:nth-child(8n+4) .hierarchy-category-header { background: linear-gradient(135deg, rgba(250, 112, 154, 0.25) 0%, rgba(250, 112, 154, 0.05) 100%); }
        .hierarchy-category:nth-child(8n+5) .hierarchy-category-header { background: linear-gradient(135deg, rgba(168, 224, 99, 0.25) 0%, rgba(168, 224, 99, 0.05) 100%); }
        .hierarchy-category:nth-child(8n+6) .hierarchy-category-header { background: linear-gradient(135deg, rgba(245, 175, 25, 0.25) 0%, rgba(245, 175, 25, 0.05) 100%); }
        .hierarchy-category:nth-child(8n+7) .hierarchy-category-header { background: linear-gradient(135deg, rgba(18, 194, 233, 0.25) 0%, rgba(18, 194, 233, 0.05) 100%); }
        .hierarchy-category:nth-child(8n+8) .hierarchy-category-header { background: linear-gradient(135deg, rgba(196, 113, 237, 0.25) 0%, rgba(196, 113, 237, 0.05) 100%); }

        .hierarchy-category-title {
            font-weight: 600;
            font-size: 0.9rem;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .hierarchy-category-count {
            font-size: 0.7rem;
            padding: 0.125rem 0.375rem;
            background: rgba(255,255,255,0.15);
            border-radius: 10px;
        }

        .hierarchy-toggle { color: var(--text-muted); font-size: 0.7rem; transition: transform 0.3s; }
        .hierarchy-category.expanded .hierarchy-toggle { transform: rotate(180deg); }

        .hierarchy-category-body {
            display: none;
            padding: 0.5rem;
            max-height: 350px;
            overflow-y: auto;
        }

        .hierarchy-category.expanded .hierarchy-category-body { display: block; }

        .hierarchy-alert {
            padding: 0.5rem 0.75rem;
            margin: 0.25rem 0;
            background: var(--bg-hover);
            border-radius: var(--radius-sm);
            font-size: 0.8rem;
            color: var(--text-secondary);
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            transition: all 0.2s;
        }

        .hierarchy-alert:hover { background: rgba(102, 126, 234, 0.2); color: var(--text-primary); }

        /* Relationship View */
        .relationship-view { display: none; }
        .relationship-view.active { display: block; }

        .relationship-container {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-xl);
            padding: 1.5rem;
            min-height: 600px;
        }

        .relationship-header {
            text-align: center;
            margin-bottom: 1.5rem;
        }

        .relationship-title {
            font-size: 1.25rem;
            font-weight: 700;
            background: linear-gradient(135deg, #667eea 0%, #f093fb 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .relationship-subtitle { color: var(--text-muted); font-size: 0.8rem; margin-top: 0.25rem; }

        .tree-wrapper {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 1.5rem;
        }

        .tree-level {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 0.75rem;
            width: 100%;
        }

        .tree-node {
            padding: 0.75rem 1.25rem;
            border-radius: var(--radius-md);
            cursor: pointer;
            transition: all 0.3s ease;
            text-align: center;
            min-width: 120px;
        }

        .tree-node.root {
            background: var(--primary-gradient);
            color: white;
            font-weight: 700;
            font-size: 1rem;
            padding: 1rem 2rem;
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.4);
        }

        .tree-node.team {
            background: var(--bg-hover);
            border: 2px solid var(--border-color);
            color: var(--text-primary);
            font-weight: 600;
            font-size: 0.85rem;
        }

        .tree-node.team:nth-child(10n+1) { border-color: #667eea; }
        .tree-node.team:nth-child(10n+2) { border-color: #f093fb; }
        .tree-node.team:nth-child(10n+3) { border-color: #4facfe; }
        .tree-node.team:nth-child(10n+4) { border-color: #fa709a; }
        .tree-node.team:nth-child(10n+5) { border-color: #a8e063; }
        .tree-node.team:nth-child(10n+6) { border-color: #f5af19; }
        .tree-node.team:nth-child(10n+7) { border-color: #12c2e9; }
        .tree-node.team:nth-child(10n+8) { border-color: #c471ed; }
        .tree-node.team:nth-child(10n+9) { border-color: #00d2d3; }
        .tree-node.team:nth-child(10n+10) { border-color: #ff9f43; }

        .tree-node:hover { transform: translateY(-3px); box-shadow: var(--shadow-md); }
        .tree-node.expanded { background: rgba(102, 126, 234, 0.3); }

        .node-count { font-size: 0.65rem; color: var(--text-muted); margin-top: 0.25rem; }

        .category-panel {
            width: 100%;
            background: var(--bg-secondary);
            border-radius: var(--radius-lg);
            padding: 1rem;
            margin-top: 0.5rem;
            display: none;
        }

        .category-panel.expanded { display: block; }

        .category-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            justify-content: center;
        }

        .tree-node.category {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            font-size: 0.8rem;
            padding: 0.5rem 0.875rem;
            min-width: auto;
        }

        .tree-node.category:hover { border-color: #667eea; }

        .alert-panel {
            width: 100%;
            background: var(--bg-hover);
            border-radius: var(--radius-md);
            padding: 0.75rem;
            margin-top: 0.5rem;
            display: none;
            max-height: 300px;
            overflow-y: auto;
        }

        .alert-panel.expanded { display: block; }

        .alert-panel-item {
            padding: 0.5rem 0.75rem;
            background: var(--bg-card);
            border-radius: var(--radius-sm);
            margin: 0.25rem 0;
            font-size: 0.8rem;
            color: var(--text-secondary);
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .alert-panel-item:hover { background: rgba(102, 126, 234, 0.2); color: var(--text-primary); }

        /* Detail View */
        .detail-view { display: none; }
        .detail-view.active { display: block; }

        .detail-header {
            background: var(--luckin-gradient);
            padding: 1.25rem 1.5rem;
            border-radius: var(--radius-xl) var(--radius-xl) 0 0;
        }

        .detail-back {
            display: inline-flex;
            align-items: center;
            gap: 0.375rem;
            color: rgba(255,255,255,0.8);
            font-size: 0.8rem;
            cursor: pointer;
            margin-bottom: 0.75rem;
            padding: 0.375rem 0.625rem;
            background: rgba(255,255,255,0.1);
            border-radius: var(--radius-sm);
            transition: all 0.2s;
        }

        .detail-back:hover { background: rgba(255,255,255,0.2); color: white; }

        .detail-title { font-size: 1.1rem; font-weight: 700; color: white; margin-bottom: 0.5rem; }

        .detail-badges { display: flex; gap: 0.375rem; flex-wrap: wrap; }

        .detail-badge {
            padding: 0.25rem 0.625rem;
            background: rgba(255,255,255,0.15);
            border-radius: var(--radius-sm);
            font-size: 0.75rem;
            color: white;
        }

        .detail-body {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-top: none;
            border-radius: 0 0 var(--radius-xl) var(--radius-xl);
        }

        .detail-tabs {
            display: flex;
            border-bottom: 1px solid var(--border-color);
            background: var(--bg-secondary);
        }

        .detail-tab {
            padding: 0.875rem 1.25rem;
            font-size: 0.85rem;
            font-weight: 500;
            color: var(--text-muted);
            cursor: pointer;
            border-bottom: 2px solid transparent;
            transition: all 0.2s;
        }

        .detail-tab:hover { color: var(--text-secondary); background: var(--bg-card); }
        .detail-tab.active { color: #667eea; border-bottom-color: #667eea; background: var(--bg-card); }

        .tab-content { display: none; padding: 1.25rem; }
        .tab-content.active { display: block; }

        /* Info Grid - Improved Design */
        .info-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 0.875rem;
            margin-bottom: 1.25rem;
        }

        .info-item {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            padding: 0.875rem;
            border-radius: var(--radius-md);
        }

        .info-label {
            font-size: 0.65rem;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 0.375rem;
        }

        .info-value {
            font-size: 0.9rem;
            font-weight: 600;
            color: var(--text-primary);
        }

        /* Expression Section - Better Visibility */
        .expression-section {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-md);
            padding: 1rem;
            margin-top: 1rem;
        }

        .expression-label {
            font-size: 0.7rem;
            font-weight: 600;
            color: #667eea;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 0.625rem;
            display: flex;
            align-items: center;
            gap: 0.375rem;
        }

        .expression-code {
            font-family: 'Fira Code', monospace;
            font-size: 0.8rem;
            background: #1a2634;
            color: #7dd3fc;
            padding: 1rem;
            border-radius: var(--radius-sm);
            overflow-x: auto;
            white-space: pre-wrap;
            word-break: break-all;
            border: 1px solid rgba(125, 211, 252, 0.2);
            line-height: 1.6;
        }

        /* Impact Section */
        .impact-section {
            margin-top: 1.25rem;
            padding-top: 1.25rem;
            border-top: 1px solid var(--border-color);
        }

        .impact-title {
            font-size: 0.8rem;
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: var(--text-primary);
        }

        .impact-services { display: flex; flex-wrap: wrap; gap: 0.5rem; }

        .service-tag {
            padding: 0.375rem 0.75rem;
            background: rgba(102, 126, 234, 0.15);
            border: 1px solid rgba(102, 126, 234, 0.3);
            color: #a5b4fc;
            border-radius: var(--radius-sm);
            font-size: 0.8rem;
            font-family: 'Fira Code', monospace;
        }

        /* Handbook Content */
        .handbook-content {
            font-size: 0.875rem;
            line-height: 1.8;
            color: var(--text-secondary);
        }

        .handbook-content h1 {
            font-size: 1.4rem;
            color: var(--text-primary);
            margin: 1.5rem 0 0.875rem;
            padding-bottom: 0.625rem;
            border-bottom: 2px solid #667eea;
        }

        .handbook-content h2 {
            font-size: 1.15rem;
            color: var(--text-primary);
            margin: 1.25rem 0 0.75rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .handbook-content h3 {
            font-size: 1rem;
            color: var(--text-primary);
            margin: 1rem 0 0.5rem;
        }

        .handbook-content h4 {
            font-size: 0.9rem;
            color: var(--text-secondary);
            margin: 0.875rem 0 0.375rem;
        }

        .handbook-content p { margin: 0.75rem 0; }

        .handbook-content ul, .handbook-content ol {
            margin: 0.75rem 0;
            padding-left: 1.5rem;
        }

        .handbook-content li { margin: 0.375rem 0; }

        .handbook-content code {
            background: rgba(102, 126, 234, 0.15);
            padding: 0.125rem 0.375rem;
            border-radius: 4px;
            font-family: 'Fira Code', monospace;
            font-size: 0.85em;
            color: #a5b4fc;
        }

        .handbook-content pre {
            background: #1a2634;
            padding: 1rem;
            border-radius: var(--radius-md);
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Fira Code', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            border: 1px solid var(--border-color);
        }

        .handbook-content pre code {
            background: none;
            padding: 0;
            color: #7dd3fc;
        }

        .handbook-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            font-size: 0.8rem;
        }

        .handbook-content th, .handbook-content td {
            padding: 0.625rem 0.875rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .handbook-content th {
            background: var(--bg-secondary);
            font-weight: 600;
            color: var(--text-primary);
        }

        .handbook-content blockquote {
            border-left: 4px solid #667eea;
            margin: 1rem 0;
            padding: 0.75rem 1rem;
            background: rgba(102, 126, 234, 0.1);
            color: var(--text-secondary);
            border-radius: 0 var(--radius-sm) var(--radius-sm) 0;
        }

        .handbook-content strong { color: var(--text-primary); font-weight: 600; }

        /* Empty State */
        .empty-state {
            text-align: center;
            padding: 3rem 1.5rem;
            color: var(--text-muted);
        }

        .empty-state-icon { font-size: 3rem; margin-bottom: 0.75rem; opacity: 0.5; }

        /* Scrollbar */
        ::-webkit-scrollbar { width: 6px; height: 6px; }
        ::-webkit-scrollbar-track { background: var(--bg-secondary); }
        ::-webkit-scrollbar-thumb { background: var(--border-color); border-radius: 3px; }
        ::-webkit-scrollbar-thumb:hover { background: var(--text-muted); }

        /* Responsive */
        @media (max-width: 1024px) {
            .sidebar { display: none; }
            .header-content { grid-template-columns: 1fr; gap: 0.75rem; }
            .header-brand { justify-content: center; }
            .header-stats { justify-content: center; }
            .card-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="header-content">
            <div class="header-brand">
                <div class="brand-title">瑞幸北美告警梳理面板</div>
            </div>
            <div class="search-container">
                <div class="search-wrapper">
                    <svg class="search-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="11" cy="11" r="8"></circle>
                        <path d="m21 21-4.35-4.35"></path>
                    </svg>
                    <input type="text" class="search-input" id="searchInput" placeholder="搜索告警名称、类别、团队、服务、PromQL...">
                </div>
            </div>
            <div class="header-stats">
                <div class="stat-badge">告警总数: <span class="count" id="totalCount">135</span></div>
                <div class="stat-badge">显示: <span class="count" id="displayCount">135</span></div>
            </div>
        </div>
    </header>

    <!-- Main Container -->
    <div class="main-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar"></aside>

        <!-- Content Area -->
        <main class="content-area" id="contentArea">
            <!-- View Navigation -->
            <div class="view-nav">
                <button class="view-btn active" data-view="cards" onclick="switchView('cards')">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <rect x="3" y="3" width="7" height="7"></rect>
                        <rect x="14" y="3" width="7" height="7"></rect>
                        <rect x="14" y="14" width="7" height="7"></rect>
                        <rect x="3" y="14" width="7" height="7"></rect>
                    </svg>
                    告警卡片
                </button>
                <button class="view-btn" data-view="hierarchy" onclick="switchView('hierarchy')">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <rect x="3" y="3" width="18" height="18" rx="2" ry="2"></rect>
                        <line x1="3" y1="9" x2="21" y2="9"></line>
                        <line x1="9" y1="21" x2="9" y2="9"></line>
                    </svg>
                    分类层级
                </button>
                <button class="view-btn" data-view="relationship" onclick="switchView('relationship')">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="5" r="3"></circle>
                        <circle cx="5" cy="19" r="3"></circle>
                        <circle cx="19" cy="19" r="3"></circle>
                        <line x1="12" y1="8" x2="5" y2="16"></line>
                        <line x1="12" y1="8" x2="19" y2="16"></line>
                    </svg>
                    关系图
                </button>
            </div>

            <!-- Card View -->
            <div id="cardsView" class="cards-view">
                <div class="card-grid" id="cardGrid"></div>
            </div>

            <!-- Hierarchy View -->
            <div id="hierarchyView" class="hierarchy-view">
                <div class="hierarchy-container" id="hierarchyContainer"></div>
            </div>

            <!-- Relationship View -->
            <div id="relationshipView" class="relationship-view">
                <div class="relationship-container" id="relationshipContainer"></div>
            </div>

            <!-- Detail View -->
            <div id="detailView" class="detail-view">
                <div class="detail-header">
                    <div class="detail-back" onclick="closeDetail()">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <polyline points="15 18 9 12 15 6"></polyline>
                        </svg>
                        返回列表
                    </div>
                    <div class="detail-title" id="detailTitle"></div>
                    <div class="detail-badges" id="detailBadges"></div>
                </div>
                <div class="detail-body">
                    <div class="detail-tabs">
                        <div class="detail-tab active" data-tab="info" onclick="switchTab('info')">告警概览</div>
                        <div class="detail-tab" data-tab="handbook" onclick="switchTab('handbook')">处理手册</div>
                    </div>
                    <div id="infoTab" class="tab-content active"></div>
                    <div id="handbookTab" class="tab-content">
                        <div class="handbook-content" id="handbookContent"></div>
                    </div>
                </div>
            </div>
        </main>
    </div>

    <script>
        // Complete Alert Data - 135 Alerts
        const alertsData = [
            { id: "ALR-001", name: "[LCP-Prod-P0] 生产环境紧急告警", priority: "P0", category: "Priority Levels", team: "全局", metric: "Multi-condition", threshold: "LCP Production Critical", duration: "N/A", expression: `sum(alert_count{priority="P0", env="production"}) > 0`, services: ["订单服务", "支付服务", "商品服务", "库存服务", "会员服务"], handbook: `# ALR-001【LCP-Prod-P0】生产环境紧急告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-001 |
| **告警名称** | [LCP-Prod-P0] P0级别告警 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Priority Levels |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-002", name: "[LCP-Prod-P1] 生产环境高优先级告警", priority: "P1", category: "Priority Levels", team: "全局", metric: "Multi-condition", threshold: "LCP Production High", duration: "N/A", expression: `sum(alert_count{priority="P1", env="production"}) > 0`, services: ["订单服务", "支付服务"], handbook: `# ALR-002【LCP-Prod-P1】生产环境高优先级告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-002 |
| **告警名称** | [LCP-Prod-P1] P1级别告警 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Priority Levels |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-003", name: "[LCP-Prod-P2] 生产环境中优先级告警", priority: "P2", category: "Priority Levels", team: "全局", metric: "Multi-condition", threshold: "LCP Production Medium", duration: "N/A", expression: `sum(alert_count{priority="P2", env="production"}) > 0`, services: [], handbook: `# ALR-003【LCP-Prod-P2】生产环境中优先级告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-003 |
| **告警名称** | [LCP-Prod-P2] P2级别告警 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Priority Levels |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-004", name: "[LCP-Prod-P3] 生产环境低优先级告警", priority: "P3", category: "Priority Levels", team: "全局", metric: "Multi-condition", threshold: "LCP Production Low", duration: "N/A", expression: `sum(alert_count{priority="P3", env="production"}) > 0`, services: [], handbook: `# ALR-004【LCP-Prod-P3】生产环境低优先级告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-004 |
| **告警名称** | [LCP-Prod-P3] P3级别告警 |
| **优先级** | P3 |
| **服务等级** | L2 |
| **类别** | Priority Levels |
| **响应时间** | 低优先级响应（< 2小时） |

---

## 告警描述

此告警属于 **P3** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-005", name: "【Datalink】黄金流程任务延迟告警（白天）", priority: "P0", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Golden Flow Task Delay (Day)", duration: "N/A", expression: `datalink_golden_flow_delay_seconds > 300`, services: ["DataLink", "订单同步", "库存同步"], handbook: `# ALR-005【Datalink】黄金流程任务延迟告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-005 |
| **告警名称** | 【数据链路】黄金流程日间-延迟告警 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | DataLink |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-006", name: "【Datalink】黄金流程任务异常告警（白天）", priority: "P0", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Golden Flow Task Exception (Day)", duration: "N/A", expression: `datalink_golden_flow_exception_count > 0`, services: ["DataLink", "订单同步", "库存同步"], handbook: `# ALR-006【Datalink】黄金流程任务异常告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-006 |
| **告警名称** | 【数据链路】黄金流程日间-异常告警 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | DataLink |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-007", name: "【Datalink】离线核心任务延迟告警（白天）", priority: "P1", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Offline Core Task Delay (Day)", duration: "N/A", expression: `datalink_offline_core_delay_seconds > 600`, services: ["DataLink"], handbook: `# ALR-007【Datalink】离线核心任务延迟告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-007 |
| **告警名称** | 【数据链路】离线核心日间-延迟告警 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | DataLink |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-008", name: "【Datalink】离线核心任务异常告警（白天）", priority: "P1", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Offline Core Task Exception (Day)", duration: "N/A", expression: `datalink_offline_core_exception_count > 0`, services: ["DataLink"], handbook: `# ALR-008【Datalink】离线核心任务异常告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-008 |
| **告警名称** | 【数据链路】离线核心日间-异常告警 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | DataLink |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-009", name: "【Datalink】重要任务延迟告警（白天）", priority: "P2", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Important Task Delay (Day)", duration: "N/A", expression: `datalink_important_delay_seconds > 900`, services: ["DataLink"], handbook: `# ALR-009【Datalink】重要任务延迟告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-009 |
| **告警名称** | 【数据链路】重要任务日间-延迟告警 |
| **优先级** | P1 |
| **服务等级** | L1 |
| **类别** | DataLink |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-010", name: "【Datalink】重要任务异常告警（白天）", priority: "P2", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Important Task Exception (Day)", duration: "N/A", expression: `datalink_important_exception_count > 0`, services: ["DataLink"], handbook: `# ALR-010【Datalink】重要任务异常告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-010 |
| **告警名称** | 【数据链路】重要任务日间-异常告警 |
| **优先级** | P1 |
| **服务等级** | L1 |
| **类别** | DataLink |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-011", name: "【Datalink】离线重要任务延迟告警（白天）", priority: "P2", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Offline Important Task Delay (Day)", duration: "N/A", expression: `datalink_offline_important_delay_seconds > 900`, services: ["DataLink"], handbook: `# ALR-011【Datalink】离线重要任务延迟告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-011 |
| **告警名称** | 【数据链路】离线重要日间-延迟告警 |
| **优先级** | P1 |
| **服务等级** | L1 |
| **类别** | DataLink |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-012", name: "【Datalink】离线重要任务异常告警（白天）", priority: "P2", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Offline Important Task Exception (Day)", duration: "N/A", expression: `datalink_offline_important_exception_count > 0`, services: ["DataLink"], handbook: `# ALR-012【Datalink】离线重要任务异常告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-012 |
| **告警名称** | 【数据链路】离线重要日间-异常告警 |
| **优先级** | P1 |
| **服务等级** | L1 |
| **类别** | DataLink |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-013", name: "【Datalink】任务延迟告警（夜晚）", priority: "P2", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Task Delay (Night)", duration: "N/A", expression: `datalink_night_delay_seconds > 1200`, services: ["DataLink"], handbook: `# ALR-013【Datalink】任务延迟告警（夜晚）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-013 |
| **告警名称** | 【数据链路】任务夜间-延迟告警 |
| **优先级** | P2 |
| **服务等级** | L2 |
| **类别** | DataLink |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-014", name: "【Datalink】任务异常告警（夜晚）", priority: "P2", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Task Exception (Night)", duration: "N/A", expression: `datalink_night_exception_count > 0`, services: ["DataLink"], handbook: `# ALR-014【Datalink】任务异常告警（夜晚）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-014 |
| **告警名称** | 【数据链路】任务夜间-异常告警 |
| **优先级** | P2 |
| **服务等级** | L2 |
| **类别** | DataLink |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-015", name: "【Datalink】普通任务延迟告警（白天）", priority: "P3", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Regular Task Delay (Day)", duration: "N/A", expression: `datalink_regular_delay_seconds > 1800`, services: ["DataLink"], handbook: `# ALR-015【Datalink】普通任务延迟告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-015 |
| **告警名称** | 【数据链路】常规任务日间-延迟告警 |
| **优先级** | P2 |
| **服务等级** | L2 |
| **类别** | DataLink |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-016", name: "【Datalink】普通任务异常告警（白天）", priority: "P3", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Regular Task Exception (Day)", duration: "N/A", expression: `datalink_regular_exception_count > 0`, services: ["DataLink"], handbook: `# ALR-016【Datalink】普通任务异常告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-016 |
| **告警名称** | 【数据链路】常规任务日间-异常告警 |
| **优先级** | P2 |
| **服务等级** | L2 |
| **类别** | DataLink |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-017", name: "【Datalink】离线普通任务延迟告警（白天）", priority: "P3", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Offline Regular Task Delay (Day)", duration: "N/A", expression: `datalink_offline_regular_delay_seconds > 1800`, services: ["DataLink"], handbook: `# ALR-017【Datalink】离线普通任务延迟告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-017 |
| **告警名称** | 【数据链路】离线常规日间-延迟告警 |
| **优先级** | P2 |
| **服务等级** | L2 |
| **类别** | DataLink |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-018", name: "【Datalink】离线普通任务异常告警（白天）", priority: "P3", category: "DataLink", team: "架构数据", metric: "Pipeline", threshold: "Offline Regular Task Exception (Day)", duration: "N/A", expression: `datalink_offline_regular_exception_count > 0`, services: ["DataLink"], handbook: `# ALR-018【Datalink】离线普通任务异常告警（白天）

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-018 |
| **告警名称** | 【数据链路】离线常规日间-异常告警 |
| **优先级** | P2 |
| **服务等级** | L2 |
| **类别** | DataLink |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DataLink任务状态
# 通过DataLink管理后台查看任务执行状态

# 检查Kafka消费者延迟
kafka-consumer-groups.sh --bootstrap-server [KAFKA_BROKER] --describe --group [GROUP_NAME]

# 检查Flink作业状态
# 通过Flink Dashboard查看作业运行状态
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-019", name: "【DB告警】AWS-RDS CPU使用率连续三分钟大于90%", priority: "P1", category: "Database-RDS", team: "DBA", metric: "CPU", threshold: ">90% for 3min", duration: "3m", expression: `aws_rds_cpuutilization_average offset 3m >= 90`, services: ["订单服务", "支付服务", "会员服务", "营销服务"], handbook: `# ALR-019【DB告警】AWS-RDS CPU使用率连续三分钟大于90%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-019 |
| **告警名称** | 【DB告警】AWS-RDS CPU使用率连续三分钟大于90% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS RDS MySQL实例的CPU使用率连续3分钟超过90%，表明数据库负载过高。

### 业务影响

- **黄金流程影响**: 如果是salesorder-rw或salespayment-rw实例，将直接影响用户下单和支付
- **服务降级**: 数据库响应延迟增加，可能导致API超时
- **级联效应**: 连接池可能耗尽，导致其他服务无法获取数据库连接

### 受影响服务

订单服务、支付服务、会员服务、营销服务等依赖该数据库的所有微服务

### PromQL表达式

\`\`\`promql
aws_rds_cpuutilization_average offset 3m >= 90
avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90
\`\`\`

### 常见根因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-020", name: "【DB告警】AWS RDS CPU使用率连续三分钟大于90%", priority: "P1", category: "Database-RDS", team: "DBA", metric: "CPU", threshold: ">90% for 3min", duration: "3m", expression: `avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90`, services: ["订单服务", "支付服务"], handbook: `# ALR-020【DB告警】AWS RDS CPU使用率连续三分钟大于90%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-020 |
| **告警名称** | 【DB告警】AWS RDS CPU使用率连续三分钟大于90% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS RDS MySQL实例的CPU使用率连续3分钟超过90%，表明数据库负载过高。

### 业务影响

- **黄金流程影响**: 如果是salesorder-rw或salespayment-rw实例，将直接影响用户下单和支付
- **服务降级**: 数据库响应延迟增加，可能导致API超时
- **级联效应**: 连接池可能耗尽，导致其他服务无法获取数据库连接

### 受影响服务

订单服务、支付服务、会员服务、营销服务等依赖该数据库的所有微服务

### PromQL表达式

\`\`\`promql
aws_rds_cpuutilization_average offset 3m >= 90
avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90
\`\`\`

### 常见根因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-021", name: "【DB告警】AWS RDS Vip 持续一分钟不通", priority: "P0", category: "Database-RDS", team: "DBA", metric: "Connectivity", threshold: "VIP unreachable 1min", duration: "1m", expression: `min_over_time(mysql_check_vip{}[1m]) == 0`, services: ["订单服务", "支付服务", "门店服务", "所有依赖MySQL的服务"], handbook: `# ALR-021【DB告警】AWS RDS Vip 持续一分钟不通

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-021 |
| **告警名称** | 【DB告警】AWS RDS Vip 持续一分钟不通 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS VIP(虚拟IP)连续1分钟无法访问，表明数据库实例可能不可达。

### 业务影响

- **严重中断**: 依赖该数据库的所有服务将无法执行读写操作
- **黄金流程中断**: 如果是核心订单/支付库，用户将无法下单和支付
- **紧急程度**: P0级别，需要立即响应

### 受影响服务

所有依赖该RDS实例的微服务

### PromQL表达式

\`\`\`promql
min_over_time(mysql_check_vip{}[1m]) == 0
\`\`\`

### 常见根因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-022", name: "【DB告警】AWS RDS Vip 持续一分钟不通_语音", priority: "P0", category: "Database-RDS", team: "DBA", metric: "Connectivity", threshold: "VIP unreachable 1min (Voice)", duration: "1m", expression: `min_over_time(mysql_check_vip{}[1m]) == 0`, services: ["订单服务", "支付服务", "门店服务"], handbook: `# ALR-022【DB告警】AWS RDS Vip 持续一分钟不通_语音

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-022 |
| **告警名称** | 【DB告警】AWS RDS Vip 持续一分钟不通 (Voice) |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS VIP(虚拟IP)连续1分钟无法访问，表明数据库实例可能不可达。

### 业务影响

- **严重中断**: 依赖该数据库的所有服务将无法执行读写操作
- **黄金流程中断**: 如果是核心订单/支付库，用户将无法下单和支付
- **紧急程度**: P0级别，需要立即响应

### 受影响服务

所有依赖该RDS实例的微服务

### PromQL表达式

\`\`\`promql
min_over_time(mysql_check_vip{}[1m]) == 0
\`\`\`

### 常见根因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-023", name: "【DB告警】AWS RDS 发生重启或者主从切换", priority: "P0", category: "Database-RDS", team: "DBA", metric: "Failover", threshold: "Restart/Failover detected", duration: "Instant", expression: `changes(mysql_global_status_uptime[5m]) > 0`, services: ["订单服务", "支付服务", "门店服务"], handbook: `# ALR-023【DB告警】AWS RDS 发生重启或者主从切换

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-023 |
| **告警名称** | 【DB告警】AWS RDS Failover或重启 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

检测到RDS实例发生重启或主从切换，mysql_global_status_uptime计数器重置。

### 业务影响

- **短暂服务中断**: 切换期间(通常30秒-2分钟)数据库不可写
- **连接重置**: 所有现有数据库连接将被断开
- **事务回滚**: 切换时未完成的事务将被回滚

### 受影响服务

所有依赖该RDS实例的微服务需要重新建立连接

### PromQL表达式

\`\`\`promql
changes(mysql_global_status_uptime[5m]) > 0
\`\`\`

### 常见根因

1. **自动故障转移**: Multi-AZ实例主节点故障触发自动切换
2. **维护窗口**: AWS计划内维护导致的重启
3. **手动操作**: DBA执行的重启或切换操作
4. **资源耗尽**: 内存或存储空间耗尽导致实例重启

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

**根据实际案例分析（2025-10-24 aws-luckyus-opshop-rw案例）：**

本次重启与主从切换由**底层宿主机故障引发**，体现为"主机因网络连通性丢失不可达"，RDS 自动执行**主机替换 + 实例重启 + Multi-AZ Failover**。

**AWS 官方知识库说明：**
- 当primary host 不可达（网络连通性丢失）时会触发Multi-AZ failover 和实例重启，属于底层网络/基础设施瞬时异常场景
- 硬件问题会在 Multi-AZ 中触发failover，同时 RDS 可能进行底层宿主机替换；这是 RDS 的自动自愈行为
- Multi-AZ 的 failover 由 RDS 自动处理，常见时长**60-120秒**（受事务恢复、负载影响）
- Failover 发生时，RDS 会更新实例的 DNS 指向到新的主库，应用**必须重新建立连接**（特别注意 JVM 的 DNS 缓存 TTL）


### 常见原因

1. **自动故障转移**: Multi-AZ实例主节点故障触发自动切换
2. **维护窗口**: AWS计划内维护导致的重启
3. **手动操作**: DBA执行的重启或切换操作
4. **资源耗尽**: 内存或存储空间耗尽导致实例重启

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 步骤1：确认告警信息

检查AWS RDS控制台 Event Log，关注以下事件：
- \`Multi-AZ instance failover started\`（开始主从切换）
- \`DB instance restarted\`（实例重启）
- \`The primary host … is unreachable due to loss of network connectivity\`（主库主机因网络连通性丢失不可达）
- \`Multi-AZ instance failover completed\`（主从切换完成）

### 步骤2：检查 Grafana / CloudWatch 监控面板

**数据库基础态势（Grafana/Performance Insights）：**
- CPU、内存、连接数（DatabaseConnections）、TPS/QPS、线程运行数
- 慢查询趋势（Slow query）

**CloudWatch 关键指标：**
- \`CPUUtilization\`、\`DatabaseConnections\`
- \`FreeableMemory\`（观察是否持续逼近极值）
- \`DiskQueueDepth\`（I/O 等待）

**指标时间范围聚焦：** 告警发生时间前后10分钟

**应用侧观测：**
- APM/网关错误峰值（\`OperationalError\`、\`Connection reset by peer\`、\`too many connections\`等）
- 连接重试/超时日志（是否按策略快速恢复）

### 步骤3：检查 LDAS（数据库审计/监控系统）

- 观察告警窗口内是否存在**长事务/锁等待**（用于排除负载诱因）
- 若发现异常语句，留存 SQL 与执行计划

### 步骤4：沟通确认

在服务树找到负责人并在告警群同步：
- 当时是否有**手工重启/维护操作**
- 当时是否有**批作业/峰值流量**
- 如未来**重复发生**，建议开AWS Support Case让官方排查AZ/宿主集群稳定性

### 步骤5：处理措施

**短期：**
- 观察应用恢复：确认所有连接池已恢复、业务无持续错误
- 开启/核对RDS 事件通知（SNS），确保此类 failover 能第一时间推送到告警系统
- 验证客户端**自动重连与指数退避**是否生效；JVM 场景下检查\`networkaddress.cache.ttl\`以降低 DNS 缓存带来的恢复延迟

**中长期：**
- **RDS Proxy**：在 MySQL/PostgreSQL 等引擎前加入 RDS Proxy，利用连接池与更平滑的故障切换降低应用可见中断
- **演练与指标**：定期在低峰期做手工 Failover 演练，记录实际 RTO（期望 60-120s 内）
- **事件覆盖**：在告警体系中补充 "RDS Event: failover started/completed、host replacement"等关键事件


---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-024", name: "【DB告警】AWS RDS 发生重启或者主从切换_语音", priority: "P0", category: "Database-RDS", team: "DBA", metric: "Failover", threshold: "Restart/Failover (Voice)", duration: "Instant", expression: `changes(mysql_global_status_uptime[5m]) > 0`, services: ["订单服务", "支付服务"], handbook: `# ALR-024【DB告警】AWS RDS 发生重启或者主从切换_语音

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-024 |
| **告警名称** | 【DB告警】AWS RDS Failover或重启 (Voice) |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

检测到RDS实例发生重启或主从切换，mysql_global_status_uptime计数器重置。

### 业务影响

- **短暂服务中断**: 切换期间(通常30秒-2分钟)数据库不可写
- **连接重置**: 所有现有数据库连接将被断开
- **事务回滚**: 切换时未完成的事务将被回滚

### 受影响服务

所有依赖该RDS实例的微服务需要重新建立连接

### PromQL表达式

\`\`\`promql
changes(mysql_global_status_uptime[5m]) > 0
\`\`\`

### 常见根因

1. **自动故障转移**: Multi-AZ实例主节点故障触发自动切换
2. **维护窗口**: AWS计划内维护导致的重启
3. **手动操作**: DBA执行的重启或切换操作
4. **资源耗尽**: 内存或存储空间耗尽导致实例重启

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **自动故障转移**: Multi-AZ实例主节点故障触发自动切换
2. **维护窗口**: AWS计划内维护导致的重启
3. **手动操作**: DBA执行的重启或切换操作
4. **资源耗尽**: 内存或存储空间耗尽导致实例重启

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-025", name: "【DB告警】AWS RDS 慢查询数量持续三分钟大于300个", priority: "P2", category: "Database-RDS", team: "DBA", metric: "Performance", threshold: ">300 slow queries for 3min", duration: "3m", expression: `avg_over_time(mysql_global_status_slow_queries[3m]) > 300`, services: ["营销服务", "数据服务"], handbook: `# ALR-025【DB告警】AWS RDS 慢查询数量持续三分钟大于300个

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-025 |
| **告警名称** | 【DB告警】AWS-RDS 慢查询数量持续三分钟大于300个 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS实例慢查询数量连续3分钟超过300个，表明存在大量执行缓慢的SQL。

### 业务影响

- **性能下降**: 服务响应时间明显增加
- **资源消耗**: CPU和IO资源被慢查询占用
- **用户体验**: 页面加载缓慢，可能出现超时

### 受影响服务

依赖该数据库的所有服务，特别是读取密集型服务

### PromQL表达式

\`\`\`promql
avg_over_time(mysql_global_status_slow_queries[3m]) > 300
rate(mysql_global_status_slow_queries[3m]) * 180 > 300
\`\`\`

### 常见根因

1. **索引问题**: 查询字段缺少索引或索引失效
2. **数据量增长**: 表数据量增加导致查询变慢
3. **复杂查询**: 未优化的JOIN或子查询
4. **锁等待**: 大量事务等待锁释放
5. **参数配置**: slow_query_log阈值设置(long_query_time)

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **索引问题**: 查询字段缺少索引或索引失效
2. **数据量增长**: 表数据量增加导致查询变慢
3. **复杂查询**: 未优化的JOIN或子查询
4. **锁等待**: 大量事务等待锁释放
5. **参数配置**: slow_query_log阈值设置(long_query_time)

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-026", name: "【DB告警】AWS-RDS 慢查询数量持续三分钟大于300个", priority: "P2", category: "Database-RDS", team: "DBA", metric: "Performance", threshold: ">300 slow queries for 3min", duration: "3m", expression: `rate(mysql_global_status_slow_queries[3m]) * 180 > 300`, services: ["营销服务"], handbook: `# ALR-026【DB告警】AWS-RDS 慢查询数量持续三分钟大于300个

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-026 |
| **告警名称** | 【DB告警】AWS RDS 慢查询数量持续三分钟大于300个 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS实例慢查询数量连续3分钟超过300个，表明存在大量执行缓慢的SQL。

### 业务影响

- **性能下降**: 服务响应时间明显增加
- **资源消耗**: CPU和IO资源被慢查询占用
- **用户体验**: 页面加载缓慢，可能出现超时

### 受影响服务

依赖该数据库的所有服务，特别是读取密集型服务

### PromQL表达式

\`\`\`promql
avg_over_time(mysql_global_status_slow_queries[3m]) > 300
rate(mysql_global_status_slow_queries[3m]) * 180 > 300
\`\`\`

### 常见根因

1. **索引问题**: 查询字段缺少索引或索引失效
2. **数据量增长**: 表数据量增加导致查询变慢
3. **复杂查询**: 未优化的JOIN或子查询
4. **锁等待**: 大量事务等待锁释放
5. **参数配置**: slow_query_log阈值设置(long_query_time)

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **索引问题**: 查询字段缺少索引或索引失效
2. **数据量增长**: 表数据量增加导致查询变慢
3. **复杂查询**: 未优化的JOIN或子查询
4. **锁等待**: 大量事务等待锁释放
5. **参数配置**: slow_query_log阈值设置(long_query_time)

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-027", name: "【DB告警】AWS RDS 活跃线程持续两分钟大于24", priority: "P2", category: "Database-RDS", team: "DBA", metric: "Performance", threshold: ">24 active threads for 2min", duration: "2m", expression: `avg_over_time(mysql_global_status_threads_running[2m]) > 24`, services: ["订单服务"], handbook: `# ALR-027【DB告警】AWS RDS 活跃线程持续两分钟大于24

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-027 |
| **告警名称** | 【DB告警】AWS-RDS 活跃线程持续两分钟大于24 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS实例活跃线程数连续2分钟超过24个，表明数据库并发压力过大。

### 业务影响

- **连接排队**: 新的数据库请求可能需要等待
- **响应延迟**: 服务端到端延迟增加
- **资源竞争**: CPU和内存资源竞争加剧

### 受影响服务

所有并发访问该数据库的服务

### PromQL表达式

\`\`\`promql
avg_over_time(mysql_global_status_threads_running[2m]) > 24
min_over_time(mysql_global_status_threads_running[2m]) > 24
\`\`\`

### 常见根因

1. **流量突增**: 促销活动或异常流量导致并发增加
2. **慢查询阻塞**: 长时间运行的查询占用线程
3. **锁等待**: 事务锁导致线程等待
4. **连接池泄露**: 应用连接未正确释放

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

**根据实际案例分析（2025-09-29 aws-luckyus-iluckyhealth-rw案例）：**

**重要提示：**
- **Luckyhealth数据库**是专门为Grafana做的监控数据库
- **Test数据库**上的操作相对安全
- **生产数据库**操作需要格外小心

**数据同步延迟说明：** Datalink等其他地方的数据都有同步延迟，爬取的数据会慢一点，Luckyhealth相对是快一些的。


### 常见原因

1. **流量突增**: 促销活动或异常流量导致并发增加
2. **慢查询阻塞**: 长时间运行的查询占用线程
3. **锁等待**: 事务锁导致线程等待
4. **连接池泄露**: 应用连接未正确释放

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 步骤1：确认告警信息

检查告警详情，确认：
- **告警等级**：P1
- **集群信息**：如 aws-luckyus-iluckyhealth-rw
- **当前值**：如 177（活跃线程数）
- **故障持续时间**

### 步骤2：检查Grafana监控面板

**Grafana关键指标检查：**

| 指标 | 正常范围 | 说明 |
|------|---------|------|
| CPU使用率 | <80% | 4.99%为正常 |
| 磁盘剩余空间 | >10GiB | 25.7 GiB为健康 |
| Current Thread Running | <24 | 监控当前运行线程 |
| Current QPS | 波动正常 | 2.67为低负载 |
| Thread Connected | 7-13范围 | 连接数正常范围 |

**性能指标分析：**
- **TPS/QPS**：正常波动范围
- **Slow Query（慢查询）**：检查是否有慢查询实例数增加
- **MySQL DML commands**：insert操作频率正常
- **DB Threads**：Thread Connected数量正常

### 步骤3：检查AWS CloudWatch监控

**关键指标检查：**
- \`CPUUtilization\`：检查是否有突增
- \`DatabaseConnections\`：查看连接数波动情况
- \`FreeableMemory\`：⚠️ **重点关注此参数**，观察内存使用趋势
- \`CPUCreditBalance/CPUCreditUsage\`：如果是突发型实例，检查CPU积分
- \`DiskQueueDepth\`：检查IO是否存在瓶颈

### 步骤4：检查LDAS数据库进程

**查看活跃进程：**
- 检查当前正在执行的SQL语句
- 查看进程运行时长
- 确认是否有长时间运行的事务

**慢查询分析：**
- 检查是否存在慢查询
- 分析慢查询的SQL语句
- 查看慢查询的执行计划

**锁等待检查：**
- 查看是否存在锁等待情况
- 确认是否有死锁或长时间持有锁的情况

### 步骤5：处理措施

**如果是慢查询导致：**
- **短期**：评估是否可以安全kill掉慢查询进程、联系业务方优化SQL语句
- **长期**：优化慢查询SQL、添加合适的索引、考虑数据归档方案

**如果是连接数过多：**
- 检查连接池配置是否合理
- 检查是否存在连接泄漏
- 考虑增加RDS实例规格或增加读副本分担读压力

**如果是内存不足：**
- **短期**：重启数据库释放缓存（需评估业务影响）
- **长期**：升级RDS实例规格、优化内存使用配置、数据归档或分库分表


---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-028", name: "【DB告警】AWS-RDS 活跃线程持续两分钟大于24", priority: "P2", category: "Database-RDS", team: "DBA", metric: "Performance", threshold: ">24 threads for 2min", duration: "2m", expression: `min_over_time(mysql_global_status_threads_running[2m]) > 24`, services: ["订单服务"], handbook: `# ALR-028【DB告警】AWS-RDS 活跃线程持续两分钟大于24

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-028 |
| **告警名称** | 【DB告警】AWS RDS 活跃线程持续两分钟大于24 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS实例活跃线程数连续2分钟超过24个，表明数据库并发压力过大。

### 业务影响

- **连接排队**: 新的数据库请求可能需要等待
- **响应延迟**: 服务端到端延迟增加
- **资源竞争**: CPU和内存资源竞争加剧

### 受影响服务

所有并发访问该数据库的服务

### PromQL表达式

\`\`\`promql
avg_over_time(mysql_global_status_threads_running[2m]) > 24
min_over_time(mysql_global_status_threads_running[2m]) > 24
\`\`\`

### 常见根因

1. **流量突增**: 促销活动或异常流量导致并发增加
2. **慢查询阻塞**: 长时间运行的查询占用线程
3. **锁等待**: 事务锁导致线程等待
4. **连接池泄露**: 应用连接未正确释放

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **流量突增**: 促销活动或异常流量导致并发增加
2. **慢查询阻塞**: 长时间运行的查询占用线程
3. **锁等待**: 事务锁导致线程等待
4. **连接池泄露**: 应用连接未正确释放

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-029", name: "【DB告警】AWS RDS 磁盘空间连续3分钟不足10G", priority: "P1", category: "Database-RDS", team: "DBA", metric: "Disk", threshold: "<10GB for 3min", duration: "3m", expression: `avg_over_time(aws_rds_freestoragespace_average[3m]) / 1024 / 1024 / 1024 < 10`, services: ["订单服务", "DataLink"], handbook: `# ALR-029【DB告警】AWS RDS 磁盘空间连续3分钟不足10G

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-029 |
| **告警名称** | 【DB告警】AWS RDS 磁盘空间连续3分钟不足10G |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-RDS |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS实例可用磁盘空间连续3分钟低于10GB，存储空间即将耗尽。

### 业务影响

- **写入失败**: 空间耗尽后数据库将无法写入
- **服务中断**: 依赖写操作的服务将完全失败
- **数据完整性**: 事务可能无法完成

### 受影响服务

所有需要写入该数据库的服务

### PromQL表达式

\`\`\`promql
avg_over_time(aws_rds_freestoragespace_average[3m]) / 1024 / 1024 / 1024 < 10
\`\`\`

### 常见根因

1. **日志文件膨胀**: binlog或慢查询日志占用大量空间
2. **数据增长**: 业务数据自然增长
3. **大表操作**: DDL操作产生临时文件
4. **未清理历史数据**: 归档策略未执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查RDS实例状态
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass}'

# 检查RDS性能指标
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=[INSTANCE_ID] \\
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 300 \\
  --statistics Average Maximum

# 检查慢查询
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW PROCESSLIST;"
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW FULL PROCESSLIST;"

# 检查InnoDB状态
mysql -h [RDS_ENDPOINT] -u admin -p -e "SHOW ENGINE INNODB STATUS\\G"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **日志文件膨胀**: binlog或慢查询日志占用大量空间
2. **数据增长**: 业务数据自然增长
3. **大表操作**: DDL操作产生临时文件
4. **未清理历史数据**: 归档策略未执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 慢查询导致

**步骤 1:** 识别消耗资源最高的查询: \`SHOW PROCESSLIST\`

**步骤 2:** 分析慢查询日志，找出问题SQL

**步骤 3:** 使用 \`EXPLAIN\` 分析查询执行计划

**步骤 4:** 添加必要索引或优化查询语句

**步骤 5:** 如需紧急处理可KILL长时间运行的查询: \`KILL [process_id]\`

### 连接数过高

**步骤 1:** 检查当前连接数: \`SHOW STATUS LIKE 'Threads_connected'\`

**步骤 2:** 识别占用连接的应用

**步骤 3:** 优化应用连接池配置

**步骤 4:** 考虑增加max_connections参数

**步骤 5:** 评估是否需要升级实例规格

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 定期审查和优化慢查询
- 设置合理的连接池参数
- 实施数据库性能监控仪表板
- 定期进行容量规划评估
- 配置自动存储扩展
- 建立索引审计机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【DB告警】AWS-RDS CPU使用率连续三分钟大于90%\`
- \`【DB告警】AWS RDS 慢查询数量持续三分钟大于300个\`
- \`【DB告警】AWS RDS 活跃线程持续两分钟大于24\`
- \`【DB告警】AWS RDS 磁盘空间连续3分钟不足10G\`
- \`【DB告警】AWS RDS Vip 持续一分钟不通\`
` },
            { id: "ALR-030", name: "【DB告警】AWS Mongo CPU使用率连续三分钟大于90%", priority: "P1", category: "Database-Mongo", team: "DBA", metric: "CPU", threshold: ">90% for 3min", duration: "3m", expression: `avg_over_time(aws_docdb_cpuutilization_average[3m]) >= 90`, services: ["风控服务"], handbook: `# ALR-030【DB告警】AWS Mongo CPU使用率连续三分钟大于90%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-030 |
| **告警名称** | 【MongoDB告警】AWS DocumentDB CPU使用率超过90% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-MongoDB |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS RDS MySQL实例的CPU使用率连续3分钟超过90%，表明数据库负载过高。

### 业务影响

- **黄金流程影响**: 如果是salesorder-rw或salespayment-rw实例，将直接影响用户下单和支付
- **服务降级**: 数据库响应延迟增加，可能导致API超时
- **级联效应**: 连接池可能耗尽，导致其他服务无法获取数据库连接

### 受影响服务

订单服务、支付服务、会员服务、营销服务等依赖该数据库的所有微服务

### PromQL表达式

\`\`\`promql
aws_rds_cpuutilization_average offset 3m >= 90
avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90
\`\`\`

### 常见根因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DocumentDB集群状态
aws docdb describe-db-clusters

# 检查DocumentDB实例状态
aws docdb describe-db-instances

# 检查MongoDB当前操作
mongo --host [DOCDB_ENDPOINT] --eval "db.currentOp()"

# 检查MongoDB服务器状态
mongo --host [DOCDB_ENDPOINT] --eval "db.serverStatus()"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-031", name: "【DB告警】AWS Mongo CPU使用率连续三分钟大于90%_语音", priority: "P0", category: "Database-Mongo", team: "DBA", metric: "CPU", threshold: ">90% for 3min (Voice)", duration: "3m", expression: `avg_over_time(aws_docdb_cpuutilization_average[3m]) >= 90`, services: ["风控服务"], handbook: `# ALR-031【DB告警】AWS Mongo CPU使用率连续三分钟大于90%_语音

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-031 |
| **告警名称** | 【MongoDB告警】AWS DocumentDB CPU使用率超过90% (Voice) |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-MongoDB |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS RDS MySQL实例的CPU使用率连续3分钟超过90%，表明数据库负载过高。

### 业务影响

- **黄金流程影响**: 如果是salesorder-rw或salespayment-rw实例，将直接影响用户下单和支付
- **服务降级**: 数据库响应延迟增加，可能导致API超时
- **级联效应**: 连接池可能耗尽，导致其他服务无法获取数据库连接

### 受影响服务

订单服务、支付服务、会员服务、营销服务等依赖该数据库的所有微服务

### PromQL表达式

\`\`\`promql
aws_rds_cpuutilization_average offset 3m >= 90
avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90
\`\`\`

### 常见根因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DocumentDB集群状态
aws docdb describe-db-clusters

# 检查DocumentDB实例状态
aws docdb describe-db-instances

# 检查MongoDB当前操作
mongo --host [DOCDB_ENDPOINT] --eval "db.currentOp()"

# 检查MongoDB服务器状态
mongo --host [DOCDB_ENDPOINT] --eval "db.serverStatus()"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-032", name: "【DB告警】AWS Mongo 可用内存连续三分钟不足500M", priority: "P1", category: "Database-Mongo", team: "DBA", metric: "Memory", threshold: "<500M for 3min", duration: "3m", expression: `avg_over_time(aws_docdb_freeable_memory_average[3m]) / 1024 / 1024 < 500`, services: ["风控服务"], handbook: `# ALR-032【DB告警】AWS Mongo 可用内存连续三分钟不足500M

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-032 |
| **告警名称** | 【MongoDB告警】DocumentDB 可用内存不足500MB |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-MongoDB |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS RDS MySQL实例的CPU使用率连续3分钟超过90%，表明数据库负载过高。

### 业务影响

- **黄金流程影响**: 如果是salesorder-rw或salespayment-rw实例，将直接影响用户下单和支付
- **服务降级**: 数据库响应延迟增加，可能导致API超时
- **级联效应**: 连接池可能耗尽，导致其他服务无法获取数据库连接

### 受影响服务

订单服务、支付服务、会员服务、营销服务等依赖该数据库的所有微服务

### PromQL表达式

\`\`\`promql
aws_rds_cpuutilization_average offset 3m >= 90
avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90
\`\`\`

### 常见根因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查DocumentDB集群状态
aws docdb describe-db-clusters

# 检查DocumentDB实例状态
aws docdb describe-db-instances

# 检查MongoDB当前操作
mongo --host [DOCDB_ENDPOINT] --eval "db.currentOp()"

# 检查MongoDB服务器状态
mongo --host [DOCDB_ENDPOINT] --eval "db.serverStatus()"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-033", name: "【DB告警】AWS-ES CPU 使用率大于90%", priority: "P1", category: "Database-ES", team: "DBA", metric: "CPU", threshold: ">90%", duration: "Instant", expression: `aws_es_cpuutilization_average > 90`, services: ["搜索服务", "日志服务"], handbook: `# ALR-033【DB告警】AWS-ES CPU 使用率大于90%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-033 |
| **告警名称** | 【ES告警】OpenSearch CPU使用率超过90% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-OpenSearch |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS RDS MySQL实例的CPU使用率连续3分钟超过90%，表明数据库负载过高。

### 业务影响

- **黄金流程影响**: 如果是salesorder-rw或salespayment-rw实例，将直接影响用户下单和支付
- **服务降级**: 数据库响应延迟增加，可能导致API超时
- **级联效应**: 连接池可能耗尽，导致其他服务无法获取数据库连接

### 受影响服务

订单服务、支付服务、会员服务、营销服务等依赖该数据库的所有微服务

### PromQL表达式

\`\`\`promql
aws_rds_cpuutilization_average offset 3m >= 90
avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90
\`\`\`

### 常见根因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查OpenSearch域状态
aws opensearch describe-domain --domain-name [DOMAIN_NAME]

# 检查集群健康状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cluster/health?pretty"

# 检查节点状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/nodes?v"

# 检查索引状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/indices?v"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-034", name: "【DB告警】AWS-ES CPU 使用率大于90%_语音", priority: "P0", category: "Database-ES", team: "DBA", metric: "CPU", threshold: ">90% (Voice)", duration: "Instant", expression: `aws_es_cpuutilization_average > 90`, services: ["搜索服务"], handbook: `# ALR-034【DB告警】AWS-ES CPU 使用率大于90%_语音

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-034 |
| **告警名称** | 【ES告警】OpenSearch CPU使用率超过90% (Voice) |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-OpenSearch |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS RDS MySQL实例的CPU使用率连续3分钟超过90%，表明数据库负载过高。

### 业务影响

- **黄金流程影响**: 如果是salesorder-rw或salespayment-rw实例，将直接影响用户下单和支付
- **服务降级**: 数据库响应延迟增加，可能导致API超时
- **级联效应**: 连接池可能耗尽，导致其他服务无法获取数据库连接

### 受影响服务

订单服务、支付服务、会员服务、营销服务等依赖该数据库的所有微服务

### PromQL表达式

\`\`\`promql
aws_rds_cpuutilization_average offset 3m >= 90
avg_over_time(aws_rds_cpuutilization_average[3m]) >= 90
\`\`\`

### 常见根因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查OpenSearch域状态
aws opensearch describe-domain --domain-name [DOMAIN_NAME]

# 检查集群健康状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cluster/health?pretty"

# 检查节点状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/nodes?v"

# 检查索引状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/indices?v"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **慢查询累积**: 未优化的SQL语句导致全表扫描
2. **索引缺失**: 关键查询字段缺少索引
3. **连接数暴增**: 应用侧连接池配置不当或流量突增
4. **锁竞争**: 大事务或死锁导致资源等待
5. **实例规格不足**: 业务增长超出当前实例容量
6. **批量任务冲击**: 定时任务或数据同步任务在业务高峰期执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-035", name: "【DB告警】AWS-ES 集群状态Red", priority: "P0", category: "Database-ES", team: "DBA", metric: "Cluster Health", threshold: "Status Red", duration: "Instant", expression: `aws_es_cluster_status_red == 1`, services: ["搜索服务", "营销服务"], handbook: `# ALR-035【DB告警】AWS-ES 集群状态Red

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-035 |
| **告警名称** | 【ES告警】OpenSearch集群状态为RED |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-OpenSearch |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS VIP(虚拟IP)连续1分钟无法访问，表明数据库实例可能不可达。

### 业务影响

- **严重中断**: 依赖该数据库的所有服务将无法执行读写操作
- **黄金流程中断**: 如果是核心订单/支付库，用户将无法下单和支付
- **紧急程度**: P0级别，需要立即响应

### 受影响服务

所有依赖该RDS实例的微服务

### PromQL表达式

\`\`\`promql
min_over_time(mysql_check_vip{}[1m]) == 0
\`\`\`

### 常见根因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查OpenSearch域状态
aws opensearch describe-domain --domain-name [DOMAIN_NAME]

# 检查集群健康状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cluster/health?pretty"

# 检查节点状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/nodes?v"

# 检查索引状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/indices?v"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-036", name: "【DB告警】AWS-ES 集群状态Red_语音", priority: "P0", category: "Database-ES", team: "DBA", metric: "Cluster Health", threshold: "Status Red (Voice)", duration: "Instant", expression: `aws_es_cluster_status_red == 1`, services: ["搜索服务"], handbook: `# ALR-036【DB告警】AWS-ES 集群状态Red_语音

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-036 |
| **告警名称** | 【ES告警】OpenSearch集群状态为RED (Voice) |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-OpenSearch |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS VIP(虚拟IP)连续1分钟无法访问，表明数据库实例可能不可达。

### 业务影响

- **严重中断**: 依赖该数据库的所有服务将无法执行读写操作
- **黄金流程中断**: 如果是核心订单/支付库，用户将无法下单和支付
- **紧急程度**: P0级别，需要立即响应

### 受影响服务

所有依赖该RDS实例的微服务

### PromQL表达式

\`\`\`promql
min_over_time(mysql_check_vip{}[1m]) == 0
\`\`\`

### 常见根因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查OpenSearch域状态
aws opensearch describe-domain --domain-name [DOMAIN_NAME]

# 检查集群健康状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cluster/health?pretty"

# 检查节点状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/nodes?v"

# 检查索引状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/indices?v"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-037", name: "【DB告警】AWS-ES 集群状态Yellow", priority: "P2", category: "Database-ES", team: "DBA", metric: "Cluster Health", threshold: "Status Yellow", duration: "Instant", expression: `aws_es_cluster_status_yellow == 1`, services: ["搜索服务"], handbook: `# ALR-037【DB告警】AWS-ES 集群状态Yellow

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-037 |
| **告警名称** | 【ES告警】OpenSearch集群状态为YELLOW |
| **优先级** | P1 |
| **服务等级** | L1 |
| **类别** | Database-OpenSearch |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

RDS VIP(虚拟IP)连续1分钟无法访问，表明数据库实例可能不可达。

### 业务影响

- **严重中断**: 依赖该数据库的所有服务将无法执行读写操作
- **黄金流程中断**: 如果是核心订单/支付库，用户将无法下单和支付
- **紧急程度**: P0级别，需要立即响应

### 受影响服务

所有依赖该RDS实例的微服务

### PromQL表达式

\`\`\`promql
min_over_time(mysql_check_vip{}[1m]) == 0
\`\`\`

### 常见根因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查OpenSearch域状态
aws opensearch describe-domain --domain-name [DOMAIN_NAME]

# 检查集群健康状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cluster/health?pretty"

# 检查节点状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/nodes?v"

# 检查索引状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/indices?v"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **网络问题**: VPC网络配置异常或安全组规则变更
2. **RDS故障**: 实例重启、主从切换进行中
3. **DNS解析失败**: 内部DNS服务异常
4. **监控探测异常**: mysql_exporter自身故障导致误报

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-038", name: "【DB告警】AWS-ES磁盘空间不足10G", priority: "P1", category: "Database-ES", team: "DBA", metric: "Disk", threshold: "<10GB", duration: "Instant", expression: `aws_es_free_storage_space_total / 1024 < 10`, services: ["搜索服务"], handbook: `# ALR-038【DB告警】AWS-ES磁盘空间不足10G

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-038 |
| **告警名称** | 【ES告警】OpenSearch磁盘空间不足10G |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-OpenSearch |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS实例可用磁盘空间连续3分钟低于10GB，存储空间即将耗尽。

### 业务影响

- **写入失败**: 空间耗尽后数据库将无法写入
- **服务中断**: 依赖写操作的服务将完全失败
- **数据完整性**: 事务可能无法完成

### 受影响服务

所有需要写入该数据库的服务

### PromQL表达式

\`\`\`promql
avg_over_time(aws_rds_freestoragespace_average[3m]) / 1024 / 1024 / 1024 < 10
\`\`\`

### 常见根因

1. **日志文件膨胀**: binlog或慢查询日志占用大量空间
2. **数据增长**: 业务数据自然增长
3. **大表操作**: DDL操作产生临时文件
4. **未清理历史数据**: 归档策略未执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查OpenSearch域状态
aws opensearch describe-domain --domain-name [DOMAIN_NAME]

# 检查集群健康状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cluster/health?pretty"

# 检查节点状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/nodes?v"

# 检查索引状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/indices?v"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

**根据实际案例分析（2025-09-13案例）：**

**日志保留规则（重要）：**

| 日志类型 | 保留策略 | 索引命名示例 |
|---------|---------|------------|
| 按月生成日志 | 保留 3-6 个月 | logs-2024.09 |
| 按日生成日志 | 保留 7 天 | logs-2024.09.13 |
| LFE集群 (luckylfe-log) | 保留 30 天 | - |
| urlog集群 (luckyur-log) | 保留 15 天 | - |
| dify集群 | 暂不处理 | - |

**清理优先级（默认）：**
1. **优先删除**：按日生成的过期日志（超过保留期限）
2. **次要删除**：大容量索引（> 1GB）中的过期数据
3. **谨慎删除**：按月生成的日志（除非确实超过 3-6 个月）
4. **不删除**：当前活跃索引、系统索引（.开头）、dify集群数据


### 常见原因

1. **日志文件膨胀**: binlog或慢查询日志占用大量空间
2. **数据增长**: 业务数据自然增长
3. **大表操作**: DDL操作产生临时文件
4. **未清理历史数据**: 归档策略未执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 步骤1：确认磁盘空间与集群状态

**登录AWS控制台：** Amazon OpenSearch Service → 选择告警集群

**检查磁盘空间相关指标（CloudWatch → Metrics → OpenSearch）：**
- \`FreeStorageSpace\`（剩余可用空间）一周趋势
- **判定标准**：若持续 < 10 GiB 且呈下降趋势，判定为磁盘压力，需要立即清理
- 同步查看 \`ClusterIndexWritesBlocked\`（是否发生写入阻塞）、\`JVMMemoryPressure\`（内存压力）

**ClusterHealthStatus 确认：**
- **Red**：立即处理，可能已影响服务
- **Yellow**：需要关注，副本可能受影响
- **Green**：正常状态

### 步骤2：访问索引管理

**通过KBX（https://ikbx.luckincoffee.us/）或AWS Kibana：**
1. 左侧菜单选择 "Index Management" > "Indices"
2. 按 **Size** 降序排序，快速定位占用空间大的索引
3. 查看索引命名规则，区分按日/按月生成的索引

### 步骤3：确定清理策略

**触发规则（磁盘空间）：**
- FreeStorageSpace 持续 < 10 GiB（约等价使用率 > 85%），且一周趋势持续下降 → **立即清理**
- 若仅短时波动，可继续观察
- 如出现 \`ClusterIndexWritesBlocked\`，优先清理

### 步骤4：执行清理操作

**截图记录（重要）：**
- 清理前：FreeStorageSpace 一周趋势、ClusterHealthStatus 截图
- 索引列表截图（包含名称、大小、文档数）
- 记录要删除的具体索引名称列表

**通知方式：企业微信告警群/DBA值班**

**KBX清理操作：**
1. Stack Management → Index Management → Indices
2. 按 Size 排序，识别大容量索引
3. 核对索引日期，确认符合删除条件
4. 选中待删除索引，点击 Manage indices → Delete indices
5. **二次确认**：输入索引名称确认删除
6. **批量操作建议**：每次删除 5-10 个索引，观察集群响应

**监控恢复情况：**
- 等待 ~5-10 分钟
- 重新查看 FreeStorageSpace 是否显著回升（> 10 GiB）
- 确认 ClusterHealthStatus 保持 Green/Yellow
- 验证 ClusterIndexWritesBlocked 解除

### 注意事项

⚠️ **关键提醒：**
- 删除操作**不可恢复**，务必：二次确认索引名称和日期、确保符合保留策略、保存删除前截图
- **按月生成**的日志谨慎删除（通常是重要汇总数据）
- 优先在**业务低峰期（美国时间晚上）**执行
- 保持与团队沟通，避免重复操作


---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-039", name: "【DB告警】AWS-ES磁盘空间不足10G_语音", priority: "P0", category: "Database-ES", team: "DBA", metric: "Disk", threshold: "<10GB (Voice)", duration: "Instant", expression: `aws_es_free_storage_space_total / 1024 < 10`, services: ["搜索服务"], handbook: `# ALR-039【DB告警】AWS-ES磁盘空间不足10G_语音

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-039 |
| **告警名称** | 【ES告警】OpenSearch磁盘空间不足10G (Voice) |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Database-OpenSearch |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

RDS实例可用磁盘空间连续3分钟低于10GB，存储空间即将耗尽。

### 业务影响

- **写入失败**: 空间耗尽后数据库将无法写入
- **服务中断**: 依赖写操作的服务将完全失败
- **数据完整性**: 事务可能无法完成

### 受影响服务

所有需要写入该数据库的服务

### PromQL表达式

\`\`\`promql
avg_over_time(aws_rds_freestoragespace_average[3m]) / 1024 / 1024 / 1024 < 10
\`\`\`

### 常见根因

1. **日志文件膨胀**: binlog或慢查询日志占用大量空间
2. **数据增长**: 业务数据自然增长
3. **大表操作**: DDL操作产生临时文件
4. **未清理历史数据**: 归档策略未执行

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查OpenSearch域状态
aws opensearch describe-domain --domain-name [DOMAIN_NAME]

# 检查集群健康状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cluster/health?pretty"

# 检查节点状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/nodes?v"

# 检查索引状态
curl -X GET "https://[OPENSEARCH_ENDPOINT]/_cat/indices?v"
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **日志文件膨胀**: binlog或慢查询日志占用大量空间
2. **数据增长**: 业务数据自然增长
3. **大表操作**: DDL操作产生临时文件
4. **未清理历史数据**: 归档策略未执行

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-040", name: "【DB告警】AWS Redis CPU使用率大于90%", priority: "P1", category: "Database-Redis", team: "存储平台", metric: "CPU", threshold: ">90%", duration: "Instant", expression: `aws_elasticache_cpuutilization_average > 90`, services: ["缓存云平台", "订单服务"], handbook: `# ALR-040【DB告警】AWS Redis CPU使用率大于90%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-040 |
| **告警名称** | 【Redis告警】ElastiCache CPU使用率超过90% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-Redis |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS ElastiCache Redis实例CPU使用率达到90%以上。

### 业务影响

- **缓存延迟**: Redis命令响应时间增加
- **会话问题**: 依赖Redis存储会话的服务可能出现登录异常
- **限流失效**: 如果Redis用于限流，可能导致限流机制失效

### 受影响服务

会话管理、缓存服务、限流服务、消息队列等

### PromQL表达式

\`\`\`promql
aws_elasticache_cpuutilization_average >= 90
\`\`\`

### 常见根因

1. **热点Key**: 某个Key访问过于频繁
2. **大Key操作**: 对大List/Set/Hash的操作
3. **Lua脚本**: 复杂Lua脚本消耗CPU
4. **持久化**: RDB/AOF持久化过程中的CPU消耗
5. **实例规格不足**: 需要升级实例

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **热点Key**: 某个Key访问过于频繁
2. **大Key操作**: 对大List/Set/Hash的操作
3. **Lua脚本**: 复杂Lua脚本消耗CPU
4. **持久化**: RDB/AOF持久化过程中的CPU消耗
5. **实例规格不足**: 需要升级实例

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-041", name: "【DB告警】Redis CPU使用率持续3分钟超过70%", priority: "P2", category: "Database-Redis", team: "存储平台", metric: "CPU", threshold: ">70% for 3min", duration: "3m", expression: `avg_over_time(redis_cpu_usage[3m]) > 70`, services: ["订单服务"], handbook: `# ALR-041【DB告警】Redis CPU使用率持续3分钟超过70%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-041 |
| **告警名称** | 【Redis告警】ElastiCache CPU使用率连续三分钟超过70% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-Redis |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

AWS ElastiCache Redis实例CPU使用率达到90%以上。

### 业务影响

- **缓存延迟**: Redis命令响应时间增加
- **会话问题**: 依赖Redis存储会话的服务可能出现登录异常
- **限流失效**: 如果Redis用于限流，可能导致限流机制失效

### 受影响服务

会话管理、缓存服务、限流服务、消息队列等

### PromQL表达式

\`\`\`promql
aws_elasticache_cpuutilization_average >= 90
\`\`\`

### 常见根因

1. **热点Key**: 某个Key访问过于频繁
2. **大Key操作**: 对大List/Set/Hash的操作
3. **Lua脚本**: 复杂Lua脚本消耗CPU
4. **持久化**: RDB/AOF持久化过程中的CPU消耗
5. **实例规格不足**: 需要升级实例

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **热点Key**: 某个Key访问过于频繁
2. **大Key操作**: 对大List/Set/Hash的操作
3. **Lua脚本**: 复杂Lua脚本消耗CPU
4. **持久化**: RDB/AOF持久化过程中的CPU消耗
5. **实例规格不足**: 需要升级实例

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-042", name: "【DB告警】Redis 内存使用率持续3分钟超过70%", priority: "P2", category: "Database-Redis", team: "存储平台", metric: "Memory", threshold: ">70% for 3min", duration: "3m", expression: `avg_over_time(redis_memory_usage_ratio[3m]) > 70`, services: ["订单服务"], handbook: `# ALR-042【DB告警】Redis 内存使用率持续3分钟超过70%

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-042 |
| **告警名称** | 【Redis告警】ElastiCache 内存使用率超过70% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-Redis |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Redis实例内存使用率连续3分钟超过70%，接近内存上限。

### 业务影响

- **Key淘汰**: 可能触发maxmemory-policy策略淘汰Key
- **OOM风险**: 继续增长可能导致Redis OOM
- **写入失败**: 达到上限后新写入可能失败

### 受影响服务

所有使用该Redis实例的缓存服务

### PromQL表达式

\`\`\`promql
avg_over_time(aws_elasticache_database_memory_usage_percentage_average[3m]) > 70
\`\`\`

### 常见根因

1. **数据未过期**: TTL设置不当导致数据累积
2. **大Key**: 存在占用大量内存的Key
3. **业务增长**: 缓存数据量自然增长
4. **内存碎片**: Redis内存碎片率过高

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

**根据实际案例分析（2025-09-17 luckyus-web集群案例）：**

**触发规则（内存）：**
- FreeableMemory 持续 < 1 GiB（约等价使用率 > 70%），且一周趋势无回升 → **扩容**
- 若仅短时波动（Spike），且随即回升，可继续观察
- 如出现 **Evictions/SWAP**，优先扩容

**扩容方式（默认）：**
1. **纵向扩容（优先）**：节点规格上升一个档位，例如：\`cache.t4g.medium\` → \`cache.t4g.large\`
2. **读多写少**：可增加 Read Replica 分担读
3. **写入压力/键空间过大**：评估开启集群分片做水平扩展


### 常见原因

1. **数据未过期**: TTL设置不当导致数据累积
2. **大Key**: 存在占用大量内存的Key
3. **业务增长**: 缓存数据量自然增长
4. **内存碎片**: Redis内存碎片率过高

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 步骤1：确认内存与CPU状态

**登录AWS控制台：** Amazon ElastiCache → 选择 Redis 集群（如 luckyus-web）

**检查内存相关指标（CloudWatch → Metrics → ElastiCache）：**
- \`FreeableMemory\`（空闲可释放内存）一周趋势
- **判定标准**：若持续 < 1 GiB 且无回升趋势，判定为内存压力，需要扩容
- 同步查看 \`Evictions\`（是否发生驱逐）、\`SwapUsage\`（是否出现交换）
- \`BytesUsedForCache / EngineUsedMemory\`：确认已用内存接近总内存上限
- **截图**：保存一周趋势图（用于变更确认与复盘）

**检查 CPU 指标（建议）：**
- \`CPUUtilization\`：如持续高于 80%（或 2 核实例总CPU > ~45%），需一并评估

### 步骤2：访问集群与参数信息

**通过AWS控制台：** ElastiCache → Redis → Clusters → 点击集群名称

**查看 Nodes / Metrics / Parameter groups：**
- **参数组**：如 luckyus-ha-6（使用 LRU 驱逐策略 volatile-lru）
- **Multi-AZ 与副本**：Enabled（可降低扩容/故障切换影响）

### 步骤3：确定扩容策略

| 情况 | 处理方式 |
|------|---------|
| FreeableMemory 持续 < 1 GiB | 扩容（纵向） |
| 读多写少场景 | 增加 Read Replica |
| 写入压力/键空间过大 | 开启集群分片（水平扩展） |

### 步骤4：执行扩容操作

**截图记录（重要）：**
- 扩容前：FreeableMemory 一周趋势、Evictions/SwapUsage/CPU 指标截图
- 集群详情页（Engine、Multi-AZ、节点类型、参数组）截图

**通知方式：企业微信告警群/DBA值班**
- 说明：问题现象、阈值命中、建议动作（上调一档）、预计影响（短暂 failover/连接重试）

**控制台扩容：**
1. ElastiCache → Redis → Clusters → 选择集群 → Modify
2. Node type：从 cache.t4g.medium 调整为上一档
3. 其他项保持不变，提交变更
4. 关注状态：modifying → available

**监控恢复情况：**
- 等待 ~20 分钟（视数据量而定）
- 重新查看 FreeableMemory 是否显著回升（> 1 GiB）
- 确认告警解除、应用无持续报错

### 注意事项

⚠️ **关键提醒：**
- 扩容会触发**主从切换**，存在**短暂连接中断**；务必确认客户端具备**重连/重试**
- 若使用 **T 系列（t4g）**，避免长期高负载耗尽 CPU Credit；持续高负载建议改用 M/R 系列
- 仅当**持续性**内存紧张时扩容；瞬时峰值应结合 Evictions/Swap 判断
- 在业务低峰进行计划性扩容，降低影响面
- 变更前后务必**留痕**（截图/记录）


---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-043", name: "【DB告警】Redis 发生客户端堵塞", priority: "P1", category: "Database-Redis", team: "存储平台", metric: "Connectivity", threshold: "Client blocked", duration: "Instant", expression: `redis_blocked_clients > 0`, services: ["订单服务", "IoT服务"], handbook: `# ALR-043【DB告警】Redis 发生客户端堵塞

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-043 |
| **告警名称** | 【Redis告警】ElastiCache 客户端阻塞 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-Redis |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

检测到Redis有客户端处于阻塞状态，通常由BLPOP/BRPOP等阻塞命令引起。

### 业务影响

- **连接占用**: 阻塞连接占用连接池资源
- **潜在死锁**: 可能导致业务逻辑等待

### 受影响服务

使用阻塞命令的消息队列类服务

### PromQL表达式

\`\`\`promql
redis_blocked_clients > 0
\`\`\`

### 常见根因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-044", name: "【DB告警】Redis 实例命令平均时延大于2ms", priority: "P2", category: "Database-Redis", team: "存储平台", metric: "Latency", threshold: ">2ms avg", duration: "Instant", expression: `redis_command_latency_avg > 2`, services: ["订单服务"], handbook: `# ALR-044【DB告警】Redis 实例命令平均时延大于2ms

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-044 |
| **告警名称** | 【Redis告警】ElastiCache 延迟超过2ms |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-Redis |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

检测到Redis有客户端处于阻塞状态，通常由BLPOP/BRPOP等阻塞命令引起。

### 业务影响

- **连接占用**: 阻塞连接占用连接池资源
- **潜在死锁**: 可能导致业务逻辑等待

### 受影响服务

使用阻塞命令的消息队列类服务

### PromQL表达式

\`\`\`promql
redis_blocked_clients > 0
\`\`\`

### 常见根因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-045", name: "【DB告警】Redis 实例客户端nomal缓冲超过32m", priority: "P2", category: "Database-Redis", team: "存储平台", metric: "Memory", threshold: ">32MB client buffer", duration: "Instant", expression: `redis_client_output_buffer_normal > 33554432`, services: ["订单服务"], handbook: `# ALR-045【DB告警】Redis 实例客户端nomal缓冲超过32m

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-045 |
| **告警名称** | 【Redis告警】ElastiCache 复制缓冲区超过32MB |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Database-Redis |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Redis实例内存使用率连续3分钟超过70%，接近内存上限。

### 业务影响

- **Key淘汰**: 可能触发maxmemory-policy策略淘汰Key
- **OOM风险**: 继续增长可能导致Redis OOM
- **写入失败**: 达到上限后新写入可能失败

### 受影响服务

所有使用该Redis实例的缓存服务

### PromQL表达式

\`\`\`promql
avg_over_time(aws_elasticache_database_memory_usage_percentage_average[3m]) > 70
\`\`\`

### 常见根因

1. **数据未过期**: TTL设置不当导致数据累积
2. **大Key**: 存在占用大量内存的Key
3. **业务增长**: 缓存数据量自然增长
4. **内存碎片**: Redis内存碎片率过高

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **数据未过期**: TTL设置不当导致数据累积
2. **大Key**: 存在占用大量内存的Key
3. **业务增长**: 缓存数据量自然增长
4. **内存碎片**: Redis内存碎片率过高

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-046", name: "【DB告警】Redis 实例流量大于32Mbps", priority: "P2", category: "Database-Redis", team: "存储平台", metric: "Network", threshold: ">32Mbps", duration: "Instant", expression: `redis_network_bytes_in_rate + redis_network_bytes_out_rate > 33554432`, services: ["订单服务"], handbook: `# ALR-046【DB告警】Redis 实例流量大于32Mbps

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-046 |
| **告警名称** | 【Redis告警】ElastiCache 网络流量超过32Mbps |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Database-Redis |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

AWS ElastiCache Redis实例CPU使用率达到90%以上。

### 业务影响

- **缓存延迟**: Redis命令响应时间增加
- **会话问题**: 依赖Redis存储会话的服务可能出现登录异常
- **限流失效**: 如果Redis用于限流，可能导致限流机制失效

### 受影响服务

会话管理、缓存服务、限流服务、消息队列等

### PromQL表达式

\`\`\`promql
aws_elasticache_cpuutilization_average >= 90
\`\`\`

### 常见根因

1. **热点Key**: 某个Key访问过于频繁
2. **大Key操作**: 对大List/Set/Hash的操作
3. **Lua脚本**: 复杂Lua脚本消耗CPU
4. **持久化**: RDB/AOF持久化过程中的CPU消耗
5. **实例规格不足**: 需要升级实例

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **热点Key**: 某个Key访问过于频繁
2. **大Key操作**: 对大List/Set/Hash的操作
3. **Lua脚本**: 复杂Lua脚本消耗CPU
4. **持久化**: RDB/AOF持久化过程中的CPU消耗
5. **实例规格不足**: 需要升级实例

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-047", name: "【DB告警】Redis 实例触发key淘汰", priority: "P2", category: "Database-Redis", team: "存储平台", metric: "Eviction", threshold: "Key eviction triggered", duration: "Instant", expression: `increase(redis_evicted_keys[5m]) > 0`, services: ["订单服务"], handbook: `# ALR-047【DB告警】Redis 实例触发key淘汰

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-047 |
| **告警名称** | 【Redis告警】ElastiCache Key驱逐告警 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Database-Redis |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Redis触发了Key淘汰机制，表明内存已达到maxmemory限制。

### 业务影响

- **缓存穿透**: 被淘汰的热点Key可能导致大量请求打到数据库
- **数据丢失**: 重要缓存数据被淘汰
- **业务异常**: 依赖缓存数据的业务逻辑异常

### 受影响服务

所有依赖缓存数据的服务

### PromQL表达式

\`\`\`promql
increase(redis_evicted_keys_total[1m]) > 0
\`\`\`

### 常见根因

1. **内存不足**: 实例内存配置不足
2. **数据膨胀**: 缓存数据量超出预期
3. **TTL问题**: 未设置合理的过期时间
4. **maxmemory-policy**: 淘汰策略配置

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **内存不足**: 实例内存配置不足
2. **数据膨胀**: 缓存数据量超出预期
3. **TTL问题**: 未设置合理的过期时间
4. **maxmemory-policy**: 淘汰策略配置

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-048", name: "【DB告警】Redis 实例连接数使用率大于30%", priority: "P2", category: "Database-Redis", team: "存储平台", metric: "Connections", threshold: ">30% usage", duration: "Instant", expression: `redis_connected_clients / redis_maxclients > 0.3`, services: ["订单服务"], handbook: `# ALR-048【DB告警】Redis 实例连接数使用率大于30%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-048 |
| **告警名称** | 【Redis告警】ElastiCache 连接数超过30 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Database-Redis |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Redis实例内存使用率连续3分钟超过70%，接近内存上限。

### 业务影响

- **Key淘汰**: 可能触发maxmemory-policy策略淘汰Key
- **OOM风险**: 继续增长可能导致Redis OOM
- **写入失败**: 达到上限后新写入可能失败

### 受影响服务

所有使用该Redis实例的缓存服务

### PromQL表达式

\`\`\`promql
avg_over_time(aws_elasticache_database_memory_usage_percentage_average[3m]) > 70
\`\`\`

### 常见根因

1. **数据未过期**: TTL设置不当导致数据累积
2. **大Key**: 存在占用大量内存的Key
3. **业务增长**: 缓存数据量自然增长
4. **内存碎片**: Redis内存碎片率过高

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **数据未过期**: TTL设置不当导致数据累积
2. **大Key**: 存在占用大量内存的Key
3. **业务增长**: 缓存数据量自然增长
4. **内存碎片**: Redis内存碎片率过高

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-049", name: "【DB告警】Redis 实例采集失败请检查是否存活", priority: "P0", category: "Database-Redis", team: "存储平台", metric: "Availability", threshold: "Collection failed", duration: "Instant", expression: `up{job="redis"} == 0`, services: ["订单服务", "营销服务", "IoT服务"], handbook: `# ALR-049【DB告警】Redis 实例采集失败请检查是否存活

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-049 |
| **告警名称** | 【Redis告警】ElastiCache 数据采集失败 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Database-Redis |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

检测到Redis有客户端处于阻塞状态，通常由BLPOP/BRPOP等阻塞命令引起。

### 业务影响

- **连接占用**: 阻塞连接占用连接池资源
- **潜在死锁**: 可能导致业务逻辑等待

### 受影响服务

使用阻塞命令的消息队列类服务

### PromQL表达式

\`\`\`promql
redis_blocked_clients > 0
\`\`\`

### 常见根因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查ElastiCache集群状态
aws elasticache describe-cache-clusters \\
  --show-cache-node-info

# 检查Redis内存使用
redis-cli -h [REDIS_ENDPOINT] INFO memory

# 检查Redis客户端连接
redis-cli -h [REDIS_ENDPOINT] CLIENT LIST

# 检查Redis慢日志
redis-cli -h [REDIS_ENDPOINT] SLOWLOG GET 10
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 内存使用过高

**步骤 1:** 检查内存使用: \`INFO memory\`

**步骤 2:** 查找大Key: \`redis-cli --bigkeys\`

**步骤 3:** 分析Key过期策略是否合理

**步骤 4:** 清理过期或无用数据

**步骤 5:** 考虑扩容或数据分片

### 客户端连接问题

**步骤 1:** 检查客户端连接: \`CLIENT LIST\`

**步骤 2:** 识别异常连接来源

**步骤 3:** 检查应用连接池配置

**步骤 4:** 关闭空闲连接: \`CLIENT KILL\`

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控内存使用趋势
- 设置合理的过期策略
- 避免大Key操作
- 配置合理的连接池
- 定期清理无用数据
- 建立容量规划机制

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Redis告警】ElastiCache CPU使用率超过90%\`
- \`【Redis告警】ElastiCache 内存使用率超过70%\`
- \`【Redis告警】ElastiCache 延迟超过2ms\`
- \`【Redis告警】ElastiCache Key驱逐告警\`
- \`【Redis告警】ElastiCache 客户端阻塞\`
` },
            { id: "ALR-050", name: "【DB告警】exporter 进程异常", priority: "P0", category: "Database-Exporter", team: "DBA", metric: "Process", threshold: "Exporter process abnormal", duration: "Instant", expression: `up{job=~".*exporter.*"} == 0`, services: ["监控采集服务"], handbook: `# ALR-050【DB告警】exporter 进程异常

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-050 |
| **告警名称** | 【DB告警】数据库Exporter异常 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Database-Exporter |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

检测到Redis有客户端处于阻塞状态，通常由BLPOP/BRPOP等阻塞命令引起。

### 业务影响

- **连接占用**: 阻塞连接占用连接池资源
- **潜在死锁**: 可能导致业务逻辑等待

### 受影响服务

使用阻塞命令的消息队列类服务

### PromQL表达式

\`\`\`promql
redis_blocked_clients > 0
\`\`\`

### 常见根因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

### 实时数据库诊断

**关键RDS实例列表:**
\`\`\`
aws-luckyus-salesorder-rw     - 订单主库 (L0核心)
aws-luckyus-salespayment-rw   - 支付主库 (L0核心)
aws-luckyus-iriskcontrolservice-rw - 风控主库
aws-luckyus-framework01-rw    - 框架库01
aws-luckyus-framework02-rw    - 框架库02
\`\`\`

**查看所有RDS实例状态:**
\`\`\`bash
aws rds describe-db-instances \\
  --query 'DBInstances[?starts_with(DBInstanceIdentifier, \`aws-luckyus\`)].{ID:DBInstanceIdentifier,Status:DBInstanceStatus,Class:DBInstanceClass,CPU:toString(EngineVersion)}' \\
  --output table
\`\`\`

**查看特定实例的CPU指标:**
\`\`\`bash
# 替换 INSTANCE_ID 为实际的实例ID
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name CPUUtilization \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

**查看数据库连接数:**
\`\`\`bash
aws cloudwatch get-metric-statistics \\
  --namespace AWS/RDS \\
  --metric-name DatabaseConnections \\
  --dimensions Name=DBInstanceIdentifier,Value=aws-luckyus-salesorder-rw \\
  --start-time $(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\
  --period 60 \\
  --statistics Average Maximum
\`\`\`

## 根因分析

### 常见原因

1. **消息队列空**: 阻塞等待消息的客户端
2. **生产者故障**: 消息生产者停止发送消息
3. **超时设置**: 阻塞命令超时时间设置过长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-051", name: "【UPUSH】五分钟短信供应商调用失败超过阈值50", priority: "P1", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: ">50 failures in 5min", duration: "5m", expression: `sum(rate(sms_provider_call_failure[5m])) > 50`, services: ["短信服务"], handbook: `# ALR-051【UPUSH】五分钟短信供应商调用失败超过阈值50

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-051 |
| **告警名称** | 【UPUSH告警】SMS通道下发失败次数超过50次 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | SMS-UPUSH |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-052", name: "【UPUSH】五分钟短信供应商返回值失败超过阈值200", priority: "P1", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: ">200 return failures in 5min", duration: "5m", expression: `sum(rate(sms_provider_return_failure[5m])) > 200`, services: ["短信服务"], handbook: `# ALR-052【UPUSH】五分钟短信供应商返回值失败超过阈值200

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-052 |
| **告警名称** | 【UPUSH告警】SMS通道返回失败次数超过200次 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | SMS-UPUSH |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-053", name: "【UPUSH】营销短信回执成功率低于60%", priority: "P2", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: "<60% success rate", duration: "N/A", expression: `sms_marketing_receipt_success_rate < 0.6`, services: ["营销服务", "短信服务"], handbook: `# ALR-053【UPUSH】营销短信回执成功率低于60%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-053 |
| **告警名称** | 【UPUSH告警】营销短信回执率低于60% |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | SMS-UPUSH |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-054", name: "【UPUSH】营销短信过滤数超过100", priority: "P2", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: ">100 filtered", duration: "N/A", expression: `sms_marketing_filtered_count > 100`, services: ["营销服务"], handbook: `# ALR-054【UPUSH】营销短信过滤数超过100

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-054 |
| **告警名称** | 【UPUSH告警】营销短信被过滤次数超过100次 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | SMS-UPUSH |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-055", name: "【UPUSH】行业短信回执成功率低于70%", priority: "P1", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: "<70% success rate", duration: "N/A", expression: `sms_transactional_receipt_success_rate < 0.7`, services: ["短信服务"], handbook: `# ALR-055【UPUSH】行业短信回执成功率低于70%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-055 |
| **告警名称** | 【UPUSH告警】行业短信回执率低于70% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | SMS-UPUSH |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-056", name: "【UPUSH】行业短信过滤数超过50", priority: "P2", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: ">50 filtered", duration: "N/A", expression: `sms_transactional_filtered_count > 50`, services: ["短信服务"], handbook: `# ALR-056【UPUSH】行业短信过滤数超过50

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-056 |
| **告警名称** | 【UPUSH告警】行业短信被过滤次数超过50次 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | SMS-UPUSH |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-057", name: "【UPUSH】验证码发送量同比增加30%", priority: "P2", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: ">30% increase WoW", duration: "N/A", expression: `(sms_verification_count - sms_verification_count offset 1w) / sms_verification_count offset 1w > 0.3`, services: ["验证码服务"], handbook: `# ALR-057【UPUSH】验证码发送量同比增加30%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-057 |
| **告警名称** | 【UPUSH告警】验证码发送量低于30/分钟 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | SMS-UPUSH |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-058", name: "【UPUSH】验证码回执成功率低于70%", priority: "P1", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: "<70% success rate", duration: "N/A", expression: `sms_verification_receipt_success_rate < 0.7`, services: ["验证码服务"], handbook: `# ALR-058【UPUSH】验证码回执成功率低于70%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-058 |
| **告警名称** | 【UPUSH告警】验证码回执率低于70% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | SMS-UPUSH |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-059", name: "【UPUSH】验证码过滤数超过50", priority: "P2", category: "SMS-UPUSH", team: "UPUSH", metric: "SMS", threshold: ">50 filtered", duration: "N/A", expression: `sms_verification_filtered_count > 50`, services: ["验证码服务"], handbook: `# ALR-059【UPUSH】验证码过滤数超过50

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-059 |
| **告警名称** | 【UPUSH告警】验证码被过滤次数超过50次 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | SMS-UPUSH |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

五分钟内短信供应商调用失败次数超过阈值，短信发送可能受影响。

### 业务影响

- **验证码发送**: 用户无法收到验证码
- **营销短信**: 营销活动短信无法发送
- **通知服务**: 系统通知无法送达

### 受影响服务

短信服务、验证码服务、营销服务

### PromQL表达式

\`\`\`promql
sum_over_time(sms_provider_call_failures_total[5m]) > 50
\`\`\`

### 常见根因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查UPUSH服务状态
kubectl get pods -n upush -o wide

# 检查UPUSH服务日志
kubectl logs -n upush -l app=upush-service --tail=100

# 检查短信通道状态
# 通过UPUSH管理后台查看通道状态和发送统计
\`\`\`

---

## 根因分析

### 常见原因

1. **供应商故障**: 短信供应商服务异常
2. **API限流**: 超过供应商API调用限制
3. **配置问题**: API密钥或配置错误
4. **网络问题**: 到供应商网络不通

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-060", name: "【iZeus-策略1】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-060【iZeus-策略1】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-060 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略1) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-061", name: "【iZeus-策略2】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-061【iZeus-策略2】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-061 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略2) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-062", name: "【iZeus-策略3】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-062【iZeus-策略3】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-062 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略3) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-063", name: "【iZeus-策略4】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-063【iZeus-策略4】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-063 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略4) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-064", name: "【iZeus-策略5】服务每分钟异常数大于3", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">3 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 3`, services: ["核心服务"], handbook: `# ALR-064【iZeus-策略5】服务每分钟异常数大于3

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-064 |
| **告警名称** | 【iZeus告警】异常数量超过3个 (策略5) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-065", name: "【iZeus-策略6】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-065【iZeus-策略6】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-065 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略6) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-066", name: "【iZeus-策略7】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-066【iZeus-策略7】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-066 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略7) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-067", name: "【iZeus-策略8】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-067【iZeus-策略8】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-067 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略8) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-068", name: "【iZeus-策略9】服务每分钟异常数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-068【iZeus-策略9】服务每分钟异常数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-068 |
| **告警名称** | 【iZeus告警】异常数量超过2个 (策略9) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-069", name: "【iZeus-策略10】JVM CPU使用率大于20", priority: "P2", category: "APM-iZeus", team: "iZeus", metric: "JVM", threshold: ">20% CPU", duration: "N/A", expression: `jvm_cpu_usage > 20`, services: ["Java服务"], handbook: `# ALR-069【iZeus-策略10】JVM CPU使用率大于20

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-069 |
| **告警名称** | 【iZeus告警】JVM CPU使用率超过20% (策略10) |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-070", name: "【iZeus-策略10】服务响应时间大于1500ms", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "Latency", threshold: ">1500ms", duration: "N/A", expression: `izeus_service_response_time_p99 > 1500`, services: ["核心服务"], handbook: `# ALR-070【iZeus-策略10】服务响应时间大于1500ms

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-070 |
| **告警名称** | 【iZeus告警】响应时间超过1500ms (策略10) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-071", name: "【iZeus-策略11】端点每分钟失败数大于等于1", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">=1 failure/min", duration: "1m", expression: `izeus_endpoint_failure_count_per_minute >= 1`, services: ["核心服务"], handbook: `# ALR-071【iZeus-策略11】端点每分钟失败数大于等于1

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-071 |
| **告警名称** | 【iZeus告警】端点错误数超过1 (策略11) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-072", name: "【iZeus-策略12】端点每分钟失败数大于等于1", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">=1 failure/min", duration: "1m", expression: `izeus_endpoint_failure_count_per_minute >= 1`, services: ["核心服务"], handbook: `# ALR-072【iZeus-策略12】端点每分钟失败数大于等于1

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-072 |
| **告警名称** | 【iZeus告警】端点错误数超过1 (策略12) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-073", name: "【iZeus-策略15】服务每分钟异常数大于3", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">3 exceptions/min", duration: "1m", expression: `izeus_service_exception_count_per_minute > 3`, services: ["核心服务"], handbook: `# ALR-073【iZeus-策略15】服务每分钟异常数大于3

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-073 |
| **告警名称** | 【iZeus告警】异常数量超过3个 (策略15) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-074", name: "【iZeus-策略16】端点每分钟失败数大于2", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">2 failures/min", duration: "1m", expression: `izeus_endpoint_failure_count_per_minute > 2`, services: ["核心服务"], handbook: `# ALR-074【iZeus-策略16】端点每分钟失败数大于2

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-074 |
| **告警名称** | 【iZeus告警】端点错误数超过2 (策略16) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-075", name: "【iZeus-策略17】端点每分钟失败数大于3", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "APM", threshold: ">3 failures/min", duration: "1m", expression: `izeus_endpoint_failure_count_per_minute > 3`, services: ["核心服务"], handbook: `# ALR-075【iZeus-策略17】端点每分钟失败数大于3

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-075 |
| **告警名称** | 【iZeus告警】端点错误数超过3 (策略17) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Strategy |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-076", name: "【iZeus】Node-CPU-85 节点CPU使用率大于85%", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "CPU", threshold: ">85%", duration: "N/A", expression: `izeus_node_cpu_usage > 85`, services: ["iZeus节点"], handbook: `# ALR-076【iZeus】Node-CPU-85 节点CPU使用率大于85%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-076 |
| **告警名称** | 【iZeus告警】节点CPU使用率超过85% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-077", name: "【iZeus】Node-Disk-85 节点磁盘使用率大于85%", priority: "P2", category: "APM-iZeus", team: "iZeus", metric: "Disk", threshold: ">85%", duration: "N/A", expression: `izeus_node_disk_usage > 85`, services: ["iZeus节点"], handbook: `# ALR-077【iZeus】Node-Disk-85 节点磁盘使用率大于85%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-077 |
| **告警名称** | 【iZeus告警】节点磁盘使用率超过85% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-078", name: "【iZeus】Node-Memory-95 节点内存使用率大于95%", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "Memory", threshold: ">95%", duration: "N/A", expression: `izeus_node_memory_usage > 95`, services: ["iZeus节点"], handbook: `# ALR-078【iZeus】Node-Memory-95 节点内存使用率大于95%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-078 |
| **告警名称** | 【iZeus告警】节点内存使用率超过95% |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-079", name: "【iZeus】OAP-FGC-5 Full GC次数大于5", priority: "P2", category: "APM-iZeus", team: "iZeus", metric: "JVM", threshold: ">5 FGC", duration: "N/A", expression: `izeus_oap_fgc_count > 5`, services: ["iZeus OAP"], handbook: `# ALR-079【iZeus】OAP-FGC-5 Full GC次数大于5

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-079 |
| **告警名称** | 【iZeus告警】OAP Full GC次数超过5次 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-080", name: "【iZeus】Storage-Receiver2Thanos-Receiver-0", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "Pipeline", threshold: "Receiver failure", duration: "N/A", expression: `izeus_storage_receiver_thanos_status == 0`, services: ["iZeus存储"], handbook: `# ALR-080【iZeus】Storage-Receiver2Thanos-Receiver-0

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-080 |
| **告警名称** | 【iZeus告警】Thanos存储采集为0 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod在最近2分钟内发生了重启，可能是异常退出或健康检查失败。

### 业务影响

- **服务中断**: 重启期间服务不可用
- **请求失败**: 正在处理的请求将失败
- **流量切换**: K8s会将流量切走，可能造成其他Pod压力增加

### 受影响服务

重启的Pod所属服务及其依赖服务

### PromQL表达式

\`\`\`promql
increase(kube_pod_container_status_restarts_total[2m]) > 0
\`\`\`

### 常见根因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-081", name: "【iZeus】Storage-Receiver2VM-0", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "Pipeline", threshold: "VM receiver failure", duration: "N/A", expression: `izeus_storage_receiver_vm_status == 0`, services: ["iZeus存储"], handbook: `# ALR-081【iZeus】Storage-Receiver2VM-0

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-081 |
| **告警名称** | 【iZeus告警】VictoriaMetrics存储采集为0 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod在最近2分钟内发生了重启，可能是异常退出或健康检查失败。

### 业务影响

- **服务中断**: 重启期间服务不可用
- **请求失败**: 正在处理的请求将失败
- **流量切换**: K8s会将流量切走，可能造成其他Pod压力增加

### 受影响服务

重启的Pod所属服务及其依赖服务

### PromQL表达式

\`\`\`promql
increase(kube_pod_container_status_restarts_total[2m]) > 0
\`\`\`

### 常见根因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-082", name: "【iZeus】Transfer-Agent2OAP-0", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "Pipeline", threshold: "Agent transfer failure", duration: "N/A", expression: `izeus_transfer_agent_oap_status == 0`, services: ["iZeus Agent"], handbook: `# ALR-082【iZeus】Transfer-Agent2OAP-0

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-082 |
| **告警名称** | 【iZeus告警】Agent到OAP传输为0 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod在最近2分钟内发生了重启，可能是异常退出或健康检查失败。

### 业务影响

- **服务中断**: 重启期间服务不可用
- **请求失败**: 正在处理的请求将失败
- **流量切换**: K8s会将流量切走，可能造成其他Pod压力增加

### 受影响服务

重启的Pod所属服务及其依赖服务

### PromQL表达式

\`\`\`promql
increase(kube_pod_container_status_restarts_total[2m]) > 0
\`\`\`

### 常见根因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-083", name: "【iZeus】Transfer-OAP2OAP-0", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "Pipeline", threshold: "OAP transfer failure", duration: "N/A", expression: `izeus_transfer_oap_oap_status == 0`, services: ["iZeus OAP"], handbook: `# ALR-083【iZeus】Transfer-OAP2OAP-0

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-083 |
| **告警名称** | 【iZeus告警】OAP到OAP传输为0 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod在最近2分钟内发生了重启，可能是异常退出或健康检查失败。

### 业务影响

- **服务中断**: 重启期间服务不可用
- **请求失败**: 正在处理的请求将失败
- **流量切换**: K8s会将流量切走，可能造成其他Pod压力增加

### 受影响服务

重启的Pod所属服务及其依赖服务

### PromQL表达式

\`\`\`promql
increase(kube_pod_container_status_restarts_total[2m]) > 0
\`\`\`

### 常见根因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-084", name: "【iZeus】Transfer-OAPTrace2Receiver-0", priority: "P1", category: "APM-iZeus", team: "iZeus", metric: "Pipeline", threshold: "Trace transfer failure", duration: "N/A", expression: `izeus_transfer_oap_trace_receiver_status == 0`, services: ["iZeus Trace"], handbook: `# ALR-084【iZeus】Transfer-OAPTrace2Receiver-0

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-084 |
| **告警名称** | 【iZeus告警】Trace Receiver传输为0 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | APM-iZeus-Infra |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod在最近2分钟内发生了重启，可能是异常退出或健康检查失败。

### 业务影响

- **服务中断**: 重启期间服务不可用
- **请求失败**: 正在处理的请求将失败
- **流量切换**: K8s会将流量切走，可能造成其他Pod压力增加

### 受影响服务

重启的Pod所属服务及其依赖服务

### PromQL表达式

\`\`\`promql
increase(kube_pod_container_status_restarts_total[2m]) > 0
\`\`\`

### 常见根因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-085", name: "【默认策略】FGC次数大于0或YGC耗时大于500毫秒", priority: "P2", category: "APM-Default", team: "应用运维", metric: "JVM", threshold: "FGC>0 or YGC>500ms", duration: "N/A", expression: `jvm_gc_fgc_count > 0 or jvm_gc_ygc_time > 500`, services: ["Java服务"], handbook: `# ALR-085【默认策略】FGC次数大于0或YGC耗时大于500毫秒

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-085 |
| **告警名称** | 【默认策略】JVM GC告警 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Default-Strategy |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-086", name: "【默认策略】异常okhttp总数大于等于50", priority: "P2", category: "APM-Default", team: "应用运维", metric: "HTTP", threshold: ">=50 errors", duration: "N/A", expression: `okhttp_exception_count >= 50`, services: ["HTTP服务"], handbook: `# ALR-086【默认策略】异常okhttp总数大于等于50

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-086 |
| **告警名称** | 【默认策略】OkHttp错误数超过50 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Default-Strategy |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-087", name: "【默认策略】服务器每分钟异常数大于20", priority: "P1", category: "APM-Default", team: "应用运维", metric: "APM", threshold: ">20 exceptions/min", duration: "1m", expression: `server_exception_count_per_minute > 20`, services: ["核心服务"], handbook: `# ALR-087【默认策略】服务器每分钟异常数大于20

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-087 |
| **告警名称** | 【默认策略】异常数量超过20个 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Default-Strategy |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-088", name: "【默认策略】服务器每分钟异常数大于5", priority: "P2", category: "APM-Default", team: "应用运维", metric: "APM", threshold: ">5 exceptions/min", duration: "1m", expression: `server_exception_count_per_minute > 5`, services: ["核心服务"], handbook: `# ALR-088【默认策略】服务器每分钟异常数大于5

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-088 |
| **告警名称** | 【默认策略】异常数量超过5个 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Default-Strategy |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-089", name: "【pod-cpu-兜底】P0 CPU使用率连续3分钟大于85%", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "CPU", threshold: ">85% for 3min", duration: "3m", expression: `avg_over_time(container_cpu_usage_percent[3m]) > 85`, services: ["容器服务"], handbook: `# ALR-089【pod-cpu-兜底】P0 CPU使用率连续3分钟大于85%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-089 |
| **告警名称** | 【Pod告警】CPU使用率超过85% (Fallback) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Pod |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-090", name: "【pod-cpu】P0 CPU使用率连续10分钟大于50%", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "CPU", threshold: ">50% for 10min", duration: "10m", expression: `avg_over_time(container_cpu_usage_percent[10m]) > 50`, services: ["容器服务"], handbook: `# ALR-090【pod-cpu】P0 CPU使用率连续10分钟大于50%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-090 |
| **告警名称** | 【Pod告警】CPU使用率持续10分钟超过50% |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Pod |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-091", name: "【pod-cpu】P0 CPU使用率连续3分钟大于70%", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "CPU", threshold: ">70% for 3min", duration: "3m", expression: `avg_over_time(container_cpu_usage_percent[3m]) > 70`, services: ["容器服务"], handbook: `# ALR-091【pod-cpu】P0 CPU使用率连续3分钟大于70%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-091 |
| **告警名称** | 【Pod告警】CPU使用率持续3分钟超过70% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Pod |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-092", name: "【pod-全局】P0 node节点up心跳丢失需检查节点是否宕机", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "Availability", threshold: "Node heartbeat lost", duration: "Instant", expression: `up{job="kubernetes-nodes"} == 0`, services: ["K8S节点"], handbook: `# ALR-092【pod-全局】P0 node节点up心跳丢失需检查节点是否宕机

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-092 |
| **告警名称** | 【Pod告警】Node心跳丢失 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Pod |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod在最近2分钟内发生了重启，可能是异常退出或健康检查失败。

### 业务影响

- **服务中断**: 重启期间服务不可用
- **请求失败**: 正在处理的请求将失败
- **流量切换**: K8s会将流量切走，可能造成其他Pod压力增加

### 受影响服务

重启的Pod所属服务及其依赖服务

### PromQL表达式

\`\`\`promql
increase(kube_pod_container_status_restarts_total[2m]) > 0
\`\`\`

### 常见根因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-093", name: "【pod-全局】P0 Pod 2m内发生重启请关注", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "Availability", threshold: "Pod restart in 2min", duration: "2m", expression: `increase(kube_pod_container_status_restarts_total[2m]) > 0`, services: ["容器服务"], handbook: `# ALR-093【pod-全局】P0 Pod 2m内发生重启请关注

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-093 |
| **告警名称** | 【Pod告警】Pod在2分钟内重启 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Pod |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod在最近2分钟内发生了重启，可能是异常退出或健康检查失败。

### 业务影响

- **服务中断**: 重启期间服务不可用
- **请求失败**: 正在处理的请求将失败
- **流量切换**: K8s会将流量切走，可能造成其他Pod压力增加

### 受影响服务

重启的Pod所属服务及其依赖服务

### PromQL表达式

\`\`\`promql
increase(kube_pod_container_status_restarts_total[2m]) > 0
\`\`\`

### 常见根因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

**根据实际案例分析（2025-09-16 iluckysentrybot-pd案例）：**

Pod重启可能由多种原因引起：
- 内存溢出（OOM）
- 健康检查失败
- 应用崩溃
- 资源限制触发
- Kubernetes调度变更

**服务等级说明：**
- L2（普通业务服务）：可按正常流程处理
- L0/L1（核心服务）：需要立即响应


### 常见原因

1. **OOM Kill**: 内存超限被系统杀死
2. **健康检查失败**: Liveness Probe失败
3. **应用崩溃**: 未捕获异常导致进程退出
4. **资源不足**: 节点资源不足触发驱逐

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 步骤1：确认告警信息

检查告警详情，确认：
- **告警等级**：P0/P1/P2
- **服务等级**：L0/L1/L2
- **Pod名称**
- **所属服务**

**打开Grafana链接查看告警详情**

### 步骤2：检查服务状态

**容器云平台检查：**
1. 登录容器云管理平台
2. 找到对应服务
3. 检查Pod状态：
   - Pod名称
   - 运行状态
   - 重启次数
   - CPU/内存使用率

### 步骤3：联系负责人

**通过服务树找到负责人信息：**
- 在LSOP → 服务树 → 基础信息中查找
- 在告警群组中询问是否知晓问题

### 步骤4：处理措施

**重启服务（如需要）：**

**方式一：持续交付平台操作：**
- 路径：PROD > LUCKY > WEB > [服务名]
- 选择受影响的Pod
- 执行滚动重启

**方式二：容器云平台操作：**
- 在Pod列表中选择对应实例
- 点击"重启"按钮
- 监控重启进度

**重启后验证：**
- 确认所有Pod恢复Running状态
- 检查健康检查endpoint响应正常
- 验证Grafana指标恢复正常

### 步骤5：问题解决确认

- [ ] 错误率恢复正常
- [ ] 响应时间符合预期
- [ ] 所有Pod运行正常
- [ ] 健康检查通过


---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-094", name: "【pod-宕机】P1 WSS内存使用率连续3分钟等于100%(OOM参考)", priority: "P1", category: "Pod/Container", team: "K8S运维", metric: "Memory", threshold: "100% for 3min (OOM)", duration: "3m", expression: `avg_over_time(container_memory_working_set_percent[3m]) == 100`, services: ["容器服务"], handbook: `# ALR-094【pod-宕机】P1 WSS内存使用率连续3分钟等于100%(OOM参考)

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-094 |
| **告警名称** | 【Pod告警】Pod内存OOM |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Pod |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod WSS(Working Set Size)内存使用率连续3分钟达到100%，即将或已经OOM。

### 业务影响

- **服务崩溃**: OOM会导致Pod被杀死
- **数据丢失**: 内存中未持久化的数据将丢失
- **级联影响**: 可能导致依赖服务调用失败

### 受影响服务

OOM的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (container_memory_working_set_bytes{container!="POD",container!=""}
  /
  kube_pod_container_resource_limits{resource="memory"} * 100)[3m:]
) >= 100
\`\`\`

### 常见根因

1. **内存泄漏**: 代码存在内存泄漏
2. **JVM配置**: 堆大小超出容器限制
3. **大对象**: 处理大文件或大数据集
4. **缓存过大**: 本地缓存未设置上限

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

**根据实际案例分析（2025-09-23 hello-world案例）：**

**注意：** 容器名字叫hello-world时，很可能是测试服务。在处理此类告警时应优先确认服务性质和业务影响范围。

**OOM（内存溢出）常见原因：**
- 内存限制配置过低
- 应用存在内存泄漏
- 突发流量导致内存暴涨
- JVM堆内存配置不当


### 常见原因

1. **内存泄漏**: 代码存在内存泄漏
2. **JVM配置**: 堆大小超出容器限制
3. **大对象**: 处理大文件或大数据集
4. **缓存过大**: 本地缓存未设置上限

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 步骤1：确认告警信息

检查告警详情，确认：
- **告警等级**：P1
- **Pod信息**：Pod名称、Pod IP、所属服务
- **当前值**：如 +Inf（表示内存使用率达到100%）

### 步骤2：初步分析

**容器名称分析：**
- 注意容器名称，如 hello-world 可能是测试环境
- 评估业务影响

**初步判断：**
- 很可能是测试环境或测试服务
- 需要确认是否影响生产业务

### 步骤3：问题调研

**联系相关团队：**
- 在告警群组中询问是否知晓问题
- 确认服务归属（如dify相关服务）

### 步骤4：检查服务状态

**容器云平台检查：**
- 登录容器云管理平台
- 找到对应服务和Pod
- 检查：
  - Pod运行状态
  - 内存使用情况
  - CPU使用率
  - 重启次数
  - 是否存在OOM Kill

### 步骤5：处理措施

**如果是测试服务：**
- 确认测试完成后可以清理
- 通知相关开发人员

**如果是正式服务出现OOM：**
- 检查内存限制配置
- 分析内存泄漏可能性
- 考虑扩容或重启操作

**容器操作（如需要）：**
- 在Pod列表中选择对应实例
- 执行重启或扩容操作
- 监控处理进度

### 步骤6：问题解决确认

- [ ] 内存使用率恢复正常
- [ ] Pod运行状态正常
- [ ] 无业务影响
- [ ] 告警解除

### 总结

**后续建议：** 建议测试环境完善资源限制配置，避免类似告警


---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-095", name: "【pod-线程】P0 容器线程数连续3分钟超过3600", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "Threads", threshold: ">3600 for 3min", duration: "3m", expression: `avg_over_time(container_threads[3m]) > 3600`, services: ["容器服务"], handbook: `# ALR-095【pod-线程】P0 容器线程数连续3分钟超过3600

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-095 |
| **告警名称** | 【Pod告警】Pod线程数超过3600 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Pod |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-096", name: "【pod-网卡】P0 分区写入速率连续3分钟大于50MBs", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "Disk IO", threshold: ">50MB/s for 3min", duration: "3m", expression: `avg_over_time(container_fs_writes_bytes_rate[3m]) > 52428800`, services: ["容器服务"], handbook: `# ALR-096【pod-网卡】P0 分区写入速率连续3分钟大于50MBs

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-096 |
| **告警名称** | 【Pod告警】Pod IO写入超过50MB/s |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Pod |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-097", name: "【pod-网卡】P0 分区读取速率连续3分钟大于50MBs", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "Disk IO", threshold: ">50MB/s for 3min", duration: "3m", expression: `avg_over_time(container_fs_reads_bytes_rate[3m]) > 52428800`, services: ["容器服务"], handbook: `# ALR-097【pod-网卡】P0 分区读取速率连续3分钟大于50MBs

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-097 |
| **告警名称** | 【Pod告警】Pod IO读取超过50MB/s |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Pod |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-098", name: "【pod-网卡】P0 网卡流入速率连续3分钟大于30MBs", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "Network", threshold: ">30MB/s for 3min", duration: "3m", expression: `avg_over_time(container_network_receive_bytes_rate[3m]) > 31457280`, services: ["容器服务"], handbook: `# ALR-098【pod-网卡】P0 网卡流入速率连续3分钟大于30MBs

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-098 |
| **告警名称** | 【Pod告警】Pod入站流量超过30MB/s |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Pod |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-099", name: "【pod-网卡】P0 网卡流出速率连续3分钟大于30MBs", priority: "P0", category: "Pod/Container", team: "K8S运维", metric: "Network", threshold: ">30MB/s for 3min", duration: "3m", expression: `avg_over_time(container_network_transmit_bytes_rate[3m]) > 31457280`, services: ["容器服务"], handbook: `# ALR-099【pod-网卡】P0 网卡流出速率连续3分钟大于30MBs

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-099 |
| **告警名称** | 【Pod告警】Pod出站流量超过30MB/s |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Pod |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查Pod状态
kubectl get pods -n [NAMESPACE] -o wide

# 检查Pod日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Pod详情
kubectl describe pod -n [NAMESPACE] [POD_NAME]

# 检查Pod资源使用
kubectl top pods -n [NAMESPACE]

# 检查Node资源
kubectl top nodes
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 检查Pod资源使用: \`kubectl top pods\`

**步骤 2:** 查看应用日志排查CPU密集操作

**步骤 3:** 分析线程堆栈: \`jstack [PID]\`

**步骤 4:** 优化代码或增加Pod资源限制

**步骤 5:** 考虑水平扩容增加副本数

### Pod重启

**步骤 1:** 检查重启原因: \`kubectl describe pod\`

**步骤 2:** 查看之前容器日志: \`kubectl logs --previous\`

**步骤 3:** 检查是否OOM或健康检查失败

**步骤 4:** 调整资源配置或修复应用问题

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 合理配置资源请求和限制
- 实施应用性能监控
- 定期进行容量评估
- 配置健康检查
- 建立自动扩容机制
- 优化JVM参数配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【Pod告警】CPU使用率超过85%\`
- \`【Pod告警】Pod内存OOM\`
- \`【Pod告警】Pod在2分钟内重启\`
- \`【Pod告警】Node心跳丢失\`
- \`【Pod告警】Pod线程数超过3600\`
` },
            { id: "ALR-100", name: "【vm-CPU】P1 CPU平均负载大于CPU核心数量的1倍已持续5分钟", priority: "P1", category: "VM/Host", team: "系统运维", metric: "CPU", threshold: "Load > cores for 5min", duration: "5m", expression: `avg_over_time(node_load1[5m]) > count(node_cpu_seconds_total{mode="idle"})`, services: ["EC2实例"], handbook: `# ALR-100【vm-CPU】P1 CPU平均负载大于CPU核心数量的1倍已持续5分钟

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-100 |
| **告警名称** | 【VM告警】CPU负载超过1倍核心数 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-101", name: "【vm-CPU】P1 服务整体CPU平均使用率超过80%", priority: "P1", category: "VM/Host", team: "系统运维", metric: "CPU", threshold: ">80% avg", duration: "N/A", expression: `avg(100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80`, services: ["EC2实例"], handbook: `# ALR-101【vm-CPU】P1 服务整体CPU平均使用率超过80%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-101 |
| **告警名称** | 【VM告警】CPU平均使用率超过80% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-102", name: "【vm-cpu】P0 5分钟内服务CPU_iowait每秒的使用率大于80%", priority: "P0", category: "VM/Host", team: "系统运维", metric: "CPU", threshold: ">80% iowait", duration: "5m", expression: `avg_over_time((irate(node_cpu_seconds_total{mode="iowait"}[5m]) * 100)[5m:]) > 80`, services: ["EC2实例"], handbook: `# ALR-102【vm-cpu】P0 5分钟内服务CPU_iowait每秒的使用率大于80%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-102 |
| **告警名称** | 【VM告警】CPU IOWait超过80% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-103", name: "【vm-cpu】P0 服务CPU使用率窃取大于10%", priority: "P0", category: "VM/Host", team: "系统运维", metric: "CPU", threshold: ">10% steal", duration: "N/A", expression: `avg(irate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10`, services: ["EC2实例"], handbook: `# ALR-103【vm-cpu】P0 服务CPU使用率窃取大于10%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-103 |
| **告警名称** | 【VM告警】CPU Steal超过10% |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-104", name: "【vm-fileSystem】P0 分区inodes使用率大于95%请立即处理", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Disk", threshold: ">95% inodes", duration: "Instant", expression: `(1 - node_filesystem_files_free / node_filesystem_files) * 100 > 95`, services: ["EC2实例"], handbook: `# ALR-104【vm-fileSystem】P0 分区inodes使用率大于95%请立即处理

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-104 |
| **告警名称** | 【VM告警】Inode使用率超过95% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-105", name: "【vm-fileSystem】P0 分区发送只读事件请检查分区读写情况", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Disk", threshold: "Read-only event", duration: "Instant", expression: `node_filesystem_readonly == 1`, services: ["EC2实例"], handbook: `# ALR-105【vm-fileSystem】P0 分区发送只读事件请检查分区读写情况

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-105 |
| **告警名称** | 【VM告警】文件系统只读 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机磁盘分区使用率超过90%，存储空间紧张。

### 业务影响

- **服务异常**: 日志写入失败，服务可能崩溃
- **数据丢失**: 新数据无法写入

### 受影响服务

运行在该主机上的所有服务

### PromQL表达式

\`\`\`promql
(1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 90
\`\`\`

### 常见根因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-106", name: "【vm-io】P0 服务io耗时大于90ms且同比超过20ms", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Disk IO", threshold: ">90ms and >20ms WoW", duration: "N/A", expression: `irate(node_disk_io_time_seconds_total[5m]) * 1000 > 90`, services: ["EC2实例"], handbook: `# ALR-106【vm-io】P0 服务io耗时大于90ms且同比超过20ms

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-106 |
| **告警名称** | 【VM告警】IO延迟超过90ms |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机磁盘分区使用率超过90%，存储空间紧张。

### 业务影响

- **服务异常**: 日志写入失败，服务可能崩溃
- **数据丢失**: 新数据无法写入

### 受影响服务

运行在该主机上的所有服务

### PromQL表达式

\`\`\`promql
(1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 90
\`\`\`

### 常见根因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-107", name: "【vm-io】P1 磁盘IO使用率大于70%且同比超过20", priority: "P1", category: "VM/Host", team: "系统运维", metric: "Disk IO", threshold: ">70% and >20 WoW", duration: "N/A", expression: `irate(node_disk_io_time_seconds_total[5m]) * 100 > 70`, services: ["EC2实例"], handbook: `# ALR-107【vm-io】P1 磁盘IO使用率大于70%且同比超过20

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-107 |
| **告警名称** | 【VM告警】IO使用率超过70% |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机磁盘分区使用率超过90%，存储空间紧张。

### 业务影响

- **服务异常**: 日志写入失败，服务可能崩溃
- **数据丢失**: 新数据无法写入

### 受影响服务

运行在该主机上的所有服务

### PromQL表达式

\`\`\`promql
(1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 90
\`\`\`

### 常见根因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-108", name: "【vm-tcp】P0 TCP每秒重传报文数超过200", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Network", threshold: ">200 retrans/s", duration: "Instant", expression: `rate(node_netstat_Tcp_RetransSegs[5m]) > 200`, services: ["EC2实例"], handbook: `# ALR-108【vm-tcp】P0 TCP每秒重传报文数超过200

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-108 |
| **告警名称** | 【VM告警】TCP重传次数超过200 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-109", name: "【vm-内存】P1 内存使用率大于90% 持续10分钟", priority: "P1", category: "VM/Host", team: "系统运维", metric: "Memory", threshold: ">90% for 10min", duration: "10m", expression: `avg_over_time((1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100[10m]) > 90`, services: ["EC2实例"], handbook: `# ALR-109【vm-内存】P1 内存使用率大于90% 持续10分钟

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-109 |
| **告警名称** | 【VM告警】内存使用率持续10分钟超过90% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-110", name: "【vm-宕机】P0 up监控指标心跳丢失10分钟需检查设备是否宕机", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Availability", threshold: "Heartbeat lost 10min", duration: "10m", expression: `up{job="node"} == 0`, services: ["EC2实例"], handbook: `# ALR-110【vm-宕机】P0 up监控指标心跳丢失10分钟需检查设备是否宕机

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-110 |
| **告警名称** | 【VM告警】心跳丢失超过10分钟 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-111", name: "【vm-磁盘】P1 分区使用率大于90%请手动处理", priority: "P1", category: "VM/Host", team: "系统运维", metric: "Disk", threshold: ">90%", duration: "Instant", expression: `(1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 90`, services: ["EC2实例"], handbook: `# ALR-111【vm-磁盘】P1 分区使用率大于90%请手动处理

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-111 |
| **告警名称** | 【VM告警】磁盘使用率超过90% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机磁盘分区使用率超过90%，存储空间紧张。

### 业务影响

- **服务异常**: 日志写入失败，服务可能崩溃
- **数据丢失**: 新数据无法写入

### 受影响服务

运行在该主机上的所有服务

### PromQL表达式

\`\`\`promql
(1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 90
\`\`\`

### 常见根因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **日志堆积**: 应用日志未及时清理
2. **容器镜像**: 过多的容器镜像占用空间
3. **临时文件**: 临时文件未清理
4. **数据增长**: 应用数据文件增长

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-112", name: "【vm-网卡】P0 入方向在5分钟内每秒丢弃的数据包大于20个", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Network", threshold: ">20 drops/s in", duration: "5m", expression: `rate(node_network_receive_drop_total[5m]) > 20`, services: ["EC2实例"], handbook: `# ALR-112【vm-网卡】P0 入方向在5分钟内每秒丢弃的数据包大于20个

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-112 |
| **告警名称** | 【VM告警】入站网络丢包超过20 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-113", name: "【vm-网卡】P0 入方向在5分钟内每秒错误的数据包大于20个", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Network", threshold: ">20 errors/s in", duration: "5m", expression: `rate(node_network_receive_errs_total[5m]) > 20`, services: ["EC2实例"], handbook: `# ALR-113【vm-网卡】P0 入方向在5分钟内每秒错误的数据包大于20个

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-113 |
| **告警名称** | 【VM告警】入站网络错误超过20 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-114", name: "【vm-网卡】P0 出方向在5分钟内每秒丢弃的数据包大于20个", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Network", threshold: ">20 drops/s out", duration: "5m", expression: `rate(node_network_transmit_drop_total[5m]) > 20`, services: ["EC2实例"], handbook: `# ALR-114【vm-网卡】P0 出方向在5分钟内每秒丢弃的数据包大于20个

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-114 |
| **告警名称** | 【VM告警】出站网络丢包超过20 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-115", name: "【vm-网卡】P0 出方向在5分钟内每秒错误的数据包大于20个", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Network", threshold: ">20 errors/s out", duration: "5m", expression: `rate(node_network_transmit_errs_total[5m]) > 20`, services: ["EC2实例"], handbook: `# ALR-115【vm-网卡】P0 出方向在5分钟内每秒错误的数据包大于20个

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-115 |
| **告警名称** | 【VM告警】出站网络错误超过20 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | VM |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-116", name: "【vm-网卡】P0 网卡状态为down", priority: "P0", category: "VM/Host", team: "系统运维", metric: "Network", threshold: "NIC down", duration: "Instant", expression: `node_network_up == 0`, services: ["EC2实例"], handbook: `# ALR-116【vm-网卡】P0 网卡状态为down

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-116 |
| **告警名称** | 【VM告警】网卡Down |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | VM |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查CPU使用率
top -bn1 | head -20

# 检查内存使用
free -h

# 检查磁盘使用
df -h

# 检查IO状态
iostat -x 1 5

# 检查网络连接
netstat -tunlp | head -20
ss -tunlp | head -20
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### CPU使用率过高

**步骤 1:** 使用 \`top\` 或 \`htop\` 查看进程CPU使用

**步骤 2:** 分析高CPU进程

**步骤 3:** 检查是否有异常进程

**步骤 4:** 优化应用或增加资源

### 磁盘空间不足

**步骤 1:** 检查磁盘使用: \`df -h\`

**步骤 2:** 查找大文件: \`du -sh /* | sort -rh | head -20\`

**步骤 3:** 清理日志文件和临时文件

**步骤 4:** 考虑扩容磁盘

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 监控资源使用趋势
- 定期清理日志和临时文件
- 配置自动扩容策略
- 建立资源使用告警
- 定期进行系统维护
- 优化应用配置

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【VM告警】CPU平均使用率超过80%\`
- \`【VM告警】内存使用率持续10分钟超过90%\`
- \`【VM告警】磁盘使用率超过90%\`
- \`【VM告警】心跳丢失超过10分钟\`
- \`【VM告警】文件系统只读\`
` },
            { id: "ALR-117", name: "【北美-业务告警】取消订单持续5分钟大于1单", priority: "P1", category: "Business", team: "业务运维", metric: "Business", threshold: ">1 cancel for 5min", duration: "5m", expression: `sum(rate(order_cancel_count[5m])) > 1`, services: ["订单服务"], handbook: `# ALR-117【北美-业务告警】取消订单持续5分钟大于1单

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-117 |
| **告警名称** | 【业务告警】取消订单数异常 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Business |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查订单服务状态
kubectl get pods -n sales -l app=isalesorderservice -o wide

# 检查订单服务日志
kubectl logs -n sales -l app=isalesorderservice --tail=100

# 检查业务指标
# 通过Grafana Business Metrics仪表板查看
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【业务告警】完成订单数为0\`
- \`【业务告警】用户注册数为0\`
- \`【业务告警】取消订单数异常\`
- \`【业务告警】支付金额异常\`
- \`【风控告警】全局熔断\`
` },
            { id: "ALR-118", name: "【北美-业务告警】新建-付款-完成订单持续10分钟少于1单", priority: "P0", category: "Business", team: "业务运维", metric: "Business", threshold: "<1 order for 10min", duration: "10m", expression: `sum(rate(order_complete_count[10m])) < 1/600`, services: ["订单服务", "支付服务"], handbook: `# ALR-118【北美-业务告警】新建-付款-完成订单持续10分钟少于1单

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-118 |
| **告警名称** | 【业务告警】完成订单数为0 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Business |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查订单服务状态
kubectl get pods -n sales -l app=isalesorderservice -o wide

# 检查订单服务日志
kubectl logs -n sales -l app=isalesorderservice --tail=100

# 检查业务指标
# 通过Grafana Business Metrics仪表板查看
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【业务告警】完成订单数为0\`
- \`【业务告警】用户注册数为0\`
- \`【业务告警】取消订单数异常\`
- \`【业务告警】支付金额异常\`
- \`【风控告警】全局熔断\`
` },
            { id: "ALR-119", name: "【北美-业务告警】订单支持持续10分钟小于1", priority: "P0", category: "Business", team: "业务运维", metric: "Business", threshold: "<1 payment for 10min", duration: "10m", expression: `sum(rate(order_payment_count[10m])) < 1/600`, services: ["支付服务"], handbook: `# ALR-119【北美-业务告警】订单支持持续10分钟小于1

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-119 |
| **告警名称** | 【业务告警】订单支持数为0 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Business |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查订单服务状态
kubectl get pods -n sales -l app=isalesorderservice -o wide

# 检查订单服务日志
kubectl logs -n sales -l app=isalesorderservice --tail=100

# 检查业务指标
# 通过Grafana Business Metrics仪表板查看
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【业务告警】完成订单数为0\`
- \`【业务告警】用户注册数为0\`
- \`【业务告警】取消订单数异常\`
- \`【业务告警】支付金额异常\`
- \`【风控告警】全局熔断\`
` },
            { id: "ALR-120", name: "【北美-业务告警】过去10分钟注册数为0", priority: "P1", category: "Business", team: "业务运维", metric: "Business", threshold: "0 registrations in 10min", duration: "10m", expression: `sum(increase(user_registration_count[10m])) == 0`, services: ["会员服务"], handbook: `# ALR-120【北美-业务告警】过去10分钟注册数为0

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-120 |
| **告警名称** | 【业务告警】用户注册数为0 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Business |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

过去10分钟新用户注册数为0，用户获取链路可能中断。

### 业务影响

- **用户增长停滞**: 无法获取新用户
- **营销失效**: 推广活动无法转化

### 受影响服务

会员服务、认证服务、短信服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_registration_count[10m]) == 0
\`\`\`

### 常见根因

1. **认证服务故障**: unionauth服务异常
2. **短信服务故障**: 验证码无法发送
3. **App故障**: 客户端注册页面异常
4. **Redis故障**: session Redis异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查订单服务状态
kubectl get pods -n sales -l app=isalesorderservice -o wide

# 检查订单服务日志
kubectl logs -n sales -l app=isalesorderservice --tail=100

# 检查业务指标
# 通过Grafana Business Metrics仪表板查看
\`\`\`

---

## 根因分析

### 常见原因

1. **认证服务故障**: unionauth服务异常
2. **短信服务故障**: 验证码无法发送
3. **App故障**: 客户端注册页面异常
4. **Redis故障**: session Redis异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【业务告警】完成订单数为0\`
- \`【业务告警】用户注册数为0\`
- \`【业务告警】取消订单数异常\`
- \`【业务告警】支付金额异常\`
- \`【风控告警】全局熔断\`
` },
            { id: "ALR-121", name: "【北美-业务告警】过去5分钟支付金额小于500分", priority: "P1", category: "Business", team: "业务运维", metric: "Business", threshold: "<500 cents in 5min", duration: "5m", expression: `sum(increase(payment_amount_cents[5m])) < 500`, services: ["支付服务"], handbook: `# ALR-121【北美-业务告警】过去5分钟支付金额小于500分

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-121 |
| **告警名称** | 【业务告警】支付金额异常(超过500) |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Business |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

新建-付款-完成订单链路持续10分钟少于1单，核心交易链路可能中断。

### 业务影响

- **黄金流程中断**: 这是最严重的业务告警
- **收入损失**: 直接影响业务营收
- **用户体验**: 用户无法正常下单购买

### 受影响服务

订单服务、支付服务、商品服务、库存服务、会员服务

### PromQL表达式

\`\`\`promql
sum_over_time(business_completed_orders_total[10m]) < 1
\`\`\`

### 常见根因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查订单服务状态
kubectl get pods -n sales -l app=isalesorderservice -o wide

# 检查订单服务日志
kubectl logs -n sales -l app=isalesorderservice --tail=100

# 检查业务指标
# 通过Grafana Business Metrics仪表板查看
\`\`\`

---

## 根因分析

### 常见原因

1. **订单服务故障**: isales-order服务异常
2. **支付服务故障**: salespayment服务异常
3. **数据库故障**: salesorder-rw或salespayment-rw数据库异常
4. **Redis故障**: isales-order或session Redis故障
5. **上游依赖**: 认证服务、商品服务等上游服务异常
6. **外部支付通道**: 支付渠道(如Stripe)异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`【业务告警】完成订单数为0\`
- \`【业务告警】用户注册数为0\`
- \`【业务告警】取消订单数异常\`
- \`【业务告警】支付金额异常\`
- \`【风控告警】全局熔断\`
` },
            { id: "ALR-122", name: "【北美亚风控】全局策略熔断前预告警", priority: "P1", category: "Risk Control", team: "风控", metric: "Risk", threshold: "Global circuit breaker warning", duration: "N/A", expression: `risk_global_circuit_breaker_warning == 1`, services: ["风控服务"], handbook: `# ALR-122【北美亚风控】全局策略熔断前预告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-122 |
| **告警名称** | 【风控告警】全局预熔断 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-123", name: "【北美亚风控】全局策略熔断告警", priority: "P0", category: "Risk Control", team: "风控", metric: "Risk", threshold: "Global circuit breaker triggered", duration: "N/A", expression: `risk_global_circuit_breaker_active == 1`, services: ["风控服务"], handbook: `# ALR-123【北美亚风控】全局策略熔断告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-123 |
| **告警名称** | 【风控告警】全局熔断 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-124", name: "【北美亚风控】场景熔断前预告警", priority: "P1", category: "Risk Control", team: "风控", metric: "Risk", threshold: "Scene circuit breaker warning", duration: "N/A", expression: `risk_scene_circuit_breaker_warning == 1`, services: ["风控服务"], handbook: `# ALR-124【北美亚风控】场景熔断前预告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-124 |
| **告警名称** | 【风控告警】场景预熔断 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-125", name: "【北美风控】场景熔断告警", priority: "P0", category: "Risk Control", team: "风控", metric: "Risk", threshold: "Scene circuit breaker triggered", duration: "N/A", expression: `risk_scene_circuit_breaker_active == 1`, services: ["风控服务"], handbook: `# ALR-125【北美风控】场景熔断告警

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-125 |
| **告警名称** | 【风控告警】场景熔断 |
| **优先级** | P0 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 立即响应（< 5分钟） |

---

## 告警描述

此告警属于 **P0** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-126", name: "【国际化北美风控】下单rpc接口调用量超过200次且比上周多60%", priority: "P2", category: "Risk Control", team: "风控", metric: "Risk", threshold: ">200 calls and >60% WoW", duration: "N/A", expression: `risk_order_rpc_count > 200 and (risk_order_rpc_count - risk_order_rpc_count offset 1w) / risk_order_rpc_count offset 1w > 0.6`, services: ["风控服务", "订单服务"], handbook: `# ALR-126【国际化北美风控】下单rpc接口调用量超过200次且比上周多60%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-126 |
| **告警名称** | 【风控告警】订单RPC调用量飙升 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-127", name: "【国际化北美风控】支付rpc接口调用量超过200次且比上周多60%", priority: "P2", category: "Risk Control", team: "风控", metric: "Risk", threshold: ">200 calls and >60% WoW", duration: "N/A", expression: `risk_payment_rpc_count > 200 and (risk_payment_rpc_count - risk_payment_rpc_count offset 1w) / risk_payment_rpc_count offset 1w > 0.6`, services: ["风控服务", "支付服务"], handbook: `# ALR-127【国际化北美风控】支付rpc接口调用量超过200次且比上周多60%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-127 |
| **告警名称** | 【风控告警】支付RPC调用量飙升 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-128", name: "【国际化北美风控】注册rpc接口调用量超过100次且比上周多60%", priority: "P2", category: "Risk Control", team: "风控", metric: "Risk", threshold: ">100 calls and >60% WoW", duration: "N/A", expression: `risk_register_rpc_count > 100 and (risk_register_rpc_count - risk_register_rpc_count offset 1w) / risk_register_rpc_count offset 1w > 0.6`, services: ["风控服务", "会员服务"], handbook: `# ALR-128【国际化北美风控】注册rpc接口调用量超过100次且比上周多60%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-128 |
| **告警名称** | 【风控告警】注册RPC调用量飙升 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-129", name: "【国际化北美风控】登录rpc接口调用量超过100次且比上周多60%", priority: "P2", category: "Risk Control", team: "风控", metric: "Risk", threshold: ">100 calls and >60% WoW", duration: "N/A", expression: `risk_login_rpc_count > 100 and (risk_login_rpc_count - risk_login_rpc_count offset 1w) / risk_login_rpc_count offset 1w > 0.6`, services: ["风控服务", "会员服务"], handbook: `# ALR-129【国际化北美风控】登录rpc接口调用量超过100次且比上周多60%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-129 |
| **告警名称** | 【风控告警】登录RPC调用量飙升 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-130", name: "【国际化北美风控】短信rpc接口调用量超过100次且比上周多60%", priority: "P2", category: "Risk Control", team: "风控", metric: "Risk", threshold: ">100 calls and >60% WoW", duration: "N/A", expression: `risk_sms_rpc_count > 100 and (risk_sms_rpc_count - risk_sms_rpc_count offset 1w) / risk_sms_rpc_count offset 1w > 0.6`, services: ["风控服务", "短信服务"], handbook: `# ALR-130【国际化北美风控】短信rpc接口调用量超过100次且比上周多60%

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-130 |
| **告警名称** | 【风控告警】短信RPC调用量飙升 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Risk Control |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Pod CPU使用率连续3分钟超过资源限制的85%，这是一个兜底告警。

### 业务影响

- **服务限流**: 接近CPU限制，可能被K8s节流
- **响应延迟**: 服务处理能力下降
- **Pod重启风险**: 持续高负载可能触发OOM Killer

### 受影响服务

告警中指定的Pod所属服务

### PromQL表达式

\`\`\`promql
avg_over_time(
  (sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[1m])) by (pod,namespace)
  /
  sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod,namespace) * 100)[3m:]
) > 85
\`\`\`

### 常见根因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查风控服务状态
kubectl get pods -n risk -o wide

# 检查风控服务日志
kubectl logs -n risk -l app=risk-service --tail=100

# 检查风控熔断状态
# 通过风控管理后台查看熔断规则和触发状态
\`\`\`

---

## 根因分析

### 常见原因

1. **代码问题**: 存在CPU密集型逻辑或死循环
2. **流量突增**: 业务流量超出预期
3. **资源配置不当**: CPU limit设置过低
4. **GC问题**: JVM频繁GC消耗CPU

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-131", name: "【网关告警】错误率大于15%", priority: "P0", category: "Gateway/Network", team: "网关", metric: "Gateway", threshold: ">15% error rate", duration: "N/A", expression: `sum(rate(gateway_requests_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.15`, services: ["API网关"], handbook: `# ALR-131【网关告警】错误率大于15%

> **⭐ 高频告警** - 此告警在生产环境中频繁出现，已有详细处理案例和最佳实践。

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-131 |
| **告警名称** | 【网关告警】错误率超过15% |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Gateway |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

API网关错误率超过15%，大量请求返回5xx错误。

### 业务影响

- **服务降级**: 大量用户请求失败
- **用户体验**: App功能不可用
- **级联故障**: 可能是后端服务问题的表现

### 受影响服务

通过网关的所有API服务

### PromQL表达式

\`\`\`promql
(sum(rate(gateway_requests_total{status=~"5.."}[1m])) / sum(rate(gateway_requests_total[1m]))) * 100 > 15
\`\`\`

### 常见根因

1. **后端服务故障**: 上游服务不可用
2. **网关配置**: 路由配置错误
3. **容量问题**: 网关实例资源不足
4. **网络问题**: 网关到后端服务网络异常

---

## 立即响应

### 第一步: 评估黄金流程影响

**立即评估此告警是否影响黄金流程（用户下单流程）:**

\`\`\`
关键检查点:
1. 用户是否可以正常打开瑞幸咖啡App
2. 用户是否可以正常浏览菜单和选择商品
3. 用户是否可以正常下单
4. 用户是否可以正常支付

如果以上任何一个环节受阻，说明黄金流程受影响!
\`\`\`

**如果黄金流程受影响:**
- 这是严重事故，需要立即响应
- 通知中国团队所有相关成员（包括半夜唤醒）
- 启动紧急响应流程
- 同步升级至Team Lead

**如果黄金流程未受影响:**
- 可以按正常流程排查
- 观察告警是否自动恢复（部分告警可能是瞬时波动）
- 记录并分析是否为误报

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查API网关状态
kubectl get pods -n gateway -o wide

# 检查网关日志
kubectl logs -n gateway -l app=luckyapigateway --tail=100

# 检查网关错误率
# 通过Grafana API Gateway仪表板查看
\`\`\`

---

## 根因分析

**根据实际案例分析（2025-10-03 luckycapiproxy案例）：**

本次告警为**偶发性网关错误率告警**，影响范围无实际业务影响，根本原因疑似网络抖动或短暂超时，3分钟内自动恢复。

**关键经验：**
- 历史告警记录是重要参考
- 必须验证业务实际影响
- 多维度交叉验证（Grafana + iZeus + 实际测试）
- 3-5分钟观察期是合理的处理窗口
- 自动恢复的短暂告警可以不升级


### 常见原因

1. **后端服务故障**: 上游服务不可用
2. **网关配置**: 路由配置错误
3. **容量问题**: 网关实例资源不足
4. **网络问题**: 网关到后端服务网络异常

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 步骤1：检查历史告警记录

**登录iZeus可观测平台：**
1. 进入 告警配置 → 基础告警 → 告警策略
2. 搜索：【网关告警】错误率大于15%
3. 查看历史告警记录
4. 同时检查企业微信历史记录

**判断依据：**
- 如果历史上有相同告警且已解决，可能是偶发性问题
- 查询LSOP中的服务树查询更多这个服务的细节

### 步骤2：验证业务影响 - Grafana订单监控

**打开Grafana监控面板：**
- 导航至：活动保障大盘 / 北美-运维决策报表
- 查看**所有订单_状态**图表

**重点关注：**
- 新建量（绿线）
- 已付款（黄线）
- 已完成（蓝线）
- 已取消（橙线）

**注意：** 监控数据存在一定延迟，属于正常现象

### 步骤3：查看整体告警态势 - iZeus报错大盘

**在iZeus平台操作：**
1. 进入 监控大盘 → 报错大盘（分钟级视图）
2. 选择告警发生时间段
3. 查看各服务报错分布情况

**关注重点：**
- 是否有多个服务同时报错
- luckycapiproxy的报错数量和频率
- 是否存在级联故障

### 步骤4：实际业务验证 - 下单测试

**打开瑞幸咖啡APP：**
1. 选择商品
2. 尝试下单
3. 完成支付流程
4. 观察是否流畅

**测试重点：**
- 加载速度
- 支付成功率
- 是否有报错提示

### 步骤5：图表分析 - Thanos

**在告警配置页面：**
1. 点击**数据查询**按钮
2. 自动跳转到Thanos界面
3. 切换到 **Graph** 标签页
4. 调整时间范围为1小时
5. 点击 Execute 执行

**分析要点：**
- 是否有规律性波动
- 确认是否为重复性问题
- 识别异常峰值的持续时间

### 步骤6：持续观察与决策

| 情况 | 处理方式 |
|------|---------|
| 3分钟内恢复正常 | 继续观察，记录事件 |
| 持续3-5分钟且有业务影响 | 准备通知中国团队 |
| 持续超过5分钟 | 立即升级至中国技术团队 |
| 影响用户下单 | 立即升级 |

### 升级标准

**需要升级中国团队的情况：**
- 告警持续超过5分钟
- Grafana订单数据明显下降
- 实际下单测试失败
- 多个服务同时告警
- 错误率持续上升


---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-132", name: "【网络质量US-机房互拨】探测目标连续15s失败", priority: "P1", category: "Gateway/Network", team: "网络", metric: "Network", threshold: "Probe failed 15s", duration: "15s", expression: `probe_success == 0 for 15s`, services: ["网络探测"], handbook: `# ALR-132【网络质量US-机房互拨】探测目标连续15s失败

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-132 |
| **告警名称** | 【网络告警】网络探测失败超过15秒 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Network |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

VM主机整体CPU平均使用率超过80%，服务器负载过高。

### 业务影响

- **性能下降**: 所有运行在该主机上的服务性能下降
- **调度影响**: K8s可能无法在该节点调度新Pod

### 受影响服务

运行在该主机上的所有服务和Pod

### PromQL表达式

\`\`\`promql
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 80
\`\`\`

### 常见根因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **负载不均**: Pod分布不均匀
2. **资源争抢**: 多个高负载Pod同时运行
3. **系统进程**: 系统级进程占用过多CPU
4. **节点容量**: 节点资源规划不足

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-133", name: "【Grafana】Slow Query Spike - High Rate Alert", priority: "P2", category: "Grafana Native", team: "DBA", metric: "Performance", threshold: "Slow query spike", duration: "N/A", expression: `increase(mysql_slow_queries[5m]) > 100`, services: ["MySQL"], handbook: `# ALR-133【Grafana】Slow Query Spike - High Rate Alert

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-133 |
| **告警名称** | 【Grafana告警】慢查询数量飙升 |
| **优先级** | P2 |
| **服务等级** | L1 |
| **类别** | Grafana |
| **响应时间** | 标准响应（< 30分钟） |

---

## 告警描述

此告警属于 **P2** 优先级，影响 **L1** 级别服务。

---

## 告警解析

### 告警含义

Grafana原生告警: MySQL慢查询速率5分钟内超过阈值(>1或>2/秒)。

### 业务影响

- **数据库性能**: 慢查询累积影响整体数据库性能
- **服务延迟**: 依赖数据库的服务响应变慢
- **资源消耗**: CPU和IO资源被大量消耗

### 受影响服务

所有依赖MySQL的服务

### PromQL表达式

\`\`\`promql
sum(rate(mysql_global_status_slow_queries[5m])) by (instance) > 1
\`\`\`

### 常见根因

1. **索引缺失**: 查询未命中索引
2. **表锁竞争**: InnoDB行锁或表锁等待
3. **复杂查询**: JOIN过多或子查询嵌套
4. **数据量增长**: 表数据量增长导致查询变慢

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **索引缺失**: 查询未命中索引
2. **表锁竞争**: InnoDB行锁或表锁等待
3. **复杂查询**: JOIN过多或子查询嵌套
4. **数据量增长**: 表数据量增长导致查询变慢

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-134", name: "【Grafana】Slow Query Critical - Very High Rate Alert", priority: "P1", category: "Grafana Native", team: "DBA", metric: "Performance", threshold: "Critical slow query rate", duration: "N/A", expression: `increase(mysql_slow_queries[5m]) > 500`, services: ["MySQL"], handbook: `# ALR-134【Grafana】Slow Query Critical - Very High Rate Alert

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-134 |
| **告警名称** | 【Grafana告警】慢查询数量严重 |
| **优先级** | P1 |
| **服务等级** | L0 |
| **类别** | Grafana |
| **响应时间** | 快速响应（< 15分钟） |

---

## 告警描述

此告警属于 **P1** 优先级，影响 **L0** 级别服务。

---

## 告警解析

### 告警含义

Grafana原生告警: MySQL慢查询速率5分钟内超过阈值(>1或>2/秒)。

### 业务影响

- **数据库性能**: 慢查询累积影响整体数据库性能
- **服务延迟**: 依赖数据库的服务响应变慢
- **资源消耗**: CPU和IO资源被大量消耗

### 受影响服务

所有依赖MySQL的服务

### PromQL表达式

\`\`\`promql
sum(rate(mysql_global_status_slow_queries[5m])) by (instance) > 1
\`\`\`

### 常见根因

1. **索引缺失**: 查询未命中索引
2. **表锁竞争**: InnoDB行锁或表锁等待
3. **复杂查询**: JOIN过多或子查询嵌套
4. **数据量增长**: 表数据量增长导致查询变慢

---

## 立即响应

### 第一步: 评估黄金流程影响

**评估此告警对黄金流程（用户下单流程）的潜在影响:**

\`\`\`
检查点:
1. 相关服务是否在订单链路中
2. 当前异常是否已扩散
3. 是否有关联的P0/P1告警
\`\`\`

**如果可能影响黄金流程:**
- 提高响应优先级
- 准备通知相关团队
- 密切监控告警状态变化

**如果暂不影响黄金流程:**
- 按常规流程处理
- 观察告警是否自动恢复
- 如果5-10分钟内恢复，可能是瞬时波动，记录并关注

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **索引缺失**: 查询未命中索引
2. **表锁竞争**: InnoDB行锁或表锁等待
3. **复杂查询**: JOIN过多或子查询嵌套
4. **数据量增长**: 表数据量增长导致查询变慢

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` },
            { id: "ALR-135", name: "【Grafana】Slow Query Weekly Increase - WoW Spike Alert", priority: "P2", category: "Grafana Native", team: "DBA", metric: "Performance", threshold: "Slow query WoW increase", duration: "N/A", expression: `(sum(increase(mysql_slow_queries[1d])) - sum(increase(mysql_slow_queries[1d] offset 1w))) / sum(increase(mysql_slow_queries[1d] offset 1w)) > 0.5`, services: ["MySQL"], handbook: `# ALR-135【Grafana】Slow Query Weekly Increase - WoW Spike Alert

> **瑞幸咖啡美国运维告警响应参考手册**
>
> 本手册为参考文档，请根据实际情况灵活处理。

---

## 告警概览

| 属性 | 值 |
|------|-----|
| **告警ID** | ALR-135 |
| **告警名称** | 【Grafana告警】慢查询数量周报 |
| **优先级** | P3 |
| **服务等级** | L2 |
| **类别** | Grafana |
| **响应时间** | 低优先级响应（< 2小时） |

---

## 告警描述

此告警属于 **P3** 优先级，影响 **L2** 级别服务。

---

## 告警解析

### 告警含义

Grafana原生告警: MySQL慢查询速率5分钟内超过阈值(>1或>2/秒)。

### 业务影响

- **数据库性能**: 慢查询累积影响整体数据库性能
- **服务延迟**: 依赖数据库的服务响应变慢
- **资源消耗**: CPU和IO资源被大量消耗

### 受影响服务

所有依赖MySQL的服务

### PromQL表达式

\`\`\`promql
sum(rate(mysql_global_status_slow_queries[5m])) by (instance) > 1
\`\`\`

### 常见根因

1. **索引缺失**: 查询未命中索引
2. **表锁竞争**: InnoDB行锁或表锁等待
3. **复杂查询**: JOIN过多或子查询嵌套
4. **数据量增长**: 表数据量增长导致查询变慢

---

## 立即响应

### 第一步: 评估告警影响

**检查此告警对业务的影响:**

\`\`\`
检查点:
1. 告警是否持续存在
2. 是否有关联的高优先级告警
3. 相关服务的整体健康状态
\`\`\`

**处理建议:**
- 此告警优先级较低，可以按正常流程处理
- 先观察5-10分钟，看告警是否自动恢复
- 部分此类告警可能是瞬时波动导致的误报
- 如果持续存在，再进行详细排查

### 第二步: 初步诊断

\`\`\`
1. 检查告警详细信息
2. 查看相关Grafana仪表板
3. 收集诊断信息
4. 检查最近变更记录
\`\`\`

### 第三步: 深入排查

如果告警持续存在且未自动恢复，执行以下诊断命令:

---


## 系统访问方式

### AWS控制台访问

**AWS账号信息:**
- **Account ID**: 257394478466
- **Region**: us-east-1 (美东)
- **控制台URL**: https://257394478466.signin.aws.amazon.com/console

### AWS CLI访问

**配置AWS CLI:**
\`\`\`bash
# 确认当前AWS身份
aws sts get-caller-identity

# 确认区域配置
aws configure get region
# 应返回: us-east-1
\`\`\`

### 数据库访问

**RDS MySQL连接方式:**

1. **通过JumpServer跳板机** (推荐):
   - JumpServer地址: 联系DBA团队获取
   - 使用SSH隧道或Web终端连接

2. **通过MySQL客户端**:
\`\`\`bash
# 连接示例 (需要在内网或VPN环境)
mysql -h <RDS_ENDPOINT> -u <USERNAME> -p

# 常用RDS端点:
# 订单库: aws-luckyus-salesorder-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 支付库: aws-luckyus-salespayment-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
# 风控库: aws-luckyus-iriskcontrolservice-rw.cxwu08m2qypw.us-east-1.rds.amazonaws.com
\`\`\`

### Redis访问

**ElastiCache Redis连接方式:**

\`\`\`bash
# 通过redis-cli连接 (需要在内网)
redis-cli -h <REDIS_ENDPOINT> -p 6379

# 常用Redis集群:
# 订单缓存: luckyus-isales-order.xxxxx.use1.cache.amazonaws.com
# 会话缓存: luckyus-session.xxxxx.use1.cache.amazonaws.com
# 认证缓存: luckyus-unionauth.xxxxx.use1.cache.amazonaws.com
\`\`\`

### Kubernetes访问

**EKS集群访问:**
\`\`\`bash
# 更新kubeconfig
aws eks update-kubeconfig --name <CLUSTER_NAME> --region us-east-1

# 查看Pod状态
kubectl get pods -n <NAMESPACE>

# 查看Pod日志
kubectl logs -f <POD_NAME> -n <NAMESPACE>
\`\`\`

### 监控系统访问

**Grafana:**
- 地址: 联系DevOps团队获取Grafana URL
- 主要Datasource UID:
  - MySQL指标: ff7hkeec6c9a8e
  - Redis指标: ff6p0gjt24phce
  - 主Prometheus: df8o21agxtkw0d

**VMAlert配置:**
- APM实例: 10.238.3.137:8880, 10.238.3.143:8880, 10.238.3.52:8880
- Basic实例: 10.238.3.153:8880
- 配置文件: \`/etc/rules/alert_rules.json\`

---

## 诊断命令

\`\`\`bash
# 检查相关服务状态
kubectl get pods -A | grep -i [SERVICE_NAME]

# 检查服务日志
kubectl logs -n [NAMESPACE] [POD_NAME] --tail=100

# 检查Grafana仪表板
# 访问相关监控仪表板查看详细指标
\`\`\`

---

## 根因分析

### 常见原因

1. **索引缺失**: 查询未命中索引
2. **表锁竞争**: InnoDB行锁或表锁等待
3. **复杂查询**: JOIN过多或子查询嵌套
4. **数据量增长**: 表数据量增长导致查询变慢

#### Luckin系统特定原因

根据系统架构，以下是可能的特定原因:

1. **核心服务影响**: 检查salesorder-rw、salespayment-rw等核心数据库
2. **缓存层问题**: 检查luckyus-isales-order、luckyus-session等Redis集群
3. **认证链路**: 检查unionauth相关服务和Redis
4. **风控链路**: 检查iriskcontrol服务状态

### 排查清单

- [ ] 确认告警触发时间和频率
- [ ] 检查相关服务健康状态
- [ ] 验证数据库/缓存连接和性能
- [ ] 检查最近的部署或配置变更
- [ ] 分析相关日志是否有异常
- [ ] 检查依赖服务状态
- [ ] 验证网络连接和延迟
- [ ] 检查资源使用情况

---

## 处理步骤

### 通用处理步骤

**步骤 1:** 检查服务状态和日志

**步骤 2:** 分析告警触发原因

**步骤 3:** 根据具体情况采取相应措施

**步骤 4:** 验证问题是否解决

**步骤 5:** 记录处理过程和经验

---

## 升级标准

### 升级条件

| 条件 | 升级目标 |
|------|---------|
| 初次响应无法解决 | DevOps值班成员 |
| 问题持续恶化 | Team Lead |
| 需要外部支持 | AWS/供应商支持 |

---

## 预防措施

- 建立完善的监控体系
- 定期进行容量规划
- 实施自动化运维
- 建立变更管理流程
- 进行定期演练
- 持续优化告警阈值

---

## 相关告警

以下告警经常与此告警同时出现或有关联关系:

- \`相关类别的其他告警\`
- \`依赖服务的告警\`
- \`资源使用相关告警\`
` }
        ];


        // State
        let filteredAlerts = [...alertsData];
        let selectedAlert = null;
        let currentView = 'cards';
        let expandedTeam = null;
        let expandedCategory = null;

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            initializeNavigation();
            renderCards();
            updateStats();
        });

        function updateStats() {
            document.getElementById('totalCount').textContent = alertsData.length;
            document.getElementById('displayCount').textContent = filteredAlerts.length;
        }

        // Navigation
        function initializeNavigation() {
            const sidebar = document.getElementById('sidebar');
            const teamGroups = {};
            const categoryGroups = {};
            
            alertsData.forEach(alert => {
                if (!teamGroups[alert.team]) teamGroups[alert.team] = [];
                teamGroups[alert.team].push(alert);
                if (!categoryGroups[alert.category]) categoryGroups[alert.category] = [];
                categoryGroups[alert.category].push(alert);
            });

            const priorityCounts = {
                P0: alertsData.filter(a => a.priority === 'P0').length,
                P1: alertsData.filter(a => a.priority === 'P1').length,
                P2: alertsData.filter(a => a.priority === 'P2').length,
                P3: alertsData.filter(a => a.priority === 'P3').length
            };

            sidebar.innerHTML = `
                <div class="nav-section">
                    <div class="nav-header expanded" onclick="toggleNavSection(this)">
                        <span>全部告警</span>
                        <span class="arrow">▶</span>
                    </div>
                    <div class="nav-body expanded">
                        <div class="nav-item active" onclick="filterByAll()">
                            <span>显示全部</span>
                            <span class="count">${alertsData.length}</span>
                        </div>
                    </div>
                </div>
                <div class="nav-section">
                    <div class="nav-header" onclick="toggleNavSection(this)">
                        <span>按优先级</span>
                        <span class="arrow">▶</span>
                    </div>
                    <div class="nav-body">
                        ${['P0', 'P1', 'P2', 'P3'].map(p => `
                            <div class="nav-item" onclick="filterByPriority('${p}')">
                                <span class="priority-dot ${p.toLowerCase()}"></span>
                                <span>${p} 告警</span>
                                <span class="count">${priorityCounts[p]}</span>
                            </div>
                        `).join('')}
                    </div>
                </div>
                <div class="nav-section">
                    <div class="nav-header" onclick="toggleNavSection(this)">
                        <span>按团队</span>
                        <span class="arrow">▶</span>
                    </div>
                    <div class="nav-body">
                        ${Object.entries(teamGroups).sort((a, b) => b[1].length - a[1].length).map(([team, alerts]) => `
                            <div class="nav-item" onclick="filterByTeam('${team}')">
                                <span>${team}</span>
                                <span class="count">${alerts.length}</span>
                            </div>
                        `).join('')}
                    </div>
                </div>
                <div class="nav-section">
                    <div class="nav-header" onclick="toggleNavSection(this)">
                        <span>按类别</span>
                        <span class="arrow">▶</span>
                    </div>
                    <div class="nav-body">
                        ${Object.entries(categoryGroups).sort((a, b) => b[1].length - a[1].length).map(([cat, alerts]) => `
                            <div class="nav-item" onclick="filterByCategory('${cat}')">
                                <span>${cat}</span>
                                <span class="count">${alerts.length}</span>
                            </div>
                        `).join('')}
                    </div>
                </div>
            `;
        }

        function toggleNavSection(header) {
            header.classList.toggle('expanded');
            header.nextElementSibling.classList.toggle('expanded');
        }

        // Filtering
        function filterByAll() {
            filteredAlerts = [...alertsData];
            updateActiveNav('显示全部');
            renderCurrentView();
            updateStats();
            closeDetail();
        }

        function filterByPriority(priority) {
            filteredAlerts = alertsData.filter(a => a.priority === priority);
            updateActiveNav(`${priority} 告警`);
            renderCurrentView();
            updateStats();
            closeDetail();
        }

        function filterByTeam(team) {
            filteredAlerts = alertsData.filter(a => a.team === team);
            updateActiveNav(team);
            renderCurrentView();
            updateStats();
            closeDetail();
        }

        function filterByCategory(category) {
            filteredAlerts = alertsData.filter(a => a.category === category);
            updateActiveNav(category);
            renderCurrentView();
            updateStats();
            closeDetail();
        }

        function updateActiveNav(text) {
            document.querySelectorAll('.nav-item').forEach(item => {
                const itemText = item.querySelector('span:not(.count):not(.priority-dot)')?.textContent || '';
                item.classList.toggle('active', itemText === text);
            });
        }

        // Search
        document.getElementById('searchInput').addEventListener('input', function(e) {
            const term = e.target.value.toLowerCase().trim();
            filteredAlerts = term === '' ? [...alertsData] : alertsData.filter(alert =>
                alert.name.toLowerCase().includes(term) ||
                alert.category.toLowerCase().includes(term) ||
                alert.team.toLowerCase().includes(term) ||
                alert.metric.toLowerCase().includes(term) ||
                alert.expression.toLowerCase().includes(term) ||
                alert.services.some(s => s.toLowerCase().includes(term)) ||
                alert.id.toLowerCase().includes(term)
            );
            renderCurrentView();
            updateStats();
            closeDetail();
        });

        // View Switching
        function switchView(view) {
            currentView = view;
            document.querySelectorAll('.view-btn').forEach(btn => btn.classList.toggle('active', btn.dataset.view === view));
            document.getElementById('cardsView').style.display = view === 'cards' ? 'block' : 'none';
            document.getElementById('hierarchyView').classList.toggle('active', view === 'hierarchy');
            document.getElementById('relationshipView').classList.toggle('active', view === 'relationship');
            document.getElementById('detailView').classList.remove('active');
            renderCurrentView();
        }

        function renderCurrentView() {
            if (currentView === 'cards') renderCards();
            else if (currentView === 'hierarchy') renderHierarchy();
            else if (currentView === 'relationship') renderRelationship();
        }

        // Card View
        function renderCards() {
            const container = document.getElementById('cardGrid');
            if (filteredAlerts.length === 0) {
                container.innerHTML = `<div class="empty-state"><div class="empty-state-icon">🔍</div><div>未找到匹配的告警</div></div>`;
                return;
            }
            container.innerHTML = filteredAlerts.map(alert => `
                <div class="alert-card ${alert.priority.toLowerCase()} ${selectedAlert?.id === alert.id ? 'selected' : ''}" onclick="selectAlert('${alert.id}')">
                    <div class="card-header">
                        <div>
                            <div class="card-id">${alert.id}</div>
                            <div class="card-title">${alert.name}</div>
                        </div>
                        <span class="priority-badge ${alert.priority.toLowerCase()}">${alert.priority}</span>
                    </div>
                    <div class="card-meta">
                        <span class="meta-tag category">📁 ${alert.category}</span>
                        <span class="meta-tag team">👥 ${alert.team}</span>
                        <span class="meta-tag">📊 ${alert.metric}</span>
                    </div>
                </div>
            `).join('');
        }

        // Hierarchy View
        function renderHierarchy() {
            const container = document.getElementById('hierarchyContainer');
            const categoryGroups = {};
            filteredAlerts.forEach(alert => {
                if (!categoryGroups[alert.category]) categoryGroups[alert.category] = [];
                categoryGroups[alert.category].push(alert);
            });

            container.innerHTML = Object.entries(categoryGroups).map(([category, alerts]) => `
                <div class="hierarchy-category">
                    <div class="hierarchy-category-header" onclick="toggleHierarchyCategory(this.parentElement)">
                        <div class="hierarchy-category-title">
                            <span>${category}</span>
                            <span class="hierarchy-category-count">${alerts.length}</span>
                        </div>
                        <span class="hierarchy-toggle">▼</span>
                    </div>
                    <div class="hierarchy-category-body">
                        ${alerts.map(alert => `
                            <div class="hierarchy-alert" onclick="selectAlert('${alert.id}')">
                                <span class="priority-dot ${alert.priority.toLowerCase()}"></span>
                                <span>${alert.name}</span>
                            </div>
                        `).join('')}
                    </div>
                </div>
            `).join('');
        }

        function toggleHierarchyCategory(element) {
            element.classList.toggle('expanded');
        }

        // Relationship View - Fixed with clickable sub-nodes
        function renderRelationship() {
            const container = document.getElementById('relationshipContainer');
            const teamGroups = {};
            alertsData.forEach(alert => {
                if (!teamGroups[alert.team]) teamGroups[alert.team] = { categories: {} };
                if (!teamGroups[alert.team].categories[alert.category]) {
                    teamGroups[alert.team].categories[alert.category] = [];
                }
                teamGroups[alert.team].categories[alert.category].push(alert);
            });

            container.innerHTML = `
                <div class="relationship-header">
                    <div class="relationship-title">告警架构关系图</div>
                    <div class="relationship-subtitle">点击团队节点展开分类，点击分类节点查看告警列表</div>
                </div>
                <div class="tree-wrapper">
                    <div class="tree-level">
                        <div class="tree-node root">瑞幸北美告警系统<div class="node-count">${alertsData.length} 告警</div></div>
                    </div>
                    <div class="tree-level" id="teamLevel">
                        ${Object.entries(teamGroups).map(([team, data], idx) => {
                            const totalAlerts = Object.values(data.categories).flat().length;
                            return `
                                <div class="tree-node team" onclick="toggleTeamExpand('${team}')" data-team="${team}">
                                    <div>${team}</div>
                                    <div class="node-count">${totalAlerts} 告警</div>
                                </div>
                            `;
                        }).join('')}
                    </div>
                    <div class="category-panel" id="categoryPanel"></div>
                    <div class="alert-panel" id="alertPanel"></div>
                </div>
            `;
        }

        function toggleTeamExpand(team) {
            const teamNodes = document.querySelectorAll('.tree-node.team');
            const categoryPanel = document.getElementById('categoryPanel');
            const alertPanel = document.getElementById('alertPanel');
            
            // Reset all team nodes
            teamNodes.forEach(node => node.classList.remove('expanded'));
            alertPanel.classList.remove('expanded');
            
            if (expandedTeam === team) {
                expandedTeam = null;
                expandedCategory = null;
                categoryPanel.classList.remove('expanded');
                categoryPanel.innerHTML = '';
                return;
            }
            
            expandedTeam = team;
            expandedCategory = null;
            
            // Highlight selected team
            document.querySelector(`.tree-node.team[data-team="${team}"]`)?.classList.add('expanded');
            
            // Get categories for this team
            const teamCategories = {};
            alertsData.filter(a => a.team === team).forEach(alert => {
                if (!teamCategories[alert.category]) teamCategories[alert.category] = [];
                teamCategories[alert.category].push(alert);
            });
            
            categoryPanel.innerHTML = `
                <div class="category-grid">
                    ${Object.entries(teamCategories).map(([category, alerts]) => `
                        <div class="tree-node category" onclick="toggleCategoryExpand('${team}', '${category}')" data-category="${category}">
                            <div>${category}</div>
                            <div class="node-count">${alerts.length} 告警</div>
                        </div>
                    `).join('')}
                </div>
            `;
            categoryPanel.classList.add('expanded');
        }

        function toggleCategoryExpand(team, category) {
            const alertPanel = document.getElementById('alertPanel');
            const categoryNodes = document.querySelectorAll('.tree-node.category');
            
            // Reset category highlights
            categoryNodes.forEach(node => node.classList.remove('expanded'));
            
            if (expandedCategory === category) {
                expandedCategory = null;
                alertPanel.classList.remove('expanded');
                alertPanel.innerHTML = '';
                return;
            }
            
            expandedCategory = category;
            
            // Highlight selected category
            document.querySelector(`.tree-node.category[data-category="${category}"]`)?.classList.add('expanded');
            
            // Get alerts for this category and team
            const categoryAlerts = alertsData.filter(a => a.team === team && a.category === category);
            
            alertPanel.innerHTML = `
                ${categoryAlerts.map(alert => `
                    <div class="alert-panel-item" onclick="selectAlert('${alert.id}')">
                        <span class="priority-dot ${alert.priority.toLowerCase()}"></span>
                        <span>${alert.name}</span>
                        <span class="priority-badge ${alert.priority.toLowerCase()}" style="margin-left:auto;">${alert.priority}</span>
                    </div>
                `).join('')}
            `;
            alertPanel.classList.add('expanded');
        }

        // Detail View
        function selectAlert(id) {
            selectedAlert = alertsData.find(a => a.id === id);
            if (!selectedAlert) return;

            document.getElementById('cardsView').style.display = 'none';
            document.getElementById('hierarchyView').classList.remove('active');
            document.getElementById('relationshipView').classList.remove('active');
            document.getElementById('detailView').classList.add('active');

            document.getElementById('detailTitle').textContent = selectedAlert.name;
            document.getElementById('detailBadges').innerHTML = `
                <span class="detail-badge">${selectedAlert.priority}</span>
                <span class="detail-badge">${selectedAlert.category}</span>
                <span class="detail-badge">${selectedAlert.team}</span>
                <span class="detail-badge">${selectedAlert.metric}</span>
            `;

            const serviceLevel = selectedAlert.priority === 'P0' ? 'L0' : (selectedAlert.priority === 'P1' ? 'L1' : 'L2');

            document.getElementById('infoTab').innerHTML = `
                <div class="info-grid">
                    <div class="info-item"><div class="info-label">告警ID</div><div class="info-value">${selectedAlert.id}</div></div>
                    <div class="info-item"><div class="info-label">优先级</div><div class="info-value" style="color:var(--${selectedAlert.priority.toLowerCase()}-color)">${selectedAlert.priority}</div></div>
                    <div class="info-item"><div class="info-label">服务等级</div><div class="info-value">${serviceLevel}</div></div>
                    <div class="info-item"><div class="info-label">类别</div><div class="info-value">${selectedAlert.category}</div></div>
                    <div class="info-item"><div class="info-label">负责团队</div><div class="info-value">${selectedAlert.team}</div></div>
                    <div class="info-item"><div class="info-label">指标类型</div><div class="info-value">${selectedAlert.metric}</div></div>
                    <div class="info-item"><div class="info-label">阈值条件</div><div class="info-value">${selectedAlert.threshold}</div></div>
                    <div class="info-item"><div class="info-label">持续时间</div><div class="info-value">${selectedAlert.duration}</div></div>
                </div>
                <div class="expression-section">
                    <div class="expression-label">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <polyline points="16 18 22 12 16 6"></polyline>
                            <polyline points="8 6 2 12 8 18"></polyline>
                        </svg>
                        PromQL 表达式
                    </div>
                    <div class="expression-code">${selectedAlert.expression}</div>
                </div>
                <div class="impact-section">
                    <div class="impact-title">📌 影响范围</div>
                    <div class="impact-services">
                        ${selectedAlert.services.length > 0 
                            ? selectedAlert.services.map(s => `<span class="service-tag">${s}</span>`).join('') 
                            : '<span style="color: var(--text-muted);">待确认</span>'}
                    </div>
                </div>
            `;

            document.getElementById('handbookContent').innerHTML = marked.parse(generateHandbook(selectedAlert));
            switchTab('info');
        }

        function generateHandbook(alert) {
            return alert.handbook || "暂无处理手册内容。";
        }

        

        function closeDetail() {
            document.getElementById('detailView').classList.remove('active');
            selectedAlert = null;
            if (currentView === 'cards') document.getElementById('cardsView').style.display = 'block';
            else if (currentView === 'hierarchy') document.getElementById('hierarchyView').classList.add('active');
            else if (currentView === 'relationship') document.getElementById('relationshipView').classList.add('active');
        }

        function switchTab(tabName) {
            document.querySelectorAll('.detail-tab').forEach(tab => tab.classList.toggle('active', tab.dataset.tab === tabName));
            document.querySelectorAll('.tab-content').forEach(content => content.classList.toggle('active', content.id === tabName + 'Tab'));
        }
    </script>
</body>
</html>
